{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spindle Token","text":"<p>The open source implementation of the Open Privacy Preserving Record Linkage (OPPRL) protocol build on Spark.</p>"},{"location":"#rationale","title":"Rationale","text":"<p>Privacy Preserving Record Linkage (PPRL) is crucial component to data de-identification systems. PPRL obfuscates identifying attributes or other sensitive information about the subjects described in the records of a dataset while still preserving the ability to link records pertaining to the same subject through the use of an encrypted token. This practice is sometimes referred to as \"tokenization\" and is one of the components of data de-identification.</p> <p>The task of PPRL is to replace the attributes of a every record denoting Personally Identifiable Information (PII) with a token produced by a one-way cryptographic function. This prevents observers of the tokenized data from obtaining the PII. The tokens are produced deterministically such that input records with the same, or similar, PII attributes will produce an identical token. This allows practitioners to associate records across datasets that are highly likely to belong to the same data subject without having access to PII.</p> <p>Tokenization is also used when data is shared between organizations to limit, or in some cases fully mitigate, the risk of subject re-identification in the event that an untrusted third party gains access to a dataset containing sensitive data. Each party produces encrypted tokens using a different secret key so that any compromised data asset is, at worst, only matchable to other datasets maintained by the same party. During data sharing transactions, a specific \"transcode\" data flow is used to re-encrypt the sender's tokens into ephemeral tokens that do not match tokens in any other dataset and can only be ingested using the recipients secret key. At no point in the \"transcode\" data flow is the original PII used.</p> <p>The spindle-token is the canonical implementation of the Open Privacy Preserving Record Linkage (OPPRL) protocol. This protocol presents a standardized methodology for tokenization that can be implemented in any data system to increase interoperability. The spindle-token implementation is a python library that distributes tokenization workloads using apache Spark across multiple cores or multiple machines in a high performance computing cluster for efficient tokenization of any scale datasets.</p> <p>The pre-v1.0 versions of this library were published under the name \"carduus\" and the deprecated APIs can be found here.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See the getting started guide on the project's web page for an detailed explanation of how carduus is used including example code snippets.</p> <p>The full API and an example usage on Databricks are also provided on the project's web page.</p>"},{"location":"#security-audit","title":"Security Audit","text":"<p>This project has received a security audit from Echelon Risk + Cyber who provided the following statement. More details on this security audit can be obtained from Echelon Risk + Cyber at this link.</p> <p>Echelon Risk + Cyber certifies that as of May 30, 2025, The Spindle Token implementation and Open Privacy Preserving Record Linkage (OPPRL) Protocol exhibit a high degree of alignment with secure cryptographic standards and secure development practices. The use of FIPS -compliant algorithms (AES-GCM-SIV, RSA-OAEP, SHA2 family), layered encryption, and privacy preserving design patterns indicate strong foundational security. Note: This certification is issued in good faith, based on the materials available to the Echelon team at the time of the review.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Please refer to the spindle-token contributing guide for information on how to get started contributing to the project.</p>"},{"location":"#organizations-that-have-contributed-to-spindle-token","title":"Organizations that have contributed to spindle-token","text":""},{"location":"#individuals-that-have-contributed-to-spindle-token","title":"Individuals that have contributed to spindle-token","text":"<p>Brian Fallik - @bfallik</p>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":"<p>These community rules are put in place in order to ensure that development of  carduus stays focused and productive.</p>"},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported anonymously using this form. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"CONTRIBUTING/","title":"Contributing to carduus","text":"<p>All interest in carduus, as a user or contributor, is greatly appreciated! This document will go into detail on how to contribute to the development of the carduus software package.</p> <p>If you are looking to contribute to the research and design of the Open Privacy Preserving Record Linkage (OPPRL) specification, see this page.</p>"},{"location":"CONTRIBUTING/#before-contributing","title":"Before Contributing","text":"<p>Before reading further we ask that you read our Code of Conduct which will be enforced by the maintainers in order to ensure that development of carduus stays focused and productive.</p> <p>If you are new to contributing to open source, or GitHub, the following links may be helpful starting places:</p> <ul> <li>How to Contribute to Open Source</li> <li>Understanding the GitHub flow</li> </ul>"},{"location":"CONTRIBUTING/#we-use-github-flow","title":"We Use Github Flow","text":"<p>This means that all code and documentation changes happen through pull requests. We actively welcome your pull requests. We highly recommend the following workflow.</p> <ol> <li>Fork the repo and create your branch from <code>main</code>.</li> <li>If you've added code that should be tested, add tests.</li> <li>If you've changed APIs, update the documentation.</li> <li>Ensure the test suite passes.</li> <li>Create the pull request.</li> </ol>"},{"location":"CONTRIBUTING/#any-contributions-you-make-will-be-under-the-mit-software-license","title":"Any contributions you make will be under the MIT Software License","text":"<p>In short, when you submit code changes, your submissions are understood to be under the same MIT License that covers the project. Feel free to contact the maintainers if that's a concern.</p>"},{"location":"CONTRIBUTING/#how-to-contribute-a","title":"How to contribute a ...","text":""},{"location":"CONTRIBUTING/#bug-report","title":"Bug Report","text":"<p>We use GitHub issues to track public bugs. Report a bug by opening a new issue.</p> <p>Great Bug Reports tend to have at least the following:</p> <ul> <li>A quick summary and/or background</li> <li>The steps to reproduce.</li> <li>When possible, minimal code that reproduces the bug.</li> <li>A description of what you expected versus what actually happens.</li> </ul>"},{"location":"CONTRIBUTING/#feature-request","title":"Feature Request","text":"<p>We like to hear in all feature requests and discussion around the direction of the project. The best place to discuss future features is the project's discussion page under the ideas category.</p>"},{"location":"CONTRIBUTING/#bug-fix-new-feature-documentation-improvement-or-other-change","title":"Bug fix, new feature, documentation improvement, or other change.","text":"<p>We welcome contribution to the codebase via pull requests. In most cases, it is beneficial to discuss your change with the community via a GitHub issue or discussion before working on a pull request. Once you decide to work on a pull request, please follow the workflow outlined in the above sections.</p> <p>Once you open the pull request, it will be tested with by CI and reviewed by other contributors (including at least one project maintainer). After all iterations of review are finished, one of the project maintainers will merge your pull request.</p>"},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<p>When working on a code change or addition to carduus, it is expected that all changes  pass existing tests and probably introduce new tests to ensure stability of future changes. </p> <p>Before you are able to run tests, you must have a virtual environment for the project. Carduus uses  Poetry to manage Python environments. Once poetry is installed, run the following in  the root directory of the project.</p> <pre><code>poetry install\n</code></pre> <p>To run the test suite, use the following command in the root of the project.</p> <pre><code>poetry run pytest\n</code></pre>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2025 Spindle Health, Inc.</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"api/","title":"API","text":"<p>The spindle-token API is broken up into 3 namespaces: </p> <ol> <li>The top-level <code>spindle_token</code> module containing the most commonly used public functions.</li> <li>An OPPRL module containing implementations of every OPPRL protocol version, including PII normalization, token specifications, and cryptographic transformations.</li> <li>A module of <code>core</code> abstractions than can be extended in advanced use cases to add additional functionality.</li> </ol>"},{"location":"api/#spindle_token","title":"<code>spindle_token</code>","text":"<p>A module containing the main API of spindle-token.</p> <p>Most users will only need to use the 3 main functions in this top-level module along with the provided configuration objects corresponding to OPPRL tokenization.</p> <p>The 3 main functions provide tokenization and transcoding capabilities for data senders and recipients  respectively.</p>"},{"location":"api/#spindle_token.tokenize","title":"<code>tokenize(df, col_mapping, tokens, private_key=None)</code>","text":"<p>Adds encrypted token columns based on PII.</p> <p>All PII columns found in the <code>DataFrame</code> are normalized according and transformed as needed according to the <code>col_mapping</code>. The PII attributes that make up of each <code>Token</code> objects in <code>tokens</code> are then hashed and encrypted together according to their respective protocol versions.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pyspark <code>DataFrame</code> containing all PII attributes.</p> required <code>col_mapping</code> <code>Mapping[PiiAttribute, str]</code> <p>A dictionary that maps instances of <code>PiiAttribute</code> to the corresponding column name of <code>df</code>.</p> required <code>tokens</code> <code>Iterable[Token]</code> <p>A collection of <code>Token</code> objects that denotes which tokens (from which PII attributes) should be added to the dataframe.</p> required <code>private_key</code> <code>bytes | None</code> <p>Your private RSA key. This argument should only be set when reading from a secrets manager or testing, otherwise it is recommended to set the SPINDLE_TOKEN_PRIVATE_KEY environment variable with your private key.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input <code>DataFrame</code> with by encrypted tokens added.</p>"},{"location":"api/#spindle_token.transcode_out","title":"<code>transcode_out(df, tokens, recipient_public_key=None, private_key=None)</code>","text":"<p>Transcodes token columns of a dataframe into ephemeral tokens.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pyspark <code>DataFrame</code> containing token columns.</p> required <code>tokens</code> <code>Iterable[Token]</code> <p>A collection of <code>Token</code> objects that denote which columns of the input dataframe will be transcoded into ephemeral tokens.</p> required <code>recipient_public_key</code> <code>bytes | None</code> <p>The public RSA key of the recipient who will be receiving the dataset with ephemeral tokens. Can also be supplied the SPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY environment variable.</p> <code>None</code> <code>private_key</code> <code>bytes | None</code> <p>Your private RSA key. This argument should only be set when reading from a secrets manager or testing, otherwise it is recommended to set the SPINDLE_TOKEN_PRIVATE_KEY environment variable with your private key.</p> <code>None</code> <p>Returns:     The <code>DataFrame</code> with the tokens replaced by ephemeral tokens for sending to the recipient.</p>"},{"location":"api/#spindle_token.transcode_in","title":"<code>transcode_in(df, tokens, private_key=None)</code>","text":"<p>Transcodes ephemeral token columns into normal tokens.</p> <p>Used by the data recipient of a dataset containing ephemeral tokens produced by <code>transcode_out</code> to transcode the ephemeral tokens such that they will match other datasets tokenized with the same private key.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark <code>DataFrame</code> with ephemeral token columns to transcode.</p> required <code>tokens</code> <code>Iterable[Token]</code> <p>A collection of <code>Token</code> objects that denote which columns of the input dataframe will be transcoded from ephemeral tokens into normal tokens.</p> required <code>private_key</code> <code>bytes | None</code> <p>Your private RSA key. Must be the corresponding private key for the public key given to the sender when calling <code>transcode_out</code>. This argument should only be set when reading from a secrets manager or testing, otherwise it is recommended to set the SPINDLE_TOKEN_PRIVATE_KEY environment variable with your private key.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The <code>DataFrame</code> with the ephemeral tokens replaced with normal tokens.</p>"},{"location":"api/#spindle_token.generate_pem_keys","title":"<code>generate_pem_keys(key_size=2048)</code>","text":"<p>Generates a fresh RSA key pair.</p> <p>Parameters:</p> Name Type Description Default <code>key_size</code> <code>int</code> <p>The size (in bits) of the key.</p> <code>2048</code> <p>Returns:</p> Type Description <code>tuple[bytes, bytes]</code> <p>A tuple containing the private key and public key bytes. Both in the PEM encoding.</p>"},{"location":"api/#spindle_token.opprl","title":"<code>spindle_token.opprl</code>","text":"<p>A module of classes that provide standard configuration for different versions of the OPPRL protocol.</p> <p>Each class is made up of class variables holding all instances of PiiAttributes, Token, and TokenProtocolFactory that make up the complete spec of the corresponding OPPRL version.</p>"},{"location":"api/#spindle_token.opprl.OpprlV0","title":"<code>OpprlV0</code>","text":"<p>All instances of PiiAttribute, Token, and TokenProtocolFactory for v0 of the OPPRL protocol.</p> <p>All members are class variables, and therefore this class does not need to be instantiated.</p> <p>Attributes:</p> Name Type Description <code>first_name</code> <code>NameAttribute</code> <p>The PII attribute for a subject's first name.</p> <code>last_name</code> <code>NameAttribute</code> <p>The PII attribute for a subject's last (aka family) name.</p> <code>gender</code> <code>GenderAttribute</code> <p>The PII attribute for a subject's gender.</p> <code>birth_date</code> <code>DateAttribute</code> <p>The PII attribute for a subject's date of birth.</p> <code>protocol</code> <code>TokenProtocolFactory</code> <p>The tokenization protocol for producing OPPRL version 0 tokens.</p> <code>token1</code> <code>Token</code> <p>A token generated from first initial, last name, gender, and birth date.</p> <code>token2</code> <code>Token</code> <p>A token generated from first soundex, last soundex, gender, and birth date.</p> <code>token3</code> <code>Token</code> <p>A token generated from first metaphone, last metaphone, gender, and birth date.</p>"},{"location":"api/#spindle_token.opprl.OpprlV1","title":"<code>OpprlV1</code>","text":"<p>All instances of PiiAttribute, Token, and TokenProtocolFactory for v1 of the OPPRL protocol.</p> <p>All members are class variables, and therefore this class does not need to be instantiated.</p> <p>Attributes:</p> Name Type Description <code>first_name</code> <code>NameAttribute</code> <p>The PII attribute for a subject's first name.</p> <code>last_name</code> <code>NameAttribute</code> <p>The PII attribute for a subject's last (aka family) name.</p> <code>gender</code> <code>GenderAttribute</code> <p>The PII attribute for a subject's gender.</p> <code>birth_date</code> <code>DateAttribute</code> <p>The PII attribute for a subject's date of birth.</p> <code>email</code> <code>EmailAttribute</code> <p>The PII attribute for a subject's email address.</p> <code>hem</code> <code>HashedEmail</code> <p>The PII attribute for a subject's SHA2 hashed email address.</p> <code>phone</code> <code>PhoneNumberAttribute</code> <p>The PII attribute for a subject's phone number.</p> <code>ssn</code> <code>SsnAttribute</code> <p>The PII attribute for a subject's social security number.</p> <code>group_number</code> <code>GroupNumberAttribute</code> <p>The PII attribute for a subject's health plan group number.</p> <code>member_id</code> <code>MemberIdAttribute</code> <p>The PII attribute for a subject's health plan member ID.</p> <code>protocol</code> <code>TokenProtocolFactory</code> <p>The tokenization protocol for producing OPPRL version 0 tokens.</p> <code>token1</code> <code>Token</code> <p>A token generated from first initial, last name, gender, and birth date.</p> <code>token2</code> <code>Token</code> <p>A token generated from first soundex, last soundex, gender, and birth date.</p> <code>token3</code> <code>Token</code> <p>A token generated from first metaphone, last metaphone, gender, and birth date.</p> <code>token4</code> <code>Token</code> <p>A token generated from first initial, last name, and birth date.</p> <code>token5</code> <code>Token</code> <p>A token generated from first soundex, last soundex, and birth date.</p> <code>token6</code> <code>Token</code> <p>A token generated from first metaphone, last metaphone, and birth date.</p> <code>token7</code> <code>Token</code> <p>A token generated from first name and phone number.</p> <code>token8</code> <code>Token</code> <p>A token generated from birth date and phone number.</p> <code>token9</code> <code>Token</code> <p>A token generated from first name and SSN.</p> <code>token10</code> <code>Token</code> <p>A token generated from birth date and SSN.</p> <code>token11</code> <code>Token</code> <p>A token generated from an email address.</p> <code>token12</code> <code>Token</code> <p>A token generated from a SHA2 hashed email address.</p> <code>token13</code> <code>Token</code> <p>A token generated from health plan group number and member ID.</p>"},{"location":"api/#spindle_token.opprl.IdentityAttribute","title":"<code>IdentityAttribute</code>","text":"<p>               Bases: <code>PiiAttribute</code></p> <p>An implementation of PiiAttribute with no transformation (normalization) logic.</p> <p>This class is useful when your data contains columns that can be used as attributes as-is with no normalization. In particular, if your data has columns that correspond to PII attributes that would typically be derived from other PII attributes, such as the initial of the first name.</p> <p>Examples:</p> <p>Create an identity attribute that uses the first initial directly as opposed to deriving from the first name.</p> <pre><code>&gt;&gt;&gt; attribute = IdentityAttribute(\"opprl.v1.first.initial\")\n</code></pre> <p>The transform method returns the input column unchanged.</p> <pre><code>&gt;&gt;&gt; attribute.transform(col(\"first_initial\"), StringType())\nColumn&lt;'first_initial'&gt;\n</code></pre> <p>There are no derivatives of identity attributes aside from the attribute itself.</p> <pre><code>&gt;&gt;&gt; attribute.derivatives()\n{'opprl.v1.first.initial': IdentityAttribute(opprl.v1.first.initial)}\n</code></pre> <p>Identity attributes can be passed to the tokenize function.</p> <pre><code>&gt;&gt;&gt; from spindle_token.opprl import OpprlV1 as v1\n&gt;&gt;&gt; tokenize(\n&gt;&gt;&gt;     df,\n&gt;&gt;&gt;     {\n&gt;&gt;&gt;         IdentityAttribute(\"opprl.v1.first.initial\"): \"first_initial\",\n&gt;&gt;&gt;         v1.last_name: \"last_name\",\n&gt;&gt;&gt;         v1.gender: \"gender\",\n&gt;&gt;&gt;         v1.birth_date: \"birth_date\",\n&gt;&gt;&gt;     },\n&gt;&gt;&gt;     [v1.token1],\n&gt;&gt;&gt; )\nDataFrame[first_initial: string, last_name: string, ..., opprl_token_1v1: string]\n</code></pre>"},{"location":"api/#spindle_token.core","title":"<code>spindle_token.core</code>","text":"<p>The core abstractions of spindle-token, including abstract base classes for extending functionality.</p> <p>The spindle-token library provides base interfaces that cane be extended by users to define custom token  specifications to encrypt with existing versions of OPPRL cryptography protocols, or define entirely new  tokenization protocols.</p> Warning <p>Extending the base classes in this module to customize the tokenization behavior has no security or  privacy guarantees. These abstractions -- like all OSS -- are \"use at your own risk\" and users should only use these advanced features if they understand them.</p>"},{"location":"api/#spindle_token.core.PiiAttribute","title":"<code>PiiAttribute</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An attribute (aka column) of personally identifiable information (PII) to use when constructing tokens.</p> <p>This abstract base class is intended to be extended by users to add support for building tokens from a custom PII attribute.</p> <p>Attributes:</p> Name Type Description <code>attr_id</code> <p>An identifier for the PiiAttribute. Should be unique across all logically different PiiAttributes.</p>"},{"location":"api/#spindle_token.core.PiiAttribute.__init__","title":"<code>__init__(attr_id)</code>","text":"<p>Initializes the PiiAttribute with the given globally unique attribute ID.</p>"},{"location":"api/#spindle_token.core.PiiAttribute.transform","title":"<code>transform(column, dtype)</code>  <code>abstractmethod</code>","text":"<p>Transforms the raw PII column into a normalized representation.</p> <p>A normalized value has eliminated all representation or encoding differences so all instances of the same logical values have identical physical values. For example, text attributes will often be normalized by filtering to alpha-numeric characters and whitespace, standardizing all whitespace to the space character, and converting all alpha characters to uppercase to ensure that all ways of representing the same phrase normalize to the exact same string.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>The spark <code>Column</code> expression for the PII attribute being normalized.</p> required <code>dtype</code> <code>DataType</code> <p>The spark <code>DataType</code> object of the <code>column</code> object found on the <code>DataFrame</code> being normalized. Can be used to delegate to different normalization logic based on different schemas of input data. For example, a subject's birth date may be a <code>DateType</code>, <code>StringType</code>, or <code>LongType</code> on input data and thus requires corresponding normalization into a <code>DateType</code>.</p> required <p>Returns:</p> Type Description <code>Column</code> <p>A pyspark Column expression of normalized PII values.</p>"},{"location":"api/#spindle_token.core.PiiAttribute.derivatives","title":"<code>derivatives()</code>","text":"<p>A collection of PII attributes that can be derived from this PII attribute, including this PiiAttribute.</p> <p>Returns:</p> Type Description <code>dict[str, PiiAttribute]</code> <p>A <code>dict</code> with globally unique (typically namespaced) attribute IDs as the key. Values are instances of</p> <code>dict[str, PiiAttribute]</code> <p>PiiAttribute that produce normalized values for each derivative attribute</p> <code>dict[str, PiiAttribute]</code> <p>from the normalized values of this PiiAttribute.</p>"},{"location":"api/#spindle_token.core.TokenProtocol","title":"<code>TokenProtocol</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for a specific version of the OPPRL tokenization protocol.</p> <p>This abstract base class is intended to be extended by users who want to implement custom tokenization protocols.</p> <p>It is assumed that instances of the <code>TokenProtocol</code> will provide any configuration or other inputs (such as encryption keys) required to produce tokens. See <code>TokenProtocolFactory</code> for more information.</p>"},{"location":"api/#spindle_token.core.TokenProtocol.tokenize","title":"<code>tokenize(attribute_ids)</code>  <code>abstractmethod</code>","text":"<p>Creates a Column expression for a single token.</p> <p>Parameters:</p> Name Type Description Default <code>attribute_ids</code> <code>list[str]</code> <p>A collection <code>PiiAttribute</code> attribute IDs corresponding to the attributes to combine into the token.</p> required <p>Returns:</p> Type Description <code>Column</code> <p>A pyspark <code>Column</code> expression representing token values.</p>"},{"location":"api/#spindle_token.core.TokenProtocol.transcode_out","title":"<code>transcode_out(token)</code>  <code>abstractmethod</code>","text":"<p>Transcodes the given token into an ephemeral token.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>Column</code> <p>A pyspark <code>Column</code> of tokens.</p> required <p>Returns:</p> Type Description <code>Column</code> <p>A pyspark <code>Column</code> expression of ephemeral tokens created from the input tokens.</p>"},{"location":"api/#spindle_token.core.TokenProtocol.transcode_in","title":"<code>transcode_in(ephemeral_token)</code>  <code>abstractmethod</code>","text":"<p>Transcodes the given ephemeral token into a normal token.</p> <p>Parameters:</p> Name Type Description Default <code>ephemeral_token</code> <code>Column</code> <p>A pyspark <code>Column</code> of ephemeral tokens.</p> required <p>Returns:</p> Type Description <code>Column</code> <p>A pyspark <code>Column</code> expression of tokens created from the input ephemeral tokens.</p>"},{"location":"api/#spindle_token.core.TokenProtocolFactory","title":"<code>TokenProtocolFactory</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[P]</code></p> <p>An abstract base class for factories that instantiate <code>TokenProtocol</code> implementations with user provided encryption keys.</p> <p>This abstract base class is intended to be extended by users who want to implement custom tokenization protocols.</p> <p>Attributes:</p> Name Type Description <code>factory_id</code> <p>An identifier for the <code>TokenProtocolFactory</code>. Should be globally unique across all logically different <code>TokenProtocolFactory</code>.</p>"},{"location":"api/#spindle_token.core.TokenProtocolFactory.__init__","title":"<code>__init__(factory_id)</code>","text":"<p>Initializes the <code>TokenProtocolFactory</code> with the given globally unique factory ID.</p>"},{"location":"api/#spindle_token.core.TokenProtocolFactory.bind","title":"<code>bind(private_key, recipient_public_key)</code>  <code>abstractmethod</code>","text":"<p>Creates an instance of the TokenProtocol with the user provided encryption keys.</p> <p>Parameters:</p> Name Type Description Default <code>private_key</code> <code>bytes</code> <p>The private RSA key to use when tokenizing PII and transcoding tokens.</p> required <code>recipient_public_key</code> <code>bytes | None</code> <p>The public RSA key of the intended data recipient to use when transcoding tokens into ephemeral tokens. Can be <code>None</code> if the instance of <code>TokenProtocol</code> will not be transcoding tokens into ephemeral tokens.</p> required <p>Returns:</p> Type Description <code>P</code> <p>An instance of a <code>TokenProtocol</code> implementation.</p>"},{"location":"api/#spindle_token.core.Token","title":"<code>Token</code>  <code>dataclass</code>","text":"<p>A specification of a token.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>An identifier safe name for the attribute. Will be used as the column name on dataframes. Must be unique across other <code>Token</code> specifications.</p> <code>protocol</code> <code>TokenProtocolFactory</code> <p>An instance of <code>TokenProtocolFactory</code> that, when provided encryption keys, produced an instance of the <code>TokenProtocol</code> that generates this kind of token.</p> <code>attribute_ids</code> <code>Iterable[str]</code> <p>A collection of attribute IDs used to lookup instances of <code>PiiAttribute</code> corresponding to the fields used to create the token.</p>"},{"location":"cli/","title":"Command Line Interface","text":"<p>The spindle-token command line interface (CLI) offers tokenization and token transcoding capabilities of data files from the local file system.</p>"},{"location":"cli/#usage-guide","title":"Usage Guide","text":"<p>The spindle-token CLI is included with every installation of the spindle-token library. Install spindle-token to your python (virtual) environment using <code>pip</code>. See our getting started guide for more information. Make sure the python interpreter directory is on your PATH.</p> <p>You can test your installation and environment setup by running the <code>--help</code> command. You should see documentation about the spindle-token CLI.</p> <pre><code>spindle-token --help\n</code></pre> <p>Once installed, you can run the commands for <code>tokenize</code> and <code>transcode</code> with the relevant options and arguments. All commands and sub-commands follow the same general design. Positional arguments and paths the the input data and desired location to write output data. Options configure how the input data is transformed. For example, options dictate which tokens should be generated, which file format to use, and the encryption key.</p> <p>This example invocation of the <code>tokenize</code> command illustrates the general pattern.</p> <pre><code>spindle-token tokenize \\\n    --token opprl_token_1v1 --token opprl_token_2v1 --token opprl_token_3v1 \\\n    --key private_key.pem \\\n    --format csv \\\n    --parallelism 1 \\\n    pii.csv tokens.csv\n</code></pre>"},{"location":"cli/#encryption-keys","title":"Encryption Keys","text":"<p>The OPPRL protocol leaves the responsibility of encryption key management to the user. The spindle-token CLI assumes the public and private keys are stored in files on the local filesystem. The location of the PEM file can be passed using the corresponding option or an environment variable. This table describes the option names and environment variables that can be use to supply private and public keys respectively.</p> <p>Private Key</p> <p>The private RSA key can be set using one of the following methods:</p> <ul> <li>Use the <code>--key</code> option (or <code>-k</code> alias) to specify a path to a PEM file.</li> <li>Set the <code>SPINDLE_TOKEN_PRIVATE_KEY_FILE</code> environment variable to specify a path the PEM file.</li> <li>Set the <code>SPINDLE_TOKEN_PRIVATE_KEY</code> environment variable to specify the key as a UTF-8 string. If both environment variables are set, the <code>_FILE</code> variant takes precedence.</li> </ul> <p>Public Keys</p> <p>The public keys of data recipients (used in transcryption) can be set using one of the following methods:</p> <ul> <li>Use the <code>--recipient</code> option (or <code>-r</code> alias) to specify a path to a PEM file.</li> <li>Set the <code>SPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY_FILE</code> environment variable to specify a path the PEM file.</li> <li>Set the <code>SPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY</code> environment variable to specify the key as a UTF-8 string. If both environment variables are set, the <code>_FILE</code> variant takes precedence.</li> </ul>"},{"location":"cli/#file-formats","title":"File Formats","text":"<p>The spindle-token CLI supports multiple data file formats. The recommended file format is parquet because parquet files are efficient, compressed, and have unambiguous schemas. The spindle-token CLI also supports CSV files. The file format must be specified using the <code>--format</code> option or <code>SPINDLE_TOKEN_FORMAT</code> environment variable.</p> <p>When using CSV files there are a few assumptions that the data file(s) must meet. </p> <ol> <li> <p>The first row of each CSV file must be column headers. See the next section for expectation on the column names.</p> </li> <li> <p>The field separate (aka delimiter) should be a pipe <code>|</code> character.</p> </li> </ol> <p>Input datasets can either be a single data file or partitioned into a directory of multiple data files. Splitting larger datasets into multiple data files can help the CLI parallelize over larger datasets. If the input dataset is a single data file the output dataset will be single data file. Similarly, if the input dataset is a directory the argument for the output location will be a directory that will include a partitioned output dataset.</p>"},{"location":"cli/#column-names","title":"Column Names","text":"<p>THe OPPRL tokenization protocol requires specific PII attributes to be normalized, transformed, concatenated, hashed, and then encrypted together. Thus, the spindle-token CLI must know which columns of the input dataset correspond to each logical PII attribute (first name, last name, birth date, etc).</p> <p>The spindle-token CLI requires specific column naming for PII columns so that the proper normalization rules are applied to each attribute and the final tokens are created from the correct subset of inputs. The following list contains the exact column names the CLI will look for.</p> <ul> <li><code>first_name</code></li> <li><code>last_name</code></li> <li><code>gender</code></li> <li><code>birth_date</code></li> <li><code>email</code></li> <li><code>hem</code></li> <li><code>phone</code></li> <li><code>ssn</code></li> <li><code>group_number</code></li> <li><code>member_id</code></li> </ul> <p>If you are only adding tokens that require a subset of these PII fields, the input dataset may omit columns for the other PII attributes that are not required. For information on which PII attributes are required for each token in the OPPRL protocol, see the official specification.</p>"},{"location":"cli/#parallelism","title":"Parallelism","text":"<p>The spindle-token CLI supports multi-threaded parallelism. This helps work with larger datasets that are partitioned into multiple part files within a directory. If <code>--parallelism</code> is set to a number &gt;1 then that number of partition files will be processed at once. This option can also be set with the <code>SPINDLE_TOKEN_PARALLELISM</code> environment variable. If parallelism is not provided, the spindle-token will default to using the same number of threads as the host machine has logical cores.</p>"},{"location":"cli/#commands","title":"Commands","text":"<p>The help text, options, and arguments of every command and sub-command of the spindle-token CLI. You can get this documentation for the specific version of the CLI installed in your python environment using the <code>--help</code> option on any command or sub-command.</p>"},{"location":"cli/#spindle-token-tokenize","title":"spindle-token tokenize","text":"<p>Add tokens to a dataset of PII.</p> <p>Creates a dataset at the OUTPUT location that adds encrypted OPPRL tokens to the INPUT dataset. Does not modify the INPUT dataset.</p> <p>INPUT is the path to the dataset to tokenize. If INPUT is a file, it must be of the format provided to the <code>--format</code> option. If INPUT is a directory, all files within the directory that match the given format will be considered a partition of the dataset.</p> <p>OUTPUT is the file or directory in which the tokenized dataset will be written. If INPUT is a file, the OUTPUT will be written to as a file. If INPUT is a directory, the OUTPUT will be a directory containing a dataset partitioned into files.</p> <p>Usage:</p> <pre><code>spindle-token tokenize [OPTIONS] INPUT OUTPUT\n</code></pre> <p>Options:</p> <pre><code>  -t, --token [opprl_token_1v0|opprl_token_2v0|opprl_token_3v0|opprl_token_1v1|opprl_token_2v1|opprl_token_3v1|opprl_token_4v1|opprl_token_5v1|opprl_token_6v1|opprl_token_7v1|opprl_token_8v1|opprl_token_9v1|opprl_token_10v1|opprl_token_11v1|opprl_token_12v1|opprl_token_13v1]\n                                  An OPPRL token to add to the dataset. Can be\n                                  passed multiple times.  [required]\n  -k, --key FILENAME              The PEM file containing your private key.\n  -f, --format [parquet|csv]      The file format of input and output data\n                                  files.  [required]\n  -p, --parallelism INTEGER       The number of worker threads to parallelize\n                                  over. Useful when the input dataset is\n                                  partitioned into multiple part files. If not\n                                  supplied, defaults to the number of logical\n                                  cores.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#spindle-token-transcode","title":"spindle-token transcode","text":"<p>Prepare tokenized datasets to be sent or received.</p> <p>Usage:</p> <pre><code>spindle-token transcode [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"cli/#in","title":"in","text":"<p>Convert a dataset of ephemeral tokens into tokens.</p> <p>INPUT is the path to the dataset of ephemeral tokens to create tokens from. If INPUT is a file, it must be of the format provided to the <code>--format</code> option. If INPUT is a directory, all files within the directory that match the given format will be run.</p> <p>OUTPUT is the file or directory in which the tokenized dataset will be written. If INPUT is a file, the OUTPUT will be written to as a file. If INPUT is a directory, the OUTPUT will be a directory containing a dataset partitioned into files.</p> <p>Usage:</p> <pre><code>spindle-token transcode in [OPTIONS] INPUT OUTPUT\n</code></pre> <p>Options:</p> <pre><code>  -t, --token [opprl_token_1v0|opprl_token_2v0|opprl_token_3v0|opprl_token_1v1|opprl_token_2v1|opprl_token_3v1|opprl_token_4v1|opprl_token_5v1|opprl_token_6v1|opprl_token_7v1|opprl_token_8v1|opprl_token_9v1|opprl_token_10v1|opprl_token_11v1|opprl_token_12v1|opprl_token_13v1]\n                                  The column name of an OPPRL token on the\n                                  input data to transcode.  [required]\n  -k, --key FILENAME              The PEM file containing your private key.\n  -f, --format [parquet|csv]      The file format of input and output data\n                                  files.  [required]\n  -p, --parallelism INTEGER       The number of worker threads to parallelize\n                                  over. Useful when the input dataset is\n                                  partitioned into multiple part files. If not\n                                  supplied, defaults to the number of logical\n                                  cores.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#out","title":"out","text":"<p>Prepare ephemeral tokens for a specific recipient.</p> <p>INPUT is the path to the dataset of tokens to create ephemeral tokens from. If INPUT is a file, it must be of the format provided to the <code>--format</code> option. If INPUT is a directory, all files within the directory that match the given format will be considered a partition of the dataset.</p> <p>OUTPUT is the file or directory in which the tokenized dataset will be written. If INPUT is a file, the OUTPUT will be written to as a file. If INPUT is a directory, the OUTPUT will be a directory containing a dataset partitioned into files.</p> <p>Usage:</p> <pre><code>spindle-token transcode out [OPTIONS] INPUT OUTPUT\n</code></pre> <p>Options:</p> <pre><code>  -t, --token [opprl_token_1v0|opprl_token_2v0|opprl_token_3v0|opprl_token_1v1|opprl_token_2v1|opprl_token_3v1|opprl_token_4v1|opprl_token_5v1|opprl_token_6v1|opprl_token_7v1|opprl_token_8v1|opprl_token_9v1|opprl_token_10v1|opprl_token_11v1|opprl_token_12v1|opprl_token_13v1]\n                                  The column name of an OPPRL token on the\n                                  input data to transcode.  [required]\n  -r, --recipient FILENAME        The PEM file containing the recipients\n                                  public key.\n  -k, --key FILENAME              The PEM file containing your private key.\n  -f, --format [parquet|csv]      The file format of input and output data\n                                  files.  [required]\n  -p, --parallelism INTEGER       The number of worker threads to parallelize\n                                  over. Useful when the input dataset is\n                                  partitioned into multiple part files. If not\n                                  supplied, defaults to the number of logical\n                                  cores.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"cli/#limitations","title":"Limitations","text":"<p>The following limitations of the spindle-token CLI are. For a superior experience, consider using spindle-token as a Python library. If your use case requires addressing some of these limitations, please open an issue with additional details. </p>"},{"location":"cli/#no-horizontal-scaling","title":"No Horizontal Scaling","text":"<p>The spindle-token CLI is built with Apache Spark to allow for data parallelism. Spark is designed to distribute workloads horizontally across a cluster of multiple machines connected to the same network. The spindle-token CLI runs spark in \"local\" mode that switches execution to a multi-threaded design on a single host machine.</p> <p>The spindle-token CLI cannot be passed to <code>spark-submit</code>, nor is there currently a way to pass spark connect information to a remote spark cluster. If you would like to use spindle-token on a Spark cluster, it is recommended that you use the spindle-token Python library.</p>"},{"location":"cli/#no-remote-file-systems","title":"No Remote File Systems","text":"<p>The spindle-token CLI reads and writes files using the local file system. This means there is no native support for files stored on remote filesystems, like S3. </p> <p>You may be able to work with remote file systems if you have a method of mounting the remote file system to the local file system.</p> <p>If you require working with datasets on remote filesystems like S3, it is recommended that you use the spindle-token Python library and configure pyspark to read from S3.</p>"},{"location":"opprl/","title":"Open Privacy Preserving Record Linkage","text":"<p>Open Privacy Preserving Record Linkage (OPPRL) is a protocol for a privacy preserving record linkage system that can be implemented across data systems to assist in the de-identification of data in a way that preserves privacy while enabling sharing of data assets between trusted organizations.</p> <p>Implementers can find the full specification on the OPPRL Specification page.</p> <p>As an open specification, all practitioners that interact with OPPRL implementations are invited to contribute to discussion, research, and proposals for improvements to the specification. For details on how to participate, see the OPPRL contributing guide.</p>"},{"location":"carduus/api/","title":"Carduus API","text":""},{"location":"carduus/api/#functions","title":"Functions","text":"<p>THe core functionality of carduus is provided by a the following functions that operate over pyspark <code>DataFrame</code>.</p> <p>The <code>TokenSpec</code> class allows for custom tokens, beyond the builtin OPPRL tokens, to be generated during tokenization. See the custom tokens guide for more information.</p>"},{"location":"carduus/api/#carduus.token.tokenize","title":"carduus.token.tokenize","text":"<pre><code>tokenize(df, pii_transforms, tokens, private_key=None)\n</code></pre> <p>Adds encrypted token columns based on PII.</p> <p>All PII columns found in the <code>DataFrame</code> are normalized using the provided <code>pii_transforms</code>. All PII attributes provided by the enhancements of the <code>pii_transforms</code> are added if they are not already present in the <code>DataFrame</code>. The fields of each <code>TokenSpec</code> from <code>tokens</code> are hashed and encrypted together according to the OPPRL specification.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pyspark <code>DataFrame</code> containing all PII attributes.</p> required <code>pii_transforms</code> <code>Mapping[str, PiiTransform | OpprlPii]</code> <p>A dictionary that maps column names of <code>df</code> to PiiTransform objects to specify how each raw PII column is normalized and enhanced into derived PII attributes. Values can also be a member of the OpprlPii enum if using the standard OPPRL tokens.</p> required <code>tokens</code> <code>Sequence[TokenSpec | OpprlToken]</code> <p>A collection of <code>TokenSpec</code> objects that denotes which PII attributes are encrypted into each token. Elements can also be a member of the OpprlToken enum if using the standard OPPRL tokens.</p> required <code>private_key</code> <code>bytes | None</code> <p>Your private RSA key. This argument should only be set when reading from a secrets manager or testing, otherwise it is recommended to set the SPINDLE_TOKEN_PRIVATE_KEY environment variable with your private key.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The <code>DataFrame</code> with PII columns replaced by encrypted tokens.</p>"},{"location":"carduus/api/#carduus.token.transcrypt_out","title":"carduus.token.transcrypt_out","text":"<pre><code>transcrypt_out(\n    df,\n    token_columns,\n    recipient_public_key=None,\n    private_key=None,\n)\n</code></pre> <p>Prepares a <code>DataFrame</code> containing encrypted tokens to be sent to a specific trusted party by re-encrypting the tokens using the recipient's public key without exposing the original PII.</p> <p>Output tokens will be unmatchable to any dataset or within the given dataset until the intended recipient processes the data with <code>transcrypt_in</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark <code>DataFrame</code> with token columns to transcrypt.</p> required <code>token_columns</code> <code>Iterable[str]</code> <p>The collection of column names that correspond to tokens.</p> required <code>recipient_public_key</code> <code>bytes | None</code> <p>The public RSA key of the recipient who will be receiving the dataset with ephemeral tokens. Can also be supplied the SPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY environment variable.</p> <code>None</code> <code>private_key</code> <code>bytes | None</code> <p>Your private RSA key. This argument should only be set when reading from a secrets manager or testing, otherwise it is recommended to set the SPINDLE_TOKEN_PRIVATE_KEY environment variable with your private key.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The <code>DataFrame</code> with the original encrypted tokens re-encrypted for sending to the recipient.</p>"},{"location":"carduus/api/#carduus.token.transcrypt_in","title":"carduus.token.transcrypt_in","text":"<pre><code>transcrypt_in(df, token_columns, private_key=None)\n</code></pre> <p>Used by the recipient of a <code>DataFrame</code> containing tokens in the intermediate representation produced by <code>transcrypt_out</code> to re-encrypt the tokens such that they will match with other datasets</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Spark <code>DataFrame</code> with token columns to transcrypt.</p> required <code>token_columns</code> <code>Iterable[str]</code> <p>The collection of column names that correspond to tokens.</p> required <code>private_key</code> <code>bytes | None</code> <p>Your private RSA key. This argument should only be set when reading from a secrets manager or testing, otherwise it is recommended to set the SPINDLE_TOKEN_PRIVATE_KEY environment variable with your private key.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The <code>DataFrame</code> with the original encrypted tokens re-encrypted for sending to the destination.</p>"},{"location":"carduus/api/#carduus.token.TokenSpec","title":"<code>carduus.token.TokenSpec</code>  <code>dataclass</code>","text":"<p>An collection of PII fields that will be encrypted together to create a token.</p> <p>For an enum of standard <code>TokenSpec</code> instances that comply with the Open Privacy Preserving Record Linkage protocol see <code>OpprlToken</code>.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the column that holds these tokens.</p> <code>fields</code> <code>Iterable[str]</code> <p>The PII fields to encrypt together to create token values.</p>"},{"location":"carduus/api/#opprl-implementation","title":"OPPRL Implementation","text":"<p>Although carduus is designed to be extensible, most users will want to use the tokenization procedure proposed by the Open Privacy Preserving Record Linkage (OPPRL) protocol. This open specification proposes standard ways of normalizing, enhancing, and encrypting data such that all user across all OPPRL implementations, including carduus, can share data between trusted parties.</p> <p>The following two <code>enum</code> objects provide <code>PiiTransform</code> instances and <code>TokenSpec</code> instances that comply with OPPRL. These can be passed to the column mapping and token set arguments of <code>tokenize</code> respectively.</p>"},{"location":"carduus/api/#carduus.token.OpprlPii","title":"<code>carduus.token.OpprlPii</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum of PiiTransform objects for the PII fields supported by the Open Privacy Preserving Record Linkage specification.</p> <p>Attributes:</p> Name Type Description <code>first_name</code> <code>NameTransform</code> <p>PiiTransform implementation for a subject's first name according to the OPPRL standard.</p> <code>middle_name</code> <code>NameTransform</code> <p>PiiTransform implementation for a subject's middle name according to the OPPRL standard.</p> <code>last_name</code> <code>NameTransform</code> <p>PiiTransform implementation for a subject's last (aka family) name according to the OPPRL standard.</p> <code>gender</code> <code>GenderTransform</code> <p>PiiTransform implementation for a subject's gender according to the OPPRL standard.</p> <code>birth_date</code> <code>DateTransform</code> <p>PiiTransform implementation for a subject's date of birth according to the OPPRL standard.</p>"},{"location":"carduus/api/#carduus.token.OpprlToken","title":"<code>carduus.token.OpprlToken</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum of <code>TokenSpec</code> objects that meet the Open Privacy Preserving Record Linkage tokenization specification.</p> <p>Attributes:</p> Name Type Description <code>opprl_token_1</code> <p>Token #1 from the OPPRL specification. Creates tokens based on <code>first_initial</code>, <code>last_name</code>, <code>gender</code>, and <code>birth_date</code>.</p> <code>opprl_token_2</code> <p>Token #2 from the OPPRL specification. Creates tokens based on <code>first_soundex</code>, <code>last_soundex</code>, <code>gender</code>, and <code>birth_date</code>.</p> <code>orrpl_token_3</code> <p>Token #3 from the OPPRL specification. Creates tokens based on the <code>first_metaphone</code>, <code>last_metaphone</code>, <code>gender</code>, and <code>birth_date</code>.</p>"},{"location":"carduus/api/#encryption-key-management","title":"Encryption Key Management","text":"<p>See the encryption key section of the \"Getting started\" guide for details about how Carduus accesses encryption keys.</p>"},{"location":"carduus/api/#carduus.keys.generate_pem_keys","title":"carduus.keys.generate_pem_keys","text":"<pre><code>generate_pem_keys(key_size=2048)\n</code></pre> <p>Generates a fresh RSA key pair.</p> <p>Parameters:</p> Name Type Description Default <code>key_size</code> <code>int</code> <p>The size (in bits) of the key.</p> <code>2048</code> <p>Returns:</p> Type Description <code>tuple[bytes, bytes]</code> <p>A tuple containing the private key and public key bytes. Both in the PEM encoding.</p>"},{"location":"carduus/api/#interfaces","title":"Interfaces","text":"<p>Carduus offers interfaces that can be extended by the user to add additional behaviors to the tokenization and transcryption processes.</p> <p>The <code>PiiTransform</code> abstract base class can be extended to add support for custom PII attributes, normalizations, and enhancements. See the custom PII guide for more details.</p>"},{"location":"carduus/api/#carduus.token.PiiTransform","title":"<code>carduus.token.PiiTransform</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for normalization and enhancement of a specific PII attribute.</p> <p>Intended to be extended by users to add support for building tokens from a custom PII attribute.</p>"},{"location":"carduus/api/#carduus.token.PiiTransform.normalize","title":"<code>normalize(column, dtype)</code>","text":"<p>A normalized representation of the PII column.</p> <p>A normalized value has eliminated all representation or encoding differences so all instances of the same logical values have identical physical values. For example, text attributes will often be normalized by filtering to alpha-numeric characters and whitespace, standardizing to whitespace to the space character, and converting all alpha characters to uppercase to ensure that all ways of representing the same phrase normalize to the exact same string.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>The spark <code>Column</code> expression for the PII attribute being normalized.</p> required <code>dtype</code> <code>DataType</code> <p>The spark <code>DataType</code> object of the <code>column</code> object found on the <code>DataFrame</code> being normalized. Can be used to delegate to different normalization logic based on different schemas of input data. For example, a subject's birth date may be a <code>DateType</code>, <code>StringType</code>, or <code>LongType</code> on input data and thus requires corresponding normalization into a <code>DateType</code>.</p> required <p>Returns:</p> Type Description <code>Column</code> <p>The normalized version of the PII attribute.</p>"},{"location":"carduus/api/#carduus.token.PiiTransform.enhancements","title":"<code>enhancements(column)</code>","text":"<p>A collection of PII attributes that can be automatically derived from a given normalized PII attribute</p> <p>If an implementation of PiiTransform does not override this method, it is assumed that no enhancements can be derived</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>The normalized PII column to produce enhancements from.</p> required Return <p>A <code>dict</code> with keys that correspond to the PII attributes of the ___ and values that correspond to the <code>Column</code> expression that produced the new PII from a normalized input attribute.</p>"},{"location":"carduus/cli/","title":"Carduus CLI","text":""},{"location":"carduus/cli/#command-line-interface","title":"Command Line Interface","text":"<p>The spindle-token command line interface (CLI) offers tokenization and transcription capabilities of data files from the local file system.</p>"},{"location":"carduus/cli/#usage-guide","title":"Usage Guide","text":"<p>The spindle-token CLI is included with every installation of the spindle-token library. Install spindle-token to your python (virtual) environment using <code>pip</code>. See our getting started guide for more information. Make sure the python interpreter directory is on your PATH.</p> <p>You can test your installation and environment setup by running the <code>--help</code> command. You should see documentation about the spindle-token CLI.</p> <pre><code>spindle-token --help\n</code></pre> <p>Once installed, you can run the commands for <code>tokenize</code> and <code>transcrypt</code> with the relevant options and arguments. All commands and sub-commands follow the same general design. Positional arguments and paths the the input data and desired location to write output data. Options configure how the input data is transformed. For example, options dictate which tokens should be generated, which file format to use, and the encryption key.</p> <p>This example invocation of the <code>tokenize</code> command illustrates the general pattern.</p> <pre><code>spindle-token tokenize \\\n    --token opprl_token_1 --token opprl_token_2 --token opprl_token_3 \\\n    --key private_key.pem \\\n    --format csv \\\n    --parallelism 1 \\\n    pii.csv tokens.csv\n</code></pre>"},{"location":"carduus/cli/#encryption-keys","title":"Encryption Keys","text":"<p>The OPPRL protocol leaves the responsibility of encryption key management to the user. The spindle-token CLI assumes the public and private keys are stored in files on the local filesystem. The location of the PEM file can be passed using the corresponding option or an environment variable. This table describes the option names and environment variables that can be use to supply private and public keys respectively.</p> <p>Private Key</p> <p>The private RSA key can be set using one of the following methods:</p> <ul> <li>Use the <code>--key</code> option (or <code>-k</code> alias) to specify a path to a PEM file.</li> <li>Set the <code>SPINDLE_TOKEN_PRIVATE_KEY_FILE</code> environment variable to specify a path the PEM file.</li> <li>Set the <code>SPINDLE_TOKEN_PRIVATE_KEY</code> environment variable to specify the key as a UTF-8 string. If both environment variables are set, the <code>_FILE</code> variant takes precedence.</li> </ul> <p>Public Keys</p> <p>The public keys of data recipients (used in transcryption) can be set using one of the following methods:</p> <ul> <li>Use the <code>--recipient</code> option (or <code>-r</code> alias) to specify a path to a PEM file.</li> <li>Set the <code>SPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY_FILE</code> environment variable to specify a path the PEM file.</li> <li>Set the <code>SPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY</code> environment variable to specify the key as a UTF-8 string. If both environment variables are set, the <code>_FILE</code> variant takes precedence.</li> </ul>"},{"location":"carduus/cli/#file-formats","title":"File Formats","text":"<p>The spindle-token CLI supports multiple data file formats. The recommended file format is parquet because parquet files are efficient, compressed, and have unambiguous schemas. The spindle-token CLI also supports CSV files. The file format must be specified using the <code>--format</code> option or <code>SPINDLE_TOKEN_FORMAT</code> environment variable.</p> <p>When using CSV files there are a few assumptions that the data file(s) must meet. </p> <ol> <li> <p>The first row of each CSV file must be column headers. See the next section for expectation on the column names.</p> </li> <li> <p>The field separate (aka delimiter) should be a pipe <code>|</code> character.</p> </li> </ol> <p>Input datasets can either be a single data file or partitioned into a directory of multiple data files. Splitting larger datasets into multiple data files can help the CLI parallelize over larger datasets. If the input dataset is a single data file the output dataset will be single data file. Similarly, if the input dataset is a directory the argument for the output location will be a directory that will include a partitioned output dataset.</p>"},{"location":"carduus/cli/#column-names","title":"Column Names","text":"<p>THe OPPRL tokenization protocol requires specific PII attributes to be normalized, transformed, concatenated, hashed, and then encrypted together. Thus, the spindle-token CLI must know which columns of the input dataset correspond to each logical PII attribute (first name, last name, birth date, etc).</p> <p>The spindle-token CLI requires specific column naming for PII columns so that the proper normalization rules are applied to each attribute and the final tokens are created from the correct subset of inputs. The following list contains the exact column names the CLI will look for.</p> <ul> <li><code>first_name</code></li> <li><code>last_name</code></li> <li><code>gender</code></li> <li><code>birth_date</code></li> </ul> <p>If you are only adding tokens that require a subset of these PII fields, the input dataset may omit columns for the other PII attributes that are not required. For information on which PII attributes are required for each token in the OPPRL protocol, see the official specification.</p>"},{"location":"carduus/cli/#parallelism","title":"Parallelism","text":"<p>The spindle-token CLI supports multi-threaded parallelism. This helps work with larger datasets that are partitioned into multiple part files within a directory. If <code>--parallelism</code> is set to a number &gt;1 then that number of partition files will be processed at once. This option can also be set with the <code>SPINDLE_TOKEN_PARALLELISM</code> environment variable. If parallelism is not provided, the spindle-token will default to using the same number of threads as the host machine has logical cores.</p>"},{"location":"carduus/cli/#commands","title":"Commands","text":"<p>The help text, options, and arguments of every command and sub-command of the spindle-token CLI. You can get this documentation for the specific version of the CLI installed in your python environment using the <code>--help</code> option on any command or sub-command.</p>"},{"location":"carduus/cli/#spindle-token-tokenize","title":"spindle-token tokenize","text":"<p>Add tokens to a dataset of PII.</p> <p>Creates a dataset at the OUTPUT location that adds encrypted OPPRL tokens to the INPUT dataset. Does not modify the INPUT dataset.</p> <p>INPUT is the path to the dataset to tokenize. If INPUT is a file, it must be of the format provided to the <code>--format</code> option. If INPUT is a directory, all files within the directory that match the given format will be considered a partition of the dataset.</p> <p>OUTPUT is the file or directory in which the tokenized dataset will be written. If INPUT is a file, the OUTPUT will be written to as a file. If INPUT is a directory, the OUTPUT will be a directory containing a dataset partitioned into files.</p> <p>Usage:</p> <pre><code>spindle-token tokenize [OPTIONS] INPUT OUTPUT\n</code></pre> <p>Options:</p> <pre><code>  -t, --token [opprl_token_1|opprl_token_2|opprl_token_3]\n                                  An OPPRL token to add to the dataset. Can be\n                                  passed multiple times.  [required]\n  -k, --key FILENAME              The PEM file containing your private key.\n  -f, --format [parquet|csv]      The file format of input and output data\n                                  files.  [required]\n  -p, --parallelism INTEGER       The number of worker threads to parallelize\n                                  over. Useful when the input dataset is\n                                  partitioned into multiple part files. If not\n                                  supplied, defaults to the number of logical\n                                  cores.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"carduus/cli/#spindle-token-transcrypt","title":"spindle-token transcrypt","text":"<p>Prepare tokenized datasets to be sent or received.</p> <p>Usage:</p> <pre><code>spindle-token transcrypt [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre>"},{"location":"carduus/cli/#in","title":"in","text":"<p>Convert a dataset of ephemeral tokens into tokens.</p> <p>INPUT is the path to the dataset of ephemeral tokens to create tokens from. If INPUT is a file, it must be of the format provided to the <code>--format</code> option. If INPUT is a directory, all files within the directory that match the given format will be run.</p> <p>OUTPUT is the file or directory in which the tokenized dataset will be written. If INPUT is a file, the OUTPUT will be written to as a file. If INPUT is a directory, the OUTPUT will be a directory containing a dataset partitioned into files.</p> <p>Usage:</p> <pre><code>spindle-token transcrypt in [OPTIONS] INPUT OUTPUT\n</code></pre> <p>Options:</p> <pre><code>  -t, --token [opprl_token_1|opprl_token_2|opprl_token_3]\n                                  The column name of an OPPRL token on the\n                                  input data to transcrypt.  [required]\n  -k, --key FILENAME              The PEM file containing your private key.\n  -f, --format [parquet|csv]      The file format of input and output data\n                                  files.  [required]\n  -p, --parallelism INTEGER       The number of worker threads to parallelize\n                                  over. Useful when the input dataset is\n                                  partitioned into multiple part files. If not\n                                  supplied, defaults to the number of logical\n                                  cores.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"carduus/cli/#out","title":"out","text":"<p>Prepare ephemeral tokens for a specific recipient.</p> <p>INPUT is the path to the dataset of tokens to create ephemeral tokens from. If INPUT is a file, it must be of the format provided to the <code>--format</code> option. If INPUT is a directory, all files within the directory that match the given format will be considered a partition of the dataset.</p> <p>OUTPUT is the file or directory in which the tokenized dataset will be written. If INPUT is a file, the OUTPUT will be written to as a file. If INPUT is a directory, the OUTPUT will be a directory containing a dataset partitioned into files.</p> <p>Usage:</p> <pre><code>spindle-token transcrypt out [OPTIONS] INPUT OUTPUT\n</code></pre> <p>Options:</p> <pre><code>  -t, --token [opprl_token_1|opprl_token_2|opprl_token_3]\n                                  The column name of an OPPRL token on the\n                                  input data to transcrypt.  [required]\n  -r, --recipient FILENAME        The PEM file containing the recipients\n                                  public key.\n  -k, --key FILENAME              The PEM file containing your private key.\n  -f, --format [parquet|csv]      The file format of input and output data\n                                  files.  [required]\n  -p, --parallelism INTEGER       The number of worker threads to parallelize\n                                  over. Useful when the input dataset is\n                                  partitioned into multiple part files. If not\n                                  supplied, defaults to the number of logical\n                                  cores.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"carduus/cli/#limitations","title":"Limitations","text":"<p>The following limitations of the spindle-token CLI are. For a superior experience, consider using spindle-token as a Python library. If your use case requires addressing some of these limitations, please open an issue with additional details. </p>"},{"location":"carduus/cli/#no-horizontal-scaling","title":"No Horizontal Scaling","text":"<p>The spindle-token CLI is built with Apache Spark to allow for data parallelism. Spark is designed to distribute workloads horizontally across a cluster of multiple machines connected to the same network. The spindle-token CLI runs spark in \"local\" mode that switches execution to a multi-threaded design on a single host machine.</p> <p>The spindle-token CLI cannot be passed to <code>spark-submit</code>, nor is there currently a way to pass spark connect information to a remote spark cluster. If you would like to use spindle-token on a Spark cluster, it is recommended that you use the spindle-token Python library.</p>"},{"location":"carduus/cli/#no-remote-file-systems","title":"No Remote File Systems","text":"<p>The spindle-token CLI reads and writes files using the local file system. This means there is no native support for files stored on remote filesystems, like S3. </p> <p>You may be able to work with remote file systems if you have a method of mounting the remote file system to the local file system.</p> <p>If you require working with datasets on remote filesystems like S3, it is recommended that you use the spindle-token Python library and configure pyspark to read from S3.</p>"},{"location":"carduus/guides/custom-tokens/","title":"Defining Custom Token Specifications","text":"<p>In essence, a token is simply the concatenation of multiple PII fields into a single string of text which is passed through a hash and a subsequent cryptographic function. Carduus provides specifications of the tokens proposed in the OPPRL protocol. These tokens are known to have low false positive and false negative match rates for subjects whose PII is close to the distribution found in the United States population, while only relying on a minimal set of PII attributes that are commonly populated in real-world datasets.</p> <p>Some users may want to create tokens by concatenating different PII fields than what is proposed by OPPRL. This is often the case when the datasets of a particular user and the parties they share data with have additional PII beyond what OPPRL leverages that will lead to lower match errors for the sample of subjects used their use case.</p> <p>To declare a custom token specification, simply instantiate the <code>TokenSpec</code> class provided by Carduus and provide it with a token name and a collection of columns to jointly encrypt.</p> <pre><code>from carduus.token import TokenSpec\n\nmy_token = TokenSpec(name=\"geo_token\", fields=[\"last_name\", \"birth_date\", \"zipcode\"])\n</code></pre> <p>This <code>my_token</code> object can be passed to the <code>tokenize</code> function to replace the PII columns with a <code>geo_token</code> column. If the PII data does not have one or more of the fields referenced by the token definition (after normalization and enhancement) the <code>tokenize</code> function will throw an exception prior to performing any significant workloads.</p>"},{"location":"carduus/guides/databricks/","title":"Carduus on Databricks","text":"In\u00a0[0]: Copied! <pre>pii = spark.createDataFrame(\n    [\n        (1, \"Jonas\", \"Salk\", \"male\", \"1914-10-28\"),\n        (1, \"jonas\", \"salk\", \"M\", \"1914-10-28\"),\n        (2, \"Elizabeth\", \"Blackwell\", \"F\", \"1821-02-03\"),\n        (3, \"Edward\", \"Jenner\", \"m\", \"1749-05-17\"),\n        (4, \"Alexander\", \"Fleming\", \"M\", \"1881-08-06\"),\n    ],\n    (\"label\", \"first_name\", \"last_name\", \"gender\", \"birth_date\"),\n)\ndisplay(pii)\n</pre> pii = spark.createDataFrame(     [         (1, \"Jonas\", \"Salk\", \"male\", \"1914-10-28\"),         (1, \"jonas\", \"salk\", \"M\", \"1914-10-28\"),         (2, \"Elizabeth\", \"Blackwell\", \"F\", \"1821-02-03\"),         (3, \"Edward\", \"Jenner\", \"m\", \"1749-05-17\"),         (4, \"Alexander\", \"Fleming\", \"M\", \"1881-08-06\"),     ],     (\"label\", \"first_name\", \"last_name\", \"gender\", \"birth_date\"), ) display(pii) labelfirst_namelast_namegenderbirth_date1JonasSalkmale1914-10-281jonassalkM1914-10-282ElizabethBlackwellF1821-02-033EdwardJennerm1749-05-174AlexanderFlemingM1881-08-06 In\u00a0[0]: Copied! <pre>%pip install carduus\n</pre> %pip install carduus In\u00a0[0]: Copied! <pre>from carduus.token import tokenize, OpprlPii, OpprlToken\n\ntokens = tokenize(\n    pii,\n    pii_transforms=dict(\n        first_name=OpprlPii.first_name,\n        last_name=OpprlPii.last_name,\n        gender=OpprlPii.gender,\n        birth_date=OpprlPii.birth_date,\n    ),\n    tokens=[OpprlToken.token1, OpprlToken.token2, OpprlToken.token3],\n    private_key=dbutils.secrets.getBytes(\"carduus\", \"PrivateKey\"),\n)\ndisplay(tokens)\n</pre> from carduus.token import tokenize, OpprlPii, OpprlToken  tokens = tokenize(     pii,     pii_transforms=dict(         first_name=OpprlPii.first_name,         last_name=OpprlPii.last_name,         gender=OpprlPii.gender,         birth_date=OpprlPii.birth_date,     ),     tokens=[OpprlToken.token1, OpprlToken.token2, OpprlToken.token3],     private_key=dbutils.secrets.getBytes(\"carduus\", \"PrivateKey\"), ) display(tokens) labelfirst_namelast_namegenderbirth_dateopprl_token_1opprl_token_2opprl_token_31JONASSALKM1914-10-28t+Yg6k4aOm5xMOMjT1nUCVbw1xM6mITKRx/APB+oU0dNo/AN2q/p20Pu2fiKd4wX5iFVK119DJAHYkFJYuI1BxgLBrzkiQKdEJKn1kMzA6k=AWkmGR88bq2Fm/OJpcliqpYxYl43BQETABQT53Y4F8m1nJBnsPJIrQECXjC2qwiX47TI78GHDTuNNu2Sw1r6UAYILDQyFu9swKwcwf99C68=Njlxk7k6GnIHAeV5rK43FOas0JrIB7SYd8xkRRhJikvR+OxYSmpwJyHM5tEc25+srlFOekqx6MTPDMQwUMQ/wxx33ETla7Q3po5Ypjf8Z7g=1JONASSALKM1914-10-28t+Yg6k4aOm5xMOMjT1nUCVbw1xM6mITKRx/APB+oU0dNo/AN2q/p20Pu2fiKd4wX5iFVK119DJAHYkFJYuI1BxgLBrzkiQKdEJKn1kMzA6k=AWkmGR88bq2Fm/OJpcliqpYxYl43BQETABQT53Y4F8m1nJBnsPJIrQECXjC2qwiX47TI78GHDTuNNu2Sw1r6UAYILDQyFu9swKwcwf99C68=Njlxk7k6GnIHAeV5rK43FOas0JrIB7SYd8xkRRhJikvR+OxYSmpwJyHM5tEc25+srlFOekqx6MTPDMQwUMQ/wxx33ETla7Q3po5Ypjf8Z7g=2ELIZABETHBLACKWELLF1821-02-03E4G7QvnP21Q7Wp71rTE7uMWEkB03SrFuJ4rhLBPrAx4IS41dAYLWIRsPeGkzlgmVmYrCa3rrABZPvAsKnUAeE+FHpJhUPacdfDYsBQuGu5I=VTY0gtsgUZoIxfoFXzwchDqFkofPZdylJlszvW8g1OoFXCrpCFzLLxk0SdNkEXTBdNof0lA1Dqk9NPIBOZcW8UfcQjGlb66Jo4TRvc9kXUo=bOTIQg+gcOQ0NToyORzUJrf64oRMlHHRZBQPDueJzHBl1tIzq6l9Y/plH1N9K7Gy3FVSPsqNl+6SWpkgTWKinJKEQLt2wiCVbLgAw86Vc/U=3EDWARDJENNERM1749-05-174wGL1/yszKMjW7E3d2pk5fTcWLAKDp/4Dc03sIL0DG27Cu+QZUYYMf8pIyBwL/6ZMPhmr2X9QsOLTaq05ri09g6zB8HhCDuHUtRavdM0x1w=4j8xonF4Lcu8dpzcKwaw9UD1Lxuz5AcZv/fOnHwZhVifVDsWNEKVvyem3s4zqb8t6WeNSsC/rd6MYWKtZp5uvbq4pnqmC4gf1cwJa1E7dc8=7XIFi7mFqbvKK5NhZHSz6ZWIoL7ZyOP2QtHxSfhwwhThSnfe2deudM5hb+3RAc7r1Osb8wXd4mkqu9zlY1CUHc6HM2eah7TYCGHGTeLgRV0=4ALEXANDERFLEMINGM1881-08-06kxwqjyikxC7WNxuX7cHCZp+Q7XEG5IYuVEshHi4RsyLgjM3lHHhi60Z3v5IpxKznFc+QNdea0S1vUyqyXDcbmzP597l91ZR4da7+cqA2Z4Q=GD/IeZRqsRYWn/D8YfTw4FS+gT6bAM4oPvHlswcrc62n3KBH1ungFhTdHmYZXnOlsTVrFBluxcv5qUNCxDrxMgczN3StZZd+LS9YaSb+fsQ=00MN/EeN8M2ILTLLdgw0qNwjDfyBOROoKZydE05dXlPlv7Y+JVuoOpFdzlSu3KF4gTRhyw8B8qBWO7uwq5wAHS+poqtP/sHf4xqcCZ8WI4Y= In\u00a0[0]: Copied! <pre>from carduus.token import transcrypt_out\n\ntokens_to_send = transcrypt_out(\n    tokens, \n    token_columns=(\"opprl_token_1\", \"opprl_token_2\", \"opprl_token_3\"), \n    recipient_public_key=dbutils.secrets.getBytes(\"carduus\", \"MyPartnerPublicKey\"),\n    private_key=dbutils.secrets.getBytes(\"carduus\", \"PrivateKey\"),\n).drop(\"first_name\", \"last_name\", \"birth_date\")\ndisplay(tokens_to_send)\n</pre> from carduus.token import transcrypt_out  tokens_to_send = transcrypt_out(     tokens,      token_columns=(\"opprl_token_1\", \"opprl_token_2\", \"opprl_token_3\"),      recipient_public_key=dbutils.secrets.getBytes(\"carduus\", \"MyPartnerPublicKey\"),     private_key=dbutils.secrets.getBytes(\"carduus\", \"PrivateKey\"), ).drop(\"first_name\", \"last_name\", \"birth_date\") display(tokens_to_send) labelgenderopprl_token_1opprl_token_2opprl_token_31MfUxhkLG+G9blbk/zH7TFlIQy23OjevfCmYlb1yMS85NvcumQLLpKiz7QcTaFsWE9CB80gvzFI6i4m4By+iGy8xDTCSaxry7BkWr5L3jn2T5mcf1ZpSOtlNJcWS9Ts5rVbigsO8EsWJSTn61o78VbOb8YZ/2p7AT63dUqqCBM7pabv9MH2I2QIzjzwgEZ0C9BmRmsXkkZ4JMstaJu+lIWZyyMFzBKe471X5rRo4HL0FxT+VSMdXz/IFeNSAImJCtBglEfCMZhfCMqU1MKG+BsqXK1mhujmm20neIwJMOqnIueG3pWKXtcn/btcaFUnwzTs793L3lZt57mrHmiJTLLVw==U7uj6e8Y5qBanGmZsPvV36cA3zhoH/kWyTv/ffC+aRlrzjUyQe4jc9bJQ+UG9OsdOwcrcr1OI+EoovhqGBpJOCgWud4tWz9SPQMRh2Cf54qbG+ODYWCxKSTpPwJ7+SHPf+8iKebVEK7wT6iYaw6GS5OLqxYKL3hHcpZupV8bu5GZfzGpeqNl8XzwE1A/H+pwrusZziRbMgoRIaW49VnrKH3Z4R5GbH0zwUEZTSN3rguljNQgtG1RqL/njy/RtQo7wh7jusDA5GHvd7X0tCDCxN/VP6NkSPj5r/xLXvgcPQwLEbG1UxR7sUldFxzl0TnLIWJNmi3fJAAbSu10dP3pTQ==jGnVejN4ODaSh+V0jA1rOrj4lpFXYprnZ+SFP63oFWNjmDudsRo1KIGTxSXi977j8OXM6HTFBUMS3PzVGvr+ixDx7VuEEJ/QfiTI5XLDlyim71sXn1YzDn0hSDFPHEF39vAL5M5oii6uCEsdEroTccfOgWQj6J+F/GAPwL7otxa5X7S7ZPJqMDTQ3E8VmLQSFImwG19SaTdxcFFwFJ4bfR0DaIIvb8/e/RrRT4oHSqcoZDIHo4jHS/7DaM+QwA9/hulErOfVUKPDqtpBgs88sL/MwHgXtkNo2xsK0mQ4hlVkETbMv4amgrnFtIWAVUx/ngT6jt1dAtbH0P8x3p0V0w==1Mgk0V1PNQ8fsPtqOm4tPEQY0M1Dic4o3l1oO/cS3XTHdujoKOmAR1sDfdvrGboNfR3hmmLCgSfQc+n7kxvbGKkMIRfc/DMcBCaYgr+3vVyE+N8d5kXd6hZE1WrCdWOgj1prxk/70Hk53n05kLQxppLney2NSgkBj6FwrJ9VXrbCm4zqTAwlwWdNBweX+7qRhuMbxUiXkChx52OqgPGdpIvMFNAJVtL/B11/3hIgh3soUKncKXEv8HChs18wJbJHrHLRRwcZS19DTctaJkKQxqqqMOlzYHAGP6lVczQGIaUh7m8Y0JCfYNpkg+3YATRf1EOOmAmgrYYuJLt4l/BHVKWg==HsSe2Dj8OjRUFzWEaTBfd46mEY4kf/KBvNfwY+pe2btJ2ES6mTaXt4QS5bRw8HyMmazjUF8Hz0pBnZZGfO236mscDvv8TaK5cRrn5HROXBxVXdpb/fz8CumggXKobY33iuk8bD97NpA6R4fwTnphtQc8E9ofrW1tsecFHYJNFO4HefjlKaeS0x/dQ9bEU1iD/n7vtNZVBp5jNQuLktSjTer+tMACvdrDPLZFL5hOSa0nOuA32+g9GYQD5pZUJ6r4LM87IHXF+ggf/Q64/aeDzFVIGiznsqsKg/cUm9msrY/thcOvYXWjb1r1XND3d/z3zjInKcTZ75JgVwLoXQbVIQ==dJDc+FiR8PZ9eco5WUBNUQ/Mnp3WRBmJOgvDUwNi09fA32Q5CvOQtfGaV7fqK0meFdP2OM7/XBAr4ZZhvp7UG/SHiZkOYTQxyKUgEAFtuEf9MpUamX5fA4M11h/bE95YPfabkuw6R52Ghxmg3yvQCM8Mv8JZjYJU416ko88+TjaMv8TKGM5mRxhpfkfOrdAgUENFVCLAi9Q95Glu26Y8EXuVqHAYfdILFe9XTpaM9BtZbfTPgFs7yPAtO/RqHVRGMGnuGwSNLhBViN/QV1uUys5tuU5yFT9MzM9VBifwHIpiGvl0l+j9fwZclergwuf3e4vGuFMnQZmDCuMAdzRsbw==2FQXS7khXYHRoNrE5Oqpy04c9+PtrOrSrgGtzje8cMVDuBTH63gsJxqeHYKk1IXaiRcniFtDaQ4M4+lOuMlnPNhMihd5kxD4fMUarL5XffQtBFr8H3gQDRbSV0kxFPJHVqtXBWO2pNWe6DN/cYJR7XtGSwY0dyWXWTRhkt6YCc5DP6uah3OUbKQEzCgONYXdHDyn7qsdOprz6EU15e2xSwfrnfH8ysYfAAHU/gQKJhZXmXtPqt5vzkXSm6H2A+4n/+AY/Nd/Qz9kl4zrqzWsctyzqDHLtw01O4w60bNV4lekYLkCBOOManC5wHwAni3PxtE5XzA4HJqHXKiBCAWAzpLg==ef3MO6eLNr8wHv9CMYXSbbEj8e+CUwJMbVVQql0Hsj/3V99SXflIP/Bec7iOaeX/nymKyAHaDkDjimp8EZqeboPa9p8BVViW0n1Tor6XRW7WRsmHjwLcKyJ1IGgRk08rO6PdLTUcHRAQXtsvlJuFq3EabqZk3ZliZcmljpVij3MKsnQ8VVYKZYGz17yUzgUIOXMT1C3Ouw0SjSgViI5vEQj0nTY9NrZyIN5FS4v4tf0x/Lm71uXpiiVULGEm1eHpV+I1UiP0J2O/fIUd50LoDIfC0G8dZXSoak55Rc1HI8F2Y6dyavuxi2t/k+bF8A6fFuy7vk9WEsKPiyDZ8AcN6A==WD9jVu0cbXhq4p7NQbI2vhFCdvU4GI/kHUjTmXzp7OLY7BvEbTNxlDvrEbeIkoxm8y/fcygVk3ioMMhZ81pFy+MxE2XlfsjWvVCers2Ltw+ua2dTS14FKRp8MYiFj7516rYt81hPINBK8/DzW9jtXqUdDWdHiivZDUBz8xPK+hmUuPdm8ppWPNelHS6autmjb1jBB6bEwKK38MEEkZRbE6Uj/78usd9f7YV2+HcTRy2jg2V6LcdxW8i0fNb5cJAKzKCabGt6r9WLISz8VYKJPNFMgAegpnywaPqAoBgBqFaqp2F8q0g/UTp3uxrQjKEtSVJ9i9ggVuhAK+Gwcfs1XA==3MHpp7B0EB4UvIxDYRbRVHXnhZL2ZgLNh9ALK7VsoQX276BsC4CfqwC0BLchSnHNEAj/6YYT3qsqg4AGbab0RnPfAjmBj8i8pBmndZK2kicyUbVKhzXABO5kEjBVPCMz/7g6fIysLHczHg3kd3cibyAyQTKDkYyoVhnwxG9Izj0aKBi5HzqDAkKopDwz/I228fEggoN39Pd8pbjdx2+a0BEFvJeI7m9twRsd26GxBLrXuVH6EGAVu/vDUEge2f8gCT9CaFCQq5JbyLf+uqxDBU2/GHT0IcEENtHpFCFNP3JHtnOPbICUPaZhpkc5u0TEz7O41oklslvQ/w8fEh7aNnfg==T/DoAbgx27LunmbwlY3WcTbAwYJRQeOEx6E0KSdvzM+n5dhMB39uWAdu9qilR4xnqw2+Vbt1dP+/M4q7tTOo00w990IVdHfaPAOTU/53YmjgkKqT/VzG9MZYIx8tqr3MTSmvwOtzz4yXKF3mtCg+xWI09zP96WYktLsbZKenZwNsHXtyAa3Zl5/FAas4Q8CTFg3F8xN+joKZCyWZVnnTRTG5SCyJW7M1dlr+nMMm9Awee6NvpgYBXHKfLWwk+aMJxpcR/311RUvQUEMuc52L7hsAHF+t8r81mIysGIuuKaCAR29KtYKNmYA2dxI0hJfs1lvLOmAVfxm6lSifAsgg0w==Ya2uz/a8dEV7D4X95pd/qfUEmkb8nMIEsTzCCdwHm6OkkNO++O0bjAlCuCeMcNVxAEBw6RtW3QGZH87/j3EWCzyUEGSS/ZRQbfcDt/BSx6YCEjA/ZfEHrpKWw/fSoLq8kO8X3CNzkB3T40rCtbp3c+376Qg0EswgquLvWxhLgBeRnDuM5jh/PKuvxPz3GBIP6JKVKy3L3tBdxv+tw2RqlUmbIh2/El3xT6K6jp0fOtcdI61tKY9B2mYoncGF9jyzZo8Mg/dr9OLbswhV6DLYeUbnJN8osT54EQ5dtdwZ6e62BsSSzzBQkpgrAA8IHMaiiPKhtMJJvr88HjHMN8zbrA==4Ma7ajv6ohrH3dZw12jt5CkSyOKhCqpvb0RGP4t8c58BWm0VrjbU2nzOwp1tXtJxOnH7uA1v7kyGbyv4sRY4X7rP3xzVVcgISygpdcfD+aF8ZdwTeHYHouwbgw8S6Ms0lIHNKKgZI74Wxj/m/WU6qt1ZVee8RYpb4a0O/5DbUBIC69Mg4QNHB0yjPfBXnf+b9ZdjfX58uQWEVAvLrE+hxEX73TEsOa5HVr+49FRdMq9zJy4YC8tjoOheRrhsb6oArIzZFqL9k2sc2SigJsdqWJMi8dAZFu2Xiw1nLPPWb/7WlyEcTjtu83BLfxh8Mif5xmh75FuoplA+6CRM863l7Zzw==K8wHTWPPDLBouAH0brBmbPLBMQyUWzPEB1ifobca6lWIwIoy/H0zPGvStngqOTfNcHkO00TkU691gGZbKP5PluCz11onslBJXYhATxx03WpbWXCai/uyb9fmXRwmWxFXNhrZ9gaVhPDhnQ8JuQT35bcP4XckcmNNOlz2z5KG11d6icJ4ExkdZWFDa/9KZqOekYWmbkZJB4Z6kFBfn1RmnsiLWIG+iBV4vcG9XDIu3xECt4cg69jog4nzRKXQXZsLsDAhCkZ2dyGTOXERlZp4H3+gklwuA7rUWG3/EpBFlzsbhnx4J7E40CGl3Rlq6PAu7oiMNoODsCFqyuRhIC2RwA==dk96GXnw9B5qaDHsOK90Dhg0ErenPL+9zY34ikYk+clTFFujgG9SJnKIhxYJQ1vsH5ExntoeY5BJdt5keUFZm4jN6XPI981p4eNdgtFPRdFddqVFBd/OotfDhoW9S9WtEKPKvKAbJZBmYhVXLvxKS95LkYhWeklIQQ4WxU4mEgVmgKQUUy115Wxy3N4jwFErJocrCnHPmfv4d18a0ch848ie1LXp3HnfRKaIWwNkZSxeDrofjHJxsGa3ETZcTPMHfYCdvkfC62mZKXBbVgZz2CPoD0D1iCN6wUeVHA3FpKg87mD30J5jVCoa93oq+LEBgMqgsJqrpxCdSO8wdT7Aaw== In\u00a0[0]: Copied! <pre>from carduus.token import transcrypt_in\n\ntokens2 = transcrypt_in(\n    tokens_to_send, \n    token_columns=(\"opprl_token_1\", \"opprl_token_2\", \"opprl_token_3\"),\n    private_key=dbutils.secrets.getBytes(\"carduus\", \"OrgBPrivateKey\"),\n)\ndisplay(tokens2)\n</pre> from carduus.token import transcrypt_in  tokens2 = transcrypt_in(     tokens_to_send,      token_columns=(\"opprl_token_1\", \"opprl_token_2\", \"opprl_token_3\"),     private_key=dbutils.secrets.getBytes(\"carduus\", \"OrgBPrivateKey\"), ) display(tokens2) labelgenderopprl_token_1opprl_token_2opprl_token_31Mn6YNZapNFyiP3rPO6JmlNZnxfz4PL9lfn3IOoERk9cSLe2w91EDdKHL+nNgZ60SzeRQzcM3nHDRQevzOfkFLf4dJTJg+8uLtcqHa3sHn0D4=gcbO8zyzdiMsWiWrChcskBcPHJ4z04CmZ4DrY+V20n6XaSpq8mJQ41um85gwSEcoOdIYE2ucFPn9mf3QKmNnBrRTYoA1q9KkSWK/Z6etbMg=AaGOVbbebDiOSq/lMVORAyeEHf8RcfrvPF7SCNei1wcSK7IJph22uxOPnfmmNNpAwzo9Q6Tr8XYOXwxgOYY7JG6NcX9O/1qVSGvEq3wb17E=1Mn6YNZapNFyiP3rPO6JmlNZnxfz4PL9lfn3IOoERk9cSLe2w91EDdKHL+nNgZ60SzeRQzcM3nHDRQevzOfkFLf4dJTJg+8uLtcqHa3sHn0D4=gcbO8zyzdiMsWiWrChcskBcPHJ4z04CmZ4DrY+V20n6XaSpq8mJQ41um85gwSEcoOdIYE2ucFPn9mf3QKmNnBrRTYoA1q9KkSWK/Z6etbMg=AaGOVbbebDiOSq/lMVORAyeEHf8RcfrvPF7SCNei1wcSK7IJph22uxOPnfmmNNpAwzo9Q6Tr8XYOXwxgOYY7JG6NcX9O/1qVSGvEq3wb17E=2FjSXy3EODCctLC6bYmp4PLwTyvIlruiSHVJG+PzkrffKsypBII409DuMDq+20mCoQIOnqZXqChkUNhrqiKPQX+E3wfFQWiJSEPk/HlHYKH7U=w1XiSj7vz4R68DpkQxMqSfFikQvylx3ZPrOoBQWBH3CypDgShSmw+6tC5Ows+hKVZkan+13letcC4u8ifO612dlg63hYZ/ONfTSGevD14ao=5Fk8udPfGm44jIDkFbhRJYIQnG4jjgpMhygEX7eHacpy1fCnuOMJB/5IP87570eISwYWqsRSuDna8iRqywrj8KTiVbMoY7p8Mg7Je3uFGKY=3M5etiSY9RzaVmpefeRGR0ajR08O6aez6ZrJUoAKrfIrNzOPHLAQqi4tXIp+4kvIhp4YygsXjleWirtfs8hbl0fwFWeGQ4vUBBsXUY0jG8MlU=WJ340Z+/qZy/5yTvABqJMYgrkTzZtXdHmKW/OduRiIpidBgVF8iijUfi/+M8nOKnUkgFbsCrqFtI57XP5GNuAvFmptyRS6EqPbOupF8PcNE=xnR2qS0zGdfBZrmBomMJezAPv/UYCQdpW0nFV7KiZotpYG5Q/d9EK5Ew27SaFP2dJNNVCwCEXCV+jM+RBXIuMnxbVWvy8FjPODx3UR5Qrrg=4MtfhWByB2vs7mTwVf1xQZ7n4KVf9U9JqdeINK6DJtMj/7msCQVAljCH+X58pbdjLfMDZlrMHZtiVjpl5i9nfrcj/iou5IClUnycIhupaT8JM=wBgFLcQj/QwyWe7YXrnHvNXoMQvNOmaBFi6q0gMbVSuO1f8xQk8fXoBN18/SH50amlmLs+4GV1eYatY+rvVEFVb4RJMSds91ZH/yjyLIkTw=/OfhKKqUTgAS+vABrSVG9sW0CrpiJmHDkWQ9fTOBAKASwsY9IDnBufIoKSs3SXjr24rYBvdFWMLZWAGfVukz91/yy39MsgeX7MJERxHigAQ= In\u00a0[0]: Copied! <pre>\n</pre>"},{"location":"carduus/guides/databricks/#carduus-databricks-getting-started-guide","title":"Carduus Databricks Getting Started Guide\u00b6","text":"<p>Carduus is an implementation of the Open Privacy Preserving Record Linkage (OPPRL) specification. It allows organizations to replace personally identifiable information (PII) with encrypted tokens that can be used to correlate data records pertaining to the same subject while keeping the subject anonymous.</p> <p>This notebook demonstrates the use of the <code>carduus</code> python package in databricks to parallelize the tokenization processes using Spark clusters.</p> <p>The following is a 5 row sample dataset of PII that will be tokenized. Notice that despite cosmetic differences the first 2 rows (label = 1) are describing the same subject and will receive the same sets of token values.</p>"},{"location":"carduus/guides/databricks/#one-time-workspace-setup","title":"One-time Workspace Setup\u00b6","text":"<p>See the Databricks documenation for managing secrets to get started. You will need an to install the Databricks CLI and authenticate as a user that can manage Databricks secrets.</p> <p>Create a secret scope for <code>carduus</code> encryption keys by running the following command with the Databricks CLI.</p> <pre><code>databricks secrets create-scope carduus\n</code></pre> <p>Add your private key as a secret in the <code>carduus</code> scope.</p> <pre><code>(cat &lt;&lt; EOF\n-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\nEOF\n) | databricks secrets put-secret carduus PrivateKey\n</code></pre> <p>Add the secrets for the public keys of the third party organizations you will be sending and receiving tokenized data to/from.</p> <pre><code>(cat &lt;&lt; EOF\n-----BEGIN PUBLIC KEY-----\n...\n-----END PUBLIC KEY-----\nEOF\n) | databricks secrets put-secret carduus MyPartnerPublicKey\n</code></pre>"},{"location":"carduus/guides/databricks/#optional-databricks-compute-setup","title":"Optional Databricks Compute Setup\u00b6","text":"<p>Users can provide encryption keys to Carduus via programmatic access to Databricks secrets (via <code>dbutils</code>) or by setting specific environment variables to take the value from Databricks secrets. See the Databricks secrets documentation for more information.</p> <p>Users who wish to use programmatic access via <code>dbutils</code> don't need any cluster configuration and can skip the remainder of this section.</p> <p>To configure your Databricks compute such that your encryption keys will be available as environment variables, use the following syntax in the Databricks Compute configuration UI. See the Databricks documentation on Environment Variables for more information.</p> <pre><code>SPINDLE_TOKEN_PRIVATE_KEY={{secrets/carduus/PrivateKey}}\nSPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY={{secrets/carduus/MyPartnerPublicKey}}\n</code></pre>"},{"location":"carduus/guides/databricks/#install-carduus","title":"Install carduus\u00b6","text":"<p>You can install <code>carduus</code> on your cluster, or within the notebook. See the Databricks docs for more information.</p>"},{"location":"carduus/guides/databricks/#tokenization-of-pii","title":"Tokenization of PII\u00b6","text":""},{"location":"carduus/guides/databricks/#transcrypt-tokens-as-sender","title":"Transcrypt Tokens As Sender\u00b6","text":"<p>trans- (latin: across) -crypt (greek: hidden, secret)</p> <p>The OPPRL specification also provides procedures for facilitating the safe transfer of data between trusted parties without a secure connection.</p> <p>The process of Transcryption is the re-encrypting of data from one scheme to another without reversing the initial encryption to recover the PII. When the sender transcrypts their tokens, the token values of the records no longer match to the sender's data and records within the data being shared no longer match with eachother regardless of the underlying PII.</p> <p>Notice that the first 2 records pertaining to the same subject (label = 1) no longer have identical tokens.</p>"},{"location":"carduus/guides/databricks/#transcrypt-tokens-as-recipient","title":"Transcrypt Tokens As Recipient\u00b6","text":"<p>When receiving the data with transcrypted tokens, the recipient must call a function to re-encrypt the tokens again so that they match with the rest of the tokens across their data assets.</p> <p>For this demonstration, we will load the encryption key our hypothetical recipient and process the transcrypted tokens we created above.</p> <p>Notice that the first 2 records pertaining to the same subject (label = 1) have identical tokens again, but do these tokens are not the same as the original tokens because they are encrypted with the scheme for the recipient.</p>"},{"location":"carduus/guides/getting-started/","title":"Getting Started","text":"<p>This guide provides a quick tour of the main features of Carduus, including instructions for how most users will be interacting with the library.</p> <p> The <code>carduus</code> package has been superseded by <code>spindle-token</code>, a newer Python package for generating and transcoding OPPRL tokens that supports a wider set of OPPRL protocol versions. It is recommended that all users migrate to <code>spindle-token</code>, however <code>carduus</code> remains a viable tool for generating token for OPPRL version 0.</p>"},{"location":"carduus/guides/getting-started/#installation","title":"Installation","text":"<p>Carduus is a cross-platform python library built on top of PySpark. It supports the following range of versions:</p> <ul> <li>Python: 3.10+</li> <li>PySpark: 3.5+</li> </ul> <p>The latest stable release of carduus can be installed from PyPI into your active Python environment using <code>pip</code>.</p> <pre><code>pip install carduus\n</code></pre> <p>You can also build Carduus from source using Poetry. The source code is hosted on Github. Checkout the commit you wish to build and run  <code>poetry build</code> in the project's root.</p>"},{"location":"carduus/guides/getting-started/#encryption-keys","title":"Encryption Keys","text":"<p>Carduus's tokenization capabilities require the use of private and public encryption keys. Carduus users are expected to manage their own encryption keys.</p> <p>There are 3 kinds of encryption keys that play different roles:</p> <ol> <li>Your private RSA key - Used to transcrypt incoming data and derive a symmetric encryption key used to tokenize PII. This key must never be shared or accessed by untrusted parties.</li> <li>Your public RSA key - The public key counterpart to your private key. This key will be shared with trusted parties that will be sending you tokenized data.</li> <li>Trusted partner public keys - A collection of public keys from the various trusted parties that you will be sending tokenized to.</li> </ol>"},{"location":"carduus/guides/getting-started/#configuring-encryption-keys","title":"Configuring Encryption Keys","text":"<p>Organization's often manage encryption keys using a secrets manager, such as AWS Secrets Manger, HashiCorp Vault, Databricks Secrets Manger, and EnvKey. It is recommended that Carduus users adopt a secrets manager in order to control which principals have access to encryption keys.</p> <p>To help encourage users to not hard-code encryption keys in their source code, Carduus will default to reading encryption keys from environment variables. Most secrets managers support the pattern of injecting secrets as ephemeral environment variables and the use of an <code>.env</code> file on developer workstations allow for easy local development. You can set the <code>SPINDLE_TOKEN_PRIVATE_KEY</code> environment variable with a private RSA keys in the PEM format.</p> <p>In some cases, it may be appropriate to explicitly pass the private keys as arguments to the relevant Carduus functions. For example, if your organization's secret manager encourages programmatic access at runtime or if you are overriding the encryption key within a test suite. Carduus function support the explicit passing of encryption keys as <code>bytes</code> but it is highly recommended that user do not hardcode their encryption keys into source code or check PEM files with production encryption keys into version control.</p> <p>This guide assumes that your private key is set via the <code>SPINDLE_TOKEN_PRIVATE_KEY</code> environment variable. For information about passing the private key as an explicit argument, see the Carduus API documentation. </p> <p>Public keys don't need to be managed as secrets. It is possible to specify the public key (in PEM format) for your intended data recipient with the <code>SPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY</code> environment variable or pass the public key to Carduus functions as an explicit argument. Users can pick whichever method is more convenient. </p>"},{"location":"carduus/guides/getting-started/#generating-new-keys","title":"Generating New Keys","text":"<p>Carduus expects encryption keys to be represented the PEM encoding. Private keys should use the PKCS #8 format and public keys should be formatted as SubjectPublicKeyInfo. Carduus recommends a key size of 2048 bits.</p> <p>You can generate these keys using tools like <code>openssl</code> or by calling the <code>generate_pem_keys</code> function provided by Carduus. This function will return a <code>tuple</code> containing 2 instances of <code>bytes</code>. The first is the PEM data for your private key that you must keep secret. The second is the PEM data for your public key that can may share with the parties you intend to receive data from.</p> <p>You can decode these keys into strings of text (using UTF-8) or write them into a <code>.pem</code> file for later use. </p> <pre><code>from carduus.keys import generate_pem_keys\n\nprivate, public = generate_pem_keys()  # Or provide key_size= \n\nprint(private.decode())\n# -----BEGIN PRIVATE KEY-----\n# ... [ Base64-encoded private key ] ...\n# -----END PRIVATE KEY-----\n\nprint(public.decode())\n# -----BEGIN PUBLIC KEY-----\n# ... [ Base64-encoded public key ] ...\n# -----END PUBLIC KEY-----\n</code></pre>"},{"location":"carduus/guides/getting-started/#tokenization","title":"Tokenization","text":"<p>Tokenization refers to the process of replacing PII with encrypted tokens using a one-way cryptographic function. Carduus implements the OPPRL tokenization protocol, which performs a standard set of PII normalizations and enhancements such that records pertaining to the same subject are more likely to receive the same token despite minor differences in PII representation across records. The OPPRL protocol hashes the normalized PII and then encrypts the hashes with a symmetric encryption based on a secret key derived from your private RSA key. In the event that the private encryption key of one party is compromised, there risk to all other parties is mitigated by the fact that everyone's tokens are encrypted with a different key.</p> <p>To demonstrate the tokenization process, we will use a dataset of 5 records shown below. Each record is assigned a <code>label</code> that corresponds to the true identity of the subject.</p> <pre><code>pii = spark.createDataFrame(\n    [\n        (1, \"Jonas\", \"Salk\", \"male\", \"1914-10-28\"),\n        (1, \"jonas\", \"salk\", \"M\", \"1914-10-28\"),\n        (2, \"Elizabeth\", \"Blackwell\", \"F\", \"1821-02-03\"),\n        (3, \"Edward\", \"Jenner\", \"m\", \"1749-05-17\"),\n        (4, \"Alexander\", \"Fleming\", \"M\", \"1881-08-06\"),\n    ],\n    (\"label\", \"first_name\", \"last_name\", \"gender\", \"birth_date\")\n)\npii.show()\n# +-----+----------+---------+------+----------+\n# |label|first_name|last_name|gender|birth_date|\n# +-----+----------+---------+------+----------+\n# |    1|     Jonas|     Salk|  male|1914-10-28|\n# |    1|     jonas|     salk|     M|1914-10-28|\n# |    2| Elizabeth|Blackwell|     F|1821-02-03|\n# |    3|    Edward|   Jenner|     m|1749-05-17|\n# |    4| Alexander|  Fleming|     M|1881-08-06|\n# +-----+----------+---------+------+----------+\n</code></pre> <p>To perform tokenization, call the <code>tokenize</code> function and pass it the PII <code>DataFrame</code>, a column mapping, a collection of specifications for token you want to generate, and the private key.</p> <pre><code>from carduus.token import tokenize, OpprlPii, OpprlToken\ntokens = tokenize(\n    pii,\n    pii_transforms=dict(\n        first_name=OpprlPii.first_name,\n        last_name=OpprlPii.last_name,\n        gender=OpprlPii.gender,\n        birth_date=OpprlPii.birth_date,\n    ),\n    tokens=[OpprlToken.token1, OpprlToken.token2, OpprlToken.token3],\n).drop(\"first_name\", \"last_name\", \"gender\", \"birth_date\")\n# +-----+--------------------+--------------------+--------------------+\n# |label|       opprl_token_1|       opprl_token_2|       opprl_token_3|\n# +-----+--------------------+--------------------+--------------------+\n# |    1|4YO6eFn0u75yrF+Td...|V6uRRgDgXylFsNM2c...|6N+/voOASNM0ivgA7...|\n# |    1|4YO6eFn0u75yrF+Td...|V6uRRgDgXylFsNM2c...|6N+/voOASNM0ivgA7...|\n# |    2|OLU6TLB4XdWmfhvIS...|3QtCKgeIqO20Jgp9E...|JrrjTxjy97Xe93afx...|\n# |    3|mks197t0d8Uhc3l3s...|YccsE4wMErZvz9oBd...|7wMcWiDjImAhuP307...|\n# |    4|kRZfUY8KScpiHKmxC...|1gKSJDcq6YtwdUc7C...|3h8jUydHTIcOU8jhW...|\n# +-----+--------------------+--------------------+--------------------+\n</code></pre> <p>Notice that both records with <code>label = 1</code> received the same pair of tokens despite slight representation differences in the original PII.</p> <p>The <code>pii_transforms</code> argument is a dictionary that maps column names from the <code>pii</code> DataFrame to the corresponding OPPRL PII field. This tells Carduus how to normalize and enhance the values found in that column. For example, the <code>OpprlPii.first_name</code> object will apply name cleaning rules to the values found in the <code>first_name</code> column and automatically derive additional PII columns called <code>first_initial</code> and <code>first_soundex</code> which are used to create both OPPRL tokens.</p> <p>The <code>tokens</code> argument is collection of OPPRL token specifications that tell Carduus which PII fields to jointly hash and encrypt to create each token. The current OPPRL protocol supports three token specifications, described below:</p> Token Fields to jointly encrypt <code>OpprlToken.token1</code> <code>first_initial</code>, <code>last_name</code>, <code>gender</code>, <code>birth_date</code> <code>OpprlToken.token2</code> <code>first_soundex</code>, <code>last_soundex</code>, <code>gender</code>, <code>birth_date</code> <code>OpprlToken.token3</code> <code>first_metaphone</code>, <code>last_metaphone</code>, <code>gender</code>, <code>birth_date</code> <p> Why multiple tokens? </p> <p>Each use case has a different tolerance for false positive and false negative matches. By producing multiple tokens for each record using PII attributes, each user can customize their match logic to trade-off between different kinds of match errors. Linking records that match on any token will result in fewer false negatives, and linking records that match all tokens will result in fewer false positives. User can design their own match strategies by using subsets of tokens.</p>"},{"location":"carduus/guides/getting-started/#transcryption","title":"Transcryption","text":"<p> noun: trans- (latin: across) -crypt (greek: hidden, secret)</p> <p>The process of transforming a ciphertext produced by one encryption into a ciphertext of a different encryption without emitting the original plaintext.</p> <p>Transcryption is performed when a user wants to share tokenized data with another party. The sender and recipient each have a corresponding transcryption function that must be invoked to ensure safe transfer of data between trusted parties. The sender performs transcryption to replace their tokens with \"ephemeral tokens\" that are specific to the transaction. In other words, the ephemeral tokens do not match the sender's tokens, the recipients data, or the ephemeral tokens from prior transactions between any sender and recipient. </p> <p>Furthermore, records from the same dataset that have identical PII will be assigned unique ephemeral tokens. This destroys the utility of the tokens until the recipient performs the reverse transcryption process using their private key. This is beneficial in the event that a third party gains access to the dataset during transfer (eg. an if transcrypted datasets are delivered over an insecure connection) because records pertaining to the same subject cannot be associated with each other.</p>"},{"location":"carduus/guides/getting-started/#sender","title":"Sender","text":"<p>Carduus provides the <code>transcrypt_out</code> function in for the sender to call on their tokenized datasets. In the following code snippet, notice the 2 records pertaining to the same subject (<code>label = 1</code>) no longer have identical tokens. The <code>tokens_to_send</code> DataFrame can safely be written files or a database and delivered to the recipient.</p> <pre><code>tokens_to_send = transcrypt_out(\n    tokens, \n    token_columns=(\"opprl_token_1\", \"opprl_token_2\", \"opprl_token_3\"), \n    recipient_public_key=b\"\"\"-----BEGIN PUBLIC KEY----- ...\"\"\",\n)\ntokens.to_send.show()\n# +-----+--------------------+--------------------+--------------------+\n# |label|       opprl_token_1|       opprl_token_2|       opprl_token_3|\n# +-----+--------------------+--------------------+--------------------+\n# |    1|EUqTdA0Yjb5F82oqj...|iiI/sd+sn+t5qhPDg...|JwpZVzJe+3CMocLGK...|\n# |    1|AXVDGgQ0KMa1ek1/v...|YJeYE9pz507CUzlks...|EKY2JdLhduCq+zj+p...|\n# |    2|XmYqnnMDqD9ZUR6yS...|A4e3L03NWeKbbL8vR...|CAZtbKDYjbbHsqVdc...|\n# |    3|mVhnJ1kz+ZRZvlfKo...|uO13H3LHjVnZ1flUp...|C16RKoV4SvWD5wuVp...|\n# |    4|oI/KS52K4H3M+WBdn...|vczsK9qlbU6TN1vBi...|fpTSrlGgtz3vASFXc...|\n# +-----+--------------------+--------------------+--------------------+\n</code></pre> <p>The <code>token_columns</code> argument is a iterable collection containing the column names of the <code>tokens</code> DataFrame that correspond to tokens that need to be transcrypted.</p> <p>The <code>recipient_public_key</code> argument is the public key provided by the intended recipient. </p>"},{"location":"carduus/guides/getting-started/#recipient","title":"Recipient","text":"<p>The <code>transcrypt_in</code> function provides the transcryption process for the recipient. It is called on a dataset produced by the sender using <code>transcrypt_out</code> to convert ephemeral tokens into normal tokens that will match other tokenized datasets maintained by the recipient, including prior datasets delivered from the same sender.</p> <p>Notice that the first 2 records pertaining to the same subject (label = 1) have identical tokens again, but do these tokens are not the same as the original tokens because they are encrypted with the scheme for the recipient.</p> <pre><code>from carduus.token import transcrypt_in\n\ntokens_received = transcrypt_in(\n    tokens_to_send, \n    token_columns=(\"opprl_token_1\", \"opprl_token_2\", \"opprl_token_3\"),\n)\ntokens_received.show()\n# +-----+--------------------+--------------------+--------------------+\n# |label|       opprl_token_1|       opprl_token_2|       opprl_token_3|\n# +-----+--------------------+--------------------+--------------------+\n# |    1|q1a5o2gg1hX+CFjdP...|gS2WMqSs0SReCN7Xp...|2XAgeBwomiKq2s4xO...|\n# |    1|q1a5o2gg1hX+CFjdP...|gS2WMqSs0SReCN7Xp...|2XAgeBwomiKq2s4xO...|\n# |    2|FEyzz52oI5hym0G/+...|UQDqblSWpqoH5znps...|ud2cG5YD/2yBhjZel...|\n# |    3|aEgXJZGUpkZxt7Z7T...|QZ5LaSiE8pMDmMKEu...|0yQxfv4EFAUHatIva...|\n# |    4|CgmXSDhY8mxZCRV+O...|wu+eNOqsOvh14Bjei...|7aGCrwy8tTDL3RkO6...|\n# +-----+--------------------+--------------------+--------------------+\n</code></pre> <p>As with <code>transcrypt_out</code>, the <code>token_columns</code> argument is a iterable collection containing the column names of the <code>tokens</code> DataFrame that correspond to tokens that need to be transcrypted. </p>"},{"location":"carduus/guides/getting-started/#deployment","title":"Deployment","text":"<p>Carduus is a Python library that uses PySpark to parallelize and distribute the tokenization and transcryption workloads. Your application that uses Carduus can be submitted to any compatible Spark cluster, or use a connection to a remote spark cluster. Otherwise, Carduus will start a local Spark process that uses the resources of the host machine.</p> <p>For more information about different modes of deployment, see the official Spark documentation.</p> <ul> <li>Submitting Applications to a Spark cluster</li> <li>Spark Connect</li> <li>Spark Standalone Mode</li> </ul>"},{"location":"carduus/guides/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Reference the full API</li> <li>Learn more about using Carduus and extending it's functionality from the advanced user guides:<ul> <li>Using Carduus on Databricks</li> <li>Defining custom token specifications</li> </ul> </li> </ul>"},{"location":"guides/custom-tokens/","title":"Custom Tokens","text":"<p>In essence, a token is simply the concatenation of multiple PII fields into a single string of text which is passed through a hash and a subsequent cryptographic function. The spindle-token library provides implementations of the standard tokens proposed in the OPPRL protocol. These tokens are known to have low false positive and false negative match rates for subjects whose PII is close to the distribution found in the United States population, while only relying on a minimal set of PII attributes that are commonly populated in real-world datasets.</p> <p>Some users may want to create tokens by concatenating different PII fields than what is proposed by OPPRL. This is often the case when the datasets of a particular user and the parties they share data with have additional PII beyond what OPPRL leverages that will lead to lower match errors for the sample of subjects used their use case. For a guide on how to extend spindle-token with support for additional PII attributes, see Custom PII Attributes.</p> <p>To create a custom token specification, simply instantiate the spindle_token.core.Token class and provide the required arguments.</p> <p>The following code example creates a new <code>Token</code> definition called <code>\"my_custom_token\"</code> that tokenizes the first initial, the metaphone phonetic encoding of the last name, and the birth date. All fields are combined, hashed, and encrypted according to v1 of the OPPRL protocol.</p> <pre><code>from spindle_token.core import Token\nfrom spindle_token.opprl import OpprlV1\n\nmy_token = Token(\n    \"my_custom_token\",\n    OpprlV1.protocol,\n    attributes=[\n        OpprlV1.first_name.initial,\n        OpprlV1.last_name.metaphone,\n        OpprlV1.birth_date,\n    ]\n)\n</code></pre> <p>The <code>name</code> attribute will be used as the the column name expected by the other spindle-token functions. Thus, it must only contain character that are safe in column names and each logically distinct <code>Token</code> instance should have a unique name. </p> <p>The <code>protocol</code> argument can be any implementation of TokenProtocolFactory but for the remainder of this section we will use the OPPRLv1 implementation provided by spindle-token. See this section for more details.</p> <p>The instances of PiiAttribute in the <code>attributes</code> argument determine the PII fields to jointly tokenize into a single token. This guide uses <code>PiiAttribute</code> objects for OPPRLv1 that are built-in to the spindle-health library. See Custom PII Attributes for a guide on defining your own PII.</p> <p>We can pass <code>my_token</code> to the core spindle-token functions -- such as <code>tokenize</code>, <code>transcode_out</code>, and <code>transcode_in</code> -- alongside the OPPRL tokens or other custom token specifications.</p>"},{"location":"guides/custom-tokens/#custom-pii-attributes","title":"Custom PII Attributes","text":"<p>The OPPRL provides specifications for normalizing and transforming some commonly used PII attributes to account for representation differences that often don't indicate a different true identity for the subject. For example, the OPPRL protocol calls for first names to have their capitalization changed to uppercase and strings of whitespace replaced with a single space character among other normalizations. Furthermore, a \"first initial\" attributes can be derived from a normalized first name attribute by taking the first letter.</p> <p>In some cases, users may be working with datasets that have PII attributes that aren't included in the OPPRL specification. In some scenarios, these additional PII attributes may be useful for creating custom tokens with the optimal match quality for your use case. Spindle-token provides the <code>PiiAttribute</code> that can be extended to add support for additional types of PII.</p> <p> Note</p> <p>Tokenization is often used in scenarios where datasets from different origins will be joined on token equality. Be sure all data origins are using the exact same PII attributes and normalization logic when creating tokens that will be compared. Using (even slightly) different PII attributes and normalization logic can negatively impact match quality or make matching impossible.</p> <p>The follow code example shows an implementation of <code>PiiAttribute</code> for a hypothetical PII attribute containing the first 3 digits of a US zipcode.</p> <pre><code>from pyspark.sql import Column\nfrom pyspark.sql.functions import lpad, substring, trim\nfrom pyspark.sql.types import DataType, StringType\nfrom spindle_token.core import PiiAttribute\n\nclass CustomZip3Attribute(PiiAttribute):\n\n    def __init__(self):\n        super().__init__(\"my-company.zip3\")\n\n    def transform(self, column: Column, dtype: DataType) -&gt; Column:\n        if not isinstance(dtype, StringType):\n            column = column.cast(StringType())\n        column = lpad(trim(column), 5, \"0\")\n        column = substring(column, 1, 3)\n        return column\n</code></pre> <p>The <code>__init__</code> method must call the <code>super()</code> constructor of <code>PiiAttribute</code> and pass an attribute identifier string. The attribute identifier is used uniquely identify the logical PII attribute expressed by the instance of <code>PiiAttribute</code>. In this case, all instances of <code>CustomZip3Attribute</code> use the same logic to produce the same normalized PII attribute (ie. zip3), therefore we hardcode <code>\"my-company.zip3\"</code> as the identifier. If <code>CustomZip3Attribute</code> was reused for multiple different PII attributes -- perhaps through some parameterization --  it would be required for the attribute ID to differ across instances.</p> <p>All built-in <code>PiiAttribute</code> classes for the OPPRL standard use the prefix <code>opprl.v{X}</code> where <code>{X}</code> is the major version number of the OPPRL protocol that specifies the normalization logic. It is recommended that implementers of <code>PiiAttribute</code> namespace their attribute IDs to avoid collision with other implementations of similar PII attributes.</p>"},{"location":"guides/custom-tokens/#custom-protocols","title":"Custom Protocols","text":"<p>The core of the OPPRL protocol is a set of specific data flows that produce tokens from sets of PII attributes using various cryptographic functions. In the spindle-token library, these data flows are provided by implementations of the <code>TokenProtocol</code> and <code>TokenProtocolFactory</code> interfaces.</p>"},{"location":"guides/databricks/","title":"Spindle Token on Databricks","text":"In\u00a0[0]: Copied! <pre>pii = spark.createDataFrame(\n    [\n        (1, \"Jonas\", \"Salk\", \"male\", \"1914-10-28\"),\n        (1, \"jonas\", \"salk\", \"M\", \"1914-10-28\"),\n        (2, \"Elizabeth\", \"Blackwell\", \"F\", \"1821-02-03\"),\n        (3, \"Edward\", \"Jenner\", \"m\", \"1749-05-17\"),\n        (4, \"Alexander\", \"Fleming\", \"M\", \"1881-08-06\"),\n    ],\n    (\"label\", \"first_name\", \"last_name\", \"gender\", \"birth_date\"),\n)\ndisplay(pii)\n</pre> pii = spark.createDataFrame(     [         (1, \"Jonas\", \"Salk\", \"male\", \"1914-10-28\"),         (1, \"jonas\", \"salk\", \"M\", \"1914-10-28\"),         (2, \"Elizabeth\", \"Blackwell\", \"F\", \"1821-02-03\"),         (3, \"Edward\", \"Jenner\", \"m\", \"1749-05-17\"),         (4, \"Alexander\", \"Fleming\", \"M\", \"1881-08-06\"),     ],     (\"label\", \"first_name\", \"last_name\", \"gender\", \"birth_date\"), ) display(pii) labelfirst_namelast_namegenderbirth_date1JonasSalkmale1914-10-281jonassalkM1914-10-282ElizabethBlackwellF1821-02-033EdwardJennerm1749-05-174AlexanderFlemingM1881-08-06 In\u00a0[0]: Copied! <pre>%pip install spindle-token\n</pre> %pip install spindle-token In\u00a0[0]: Copied! <pre>from spindle_token import tokenize\nfrom spindle_token.opprl import OpprlV1\n\ntokens = tokenize(\n    pii,\n    col_mapping={\n        OpprlV1.first_name: \"first_name\",\n        OpprlV1.last_name: \"last_name\",\n        OpprlV1.gender: \"gender\",\n        OpprlV1.birth_date: \"birth_date\",\n    },\n    tokens=[OpprlV1.token1, OpprlV1.token2, OpprlV1.token3],\n    private_key=dbutils.secrets.getBytes(\"spindle-token\", \"PrivateKey\"),\n)\ndisplay(tokens)\n</pre> from spindle_token import tokenize from spindle_token.opprl import OpprlV1  tokens = tokenize(     pii,     col_mapping={         OpprlV1.first_name: \"first_name\",         OpprlV1.last_name: \"last_name\",         OpprlV1.gender: \"gender\",         OpprlV1.birth_date: \"birth_date\",     },     tokens=[OpprlV1.token1, OpprlV1.token2, OpprlV1.token3],     private_key=dbutils.secrets.getBytes(\"spindle-token\", \"PrivateKey\"), ) display(tokens) labelfirst_namelast_namegenderbirth_dateopprl_token_1opprl_token_2opprl_token_31JONASSALKM1914-10-28t+Yg6k4aOm5xMOMjT1nUCVbw1xM6mITKRx/APB+oU0dNo/AN2q/p20Pu2fiKd4wX5iFVK119DJAHYkFJYuI1BxgLBrzkiQKdEJKn1kMzA6k=AWkmGR88bq2Fm/OJpcliqpYxYl43BQETABQT53Y4F8m1nJBnsPJIrQECXjC2qwiX47TI78GHDTuNNu2Sw1r6UAYILDQyFu9swKwcwf99C68=Njlxk7k6GnIHAeV5rK43FOas0JrIB7SYd8xkRRhJikvR+OxYSmpwJyHM5tEc25+srlFOekqx6MTPDMQwUMQ/wxx33ETla7Q3po5Ypjf8Z7g=1JONASSALKM1914-10-28t+Yg6k4aOm5xMOMjT1nUCVbw1xM6mITKRx/APB+oU0dNo/AN2q/p20Pu2fiKd4wX5iFVK119DJAHYkFJYuI1BxgLBrzkiQKdEJKn1kMzA6k=AWkmGR88bq2Fm/OJpcliqpYxYl43BQETABQT53Y4F8m1nJBnsPJIrQECXjC2qwiX47TI78GHDTuNNu2Sw1r6UAYILDQyFu9swKwcwf99C68=Njlxk7k6GnIHAeV5rK43FOas0JrIB7SYd8xkRRhJikvR+OxYSmpwJyHM5tEc25+srlFOekqx6MTPDMQwUMQ/wxx33ETla7Q3po5Ypjf8Z7g=2ELIZABETHBLACKWELLF1821-02-03E4G7QvnP21Q7Wp71rTE7uMWEkB03SrFuJ4rhLBPrAx4IS41dAYLWIRsPeGkzlgmVmYrCa3rrABZPvAsKnUAeE+FHpJhUPacdfDYsBQuGu5I=VTY0gtsgUZoIxfoFXzwchDqFkofPZdylJlszvW8g1OoFXCrpCFzLLxk0SdNkEXTBdNof0lA1Dqk9NPIBOZcW8UfcQjGlb66Jo4TRvc9kXUo=bOTIQg+gcOQ0NToyORzUJrf64oRMlHHRZBQPDueJzHBl1tIzq6l9Y/plH1N9K7Gy3FVSPsqNl+6SWpkgTWKinJKEQLt2wiCVbLgAw86Vc/U=3EDWARDJENNERM1749-05-174wGL1/yszKMjW7E3d2pk5fTcWLAKDp/4Dc03sIL0DG27Cu+QZUYYMf8pIyBwL/6ZMPhmr2X9QsOLTaq05ri09g6zB8HhCDuHUtRavdM0x1w=4j8xonF4Lcu8dpzcKwaw9UD1Lxuz5AcZv/fOnHwZhVifVDsWNEKVvyem3s4zqb8t6WeNSsC/rd6MYWKtZp5uvbq4pnqmC4gf1cwJa1E7dc8=7XIFi7mFqbvKK5NhZHSz6ZWIoL7ZyOP2QtHxSfhwwhThSnfe2deudM5hb+3RAc7r1Osb8wXd4mkqu9zlY1CUHc6HM2eah7TYCGHGTeLgRV0=4ALEXANDERFLEMINGM1881-08-06kxwqjyikxC7WNxuX7cHCZp+Q7XEG5IYuVEshHi4RsyLgjM3lHHhi60Z3v5IpxKznFc+QNdea0S1vUyqyXDcbmzP597l91ZR4da7+cqA2Z4Q=GD/IeZRqsRYWn/D8YfTw4FS+gT6bAM4oPvHlswcrc62n3KBH1ungFhTdHmYZXnOlsTVrFBluxcv5qUNCxDrxMgczN3StZZd+LS9YaSb+fsQ=00MN/EeN8M2ILTLLdgw0qNwjDfyBOROoKZydE05dXlPlv7Y+JVuoOpFdzlSu3KF4gTRhyw8B8qBWO7uwq5wAHS+poqtP/sHf4xqcCZ8WI4Y= In\u00a0[0]: Copied! <pre>from spindle_token import transcode_out\nfrom spindle_token.opprl import OpprlV1\n\nephemeral_tokens = transcode_out(\n    tokens, \n    tokens=(OpprlV1.token1, OpprlV1.token2, OpprlV1.token3), \n    recipient_public_key=dbutils.secrets.getBytes(\"spindle-token\", \"MyPartnerPublicKey\"),\n    private_key=dbutils.secrets.getBytes(\"spindle-token\", \"PrivateKey\"),\n).drop(\"first_name\", \"last_name\", \"birth_date\")\ndisplay(ephemeral_tokens)\n</pre> from spindle_token import transcode_out from spindle_token.opprl import OpprlV1  ephemeral_tokens = transcode_out(     tokens,      tokens=(OpprlV1.token1, OpprlV1.token2, OpprlV1.token3),      recipient_public_key=dbutils.secrets.getBytes(\"spindle-token\", \"MyPartnerPublicKey\"),     private_key=dbutils.secrets.getBytes(\"spindle-token\", \"PrivateKey\"), ).drop(\"first_name\", \"last_name\", \"birth_date\") display(ephemeral_tokens) labelgenderopprl_token_1opprl_token_2opprl_token_31MfUxhkLG+G9blbk/zH7TFlIQy23OjevfCmYlb1yMS85NvcumQLLpKiz7QcTaFsWE9CB80gvzFI6i4m4By+iGy8xDTCSaxry7BkWr5L3jn2T5mcf1ZpSOtlNJcWS9Ts5rVbigsO8EsWJSTn61o78VbOb8YZ/2p7AT63dUqqCBM7pabv9MH2I2QIzjzwgEZ0C9BmRmsXkkZ4JMstaJu+lIWZyyMFzBKe471X5rRo4HL0FxT+VSMdXz/IFeNSAImJCtBglEfCMZhfCMqU1MKG+BsqXK1mhujmm20neIwJMOqnIueG3pWKXtcn/btcaFUnwzTs793L3lZt57mrHmiJTLLVw==U7uj6e8Y5qBanGmZsPvV36cA3zhoH/kWyTv/ffC+aRlrzjUyQe4jc9bJQ+UG9OsdOwcrcr1OI+EoovhqGBpJOCgWud4tWz9SPQMRh2Cf54qbG+ODYWCxKSTpPwJ7+SHPf+8iKebVEK7wT6iYaw6GS5OLqxYKL3hHcpZupV8bu5GZfzGpeqNl8XzwE1A/H+pwrusZziRbMgoRIaW49VnrKH3Z4R5GbH0zwUEZTSN3rguljNQgtG1RqL/njy/RtQo7wh7jusDA5GHvd7X0tCDCxN/VP6NkSPj5r/xLXvgcPQwLEbG1UxR7sUldFxzl0TnLIWJNmi3fJAAbSu10dP3pTQ==jGnVejN4ODaSh+V0jA1rOrj4lpFXYprnZ+SFP63oFWNjmDudsRo1KIGTxSXi977j8OXM6HTFBUMS3PzVGvr+ixDx7VuEEJ/QfiTI5XLDlyim71sXn1YzDn0hSDFPHEF39vAL5M5oii6uCEsdEroTccfOgWQj6J+F/GAPwL7otxa5X7S7ZPJqMDTQ3E8VmLQSFImwG19SaTdxcFFwFJ4bfR0DaIIvb8/e/RrRT4oHSqcoZDIHo4jHS/7DaM+QwA9/hulErOfVUKPDqtpBgs88sL/MwHgXtkNo2xsK0mQ4hlVkETbMv4amgrnFtIWAVUx/ngT6jt1dAtbH0P8x3p0V0w==1Mgk0V1PNQ8fsPtqOm4tPEQY0M1Dic4o3l1oO/cS3XTHdujoKOmAR1sDfdvrGboNfR3hmmLCgSfQc+n7kxvbGKkMIRfc/DMcBCaYgr+3vVyE+N8d5kXd6hZE1WrCdWOgj1prxk/70Hk53n05kLQxppLney2NSgkBj6FwrJ9VXrbCm4zqTAwlwWdNBweX+7qRhuMbxUiXkChx52OqgPGdpIvMFNAJVtL/B11/3hIgh3soUKncKXEv8HChs18wJbJHrHLRRwcZS19DTctaJkKQxqqqMOlzYHAGP6lVczQGIaUh7m8Y0JCfYNpkg+3YATRf1EOOmAmgrYYuJLt4l/BHVKWg==HsSe2Dj8OjRUFzWEaTBfd46mEY4kf/KBvNfwY+pe2btJ2ES6mTaXt4QS5bRw8HyMmazjUF8Hz0pBnZZGfO236mscDvv8TaK5cRrn5HROXBxVXdpb/fz8CumggXKobY33iuk8bD97NpA6R4fwTnphtQc8E9ofrW1tsecFHYJNFO4HefjlKaeS0x/dQ9bEU1iD/n7vtNZVBp5jNQuLktSjTer+tMACvdrDPLZFL5hOSa0nOuA32+g9GYQD5pZUJ6r4LM87IHXF+ggf/Q64/aeDzFVIGiznsqsKg/cUm9msrY/thcOvYXWjb1r1XND3d/z3zjInKcTZ75JgVwLoXQbVIQ==dJDc+FiR8PZ9eco5WUBNUQ/Mnp3WRBmJOgvDUwNi09fA32Q5CvOQtfGaV7fqK0meFdP2OM7/XBAr4ZZhvp7UG/SHiZkOYTQxyKUgEAFtuEf9MpUamX5fA4M11h/bE95YPfabkuw6R52Ghxmg3yvQCM8Mv8JZjYJU416ko88+TjaMv8TKGM5mRxhpfkfOrdAgUENFVCLAi9Q95Glu26Y8EXuVqHAYfdILFe9XTpaM9BtZbfTPgFs7yPAtO/RqHVRGMGnuGwSNLhBViN/QV1uUys5tuU5yFT9MzM9VBifwHIpiGvl0l+j9fwZclergwuf3e4vGuFMnQZmDCuMAdzRsbw==2FQXS7khXYHRoNrE5Oqpy04c9+PtrOrSrgGtzje8cMVDuBTH63gsJxqeHYKk1IXaiRcniFtDaQ4M4+lOuMlnPNhMihd5kxD4fMUarL5XffQtBFr8H3gQDRbSV0kxFPJHVqtXBWO2pNWe6DN/cYJR7XtGSwY0dyWXWTRhkt6YCc5DP6uah3OUbKQEzCgONYXdHDyn7qsdOprz6EU15e2xSwfrnfH8ysYfAAHU/gQKJhZXmXtPqt5vzkXSm6H2A+4n/+AY/Nd/Qz9kl4zrqzWsctyzqDHLtw01O4w60bNV4lekYLkCBOOManC5wHwAni3PxtE5XzA4HJqHXKiBCAWAzpLg==ef3MO6eLNr8wHv9CMYXSbbEj8e+CUwJMbVVQql0Hsj/3V99SXflIP/Bec7iOaeX/nymKyAHaDkDjimp8EZqeboPa9p8BVViW0n1Tor6XRW7WRsmHjwLcKyJ1IGgRk08rO6PdLTUcHRAQXtsvlJuFq3EabqZk3ZliZcmljpVij3MKsnQ8VVYKZYGz17yUzgUIOXMT1C3Ouw0SjSgViI5vEQj0nTY9NrZyIN5FS4v4tf0x/Lm71uXpiiVULGEm1eHpV+I1UiP0J2O/fIUd50LoDIfC0G8dZXSoak55Rc1HI8F2Y6dyavuxi2t/k+bF8A6fFuy7vk9WEsKPiyDZ8AcN6A==WD9jVu0cbXhq4p7NQbI2vhFCdvU4GI/kHUjTmXzp7OLY7BvEbTNxlDvrEbeIkoxm8y/fcygVk3ioMMhZ81pFy+MxE2XlfsjWvVCers2Ltw+ua2dTS14FKRp8MYiFj7516rYt81hPINBK8/DzW9jtXqUdDWdHiivZDUBz8xPK+hmUuPdm8ppWPNelHS6autmjb1jBB6bEwKK38MEEkZRbE6Uj/78usd9f7YV2+HcTRy2jg2V6LcdxW8i0fNb5cJAKzKCabGt6r9WLISz8VYKJPNFMgAegpnywaPqAoBgBqFaqp2F8q0g/UTp3uxrQjKEtSVJ9i9ggVuhAK+Gwcfs1XA==3MHpp7B0EB4UvIxDYRbRVHXnhZL2ZgLNh9ALK7VsoQX276BsC4CfqwC0BLchSnHNEAj/6YYT3qsqg4AGbab0RnPfAjmBj8i8pBmndZK2kicyUbVKhzXABO5kEjBVPCMz/7g6fIysLHczHg3kd3cibyAyQTKDkYyoVhnwxG9Izj0aKBi5HzqDAkKopDwz/I228fEggoN39Pd8pbjdx2+a0BEFvJeI7m9twRsd26GxBLrXuVH6EGAVu/vDUEge2f8gCT9CaFCQq5JbyLf+uqxDBU2/GHT0IcEENtHpFCFNP3JHtnOPbICUPaZhpkc5u0TEz7O41oklslvQ/w8fEh7aNnfg==T/DoAbgx27LunmbwlY3WcTbAwYJRQeOEx6E0KSdvzM+n5dhMB39uWAdu9qilR4xnqw2+Vbt1dP+/M4q7tTOo00w990IVdHfaPAOTU/53YmjgkKqT/VzG9MZYIx8tqr3MTSmvwOtzz4yXKF3mtCg+xWI09zP96WYktLsbZKenZwNsHXtyAa3Zl5/FAas4Q8CTFg3F8xN+joKZCyWZVnnTRTG5SCyJW7M1dlr+nMMm9Awee6NvpgYBXHKfLWwk+aMJxpcR/311RUvQUEMuc52L7hsAHF+t8r81mIysGIuuKaCAR29KtYKNmYA2dxI0hJfs1lvLOmAVfxm6lSifAsgg0w==Ya2uz/a8dEV7D4X95pd/qfUEmkb8nMIEsTzCCdwHm6OkkNO++O0bjAlCuCeMcNVxAEBw6RtW3QGZH87/j3EWCzyUEGSS/ZRQbfcDt/BSx6YCEjA/ZfEHrpKWw/fSoLq8kO8X3CNzkB3T40rCtbp3c+376Qg0EswgquLvWxhLgBeRnDuM5jh/PKuvxPz3GBIP6JKVKy3L3tBdxv+tw2RqlUmbIh2/El3xT6K6jp0fOtcdI61tKY9B2mYoncGF9jyzZo8Mg/dr9OLbswhV6DLYeUbnJN8osT54EQ5dtdwZ6e62BsSSzzBQkpgrAA8IHMaiiPKhtMJJvr88HjHMN8zbrA==4Ma7ajv6ohrH3dZw12jt5CkSyOKhCqpvb0RGP4t8c58BWm0VrjbU2nzOwp1tXtJxOnH7uA1v7kyGbyv4sRY4X7rP3xzVVcgISygpdcfD+aF8ZdwTeHYHouwbgw8S6Ms0lIHNKKgZI74Wxj/m/WU6qt1ZVee8RYpb4a0O/5DbUBIC69Mg4QNHB0yjPfBXnf+b9ZdjfX58uQWEVAvLrE+hxEX73TEsOa5HVr+49FRdMq9zJy4YC8tjoOheRrhsb6oArIzZFqL9k2sc2SigJsdqWJMi8dAZFu2Xiw1nLPPWb/7WlyEcTjtu83BLfxh8Mif5xmh75FuoplA+6CRM863l7Zzw==K8wHTWPPDLBouAH0brBmbPLBMQyUWzPEB1ifobca6lWIwIoy/H0zPGvStngqOTfNcHkO00TkU691gGZbKP5PluCz11onslBJXYhATxx03WpbWXCai/uyb9fmXRwmWxFXNhrZ9gaVhPDhnQ8JuQT35bcP4XckcmNNOlz2z5KG11d6icJ4ExkdZWFDa/9KZqOekYWmbkZJB4Z6kFBfn1RmnsiLWIG+iBV4vcG9XDIu3xECt4cg69jog4nzRKXQXZsLsDAhCkZ2dyGTOXERlZp4H3+gklwuA7rUWG3/EpBFlzsbhnx4J7E40CGl3Rlq6PAu7oiMNoODsCFqyuRhIC2RwA==dk96GXnw9B5qaDHsOK90Dhg0ErenPL+9zY34ikYk+clTFFujgG9SJnKIhxYJQ1vsH5ExntoeY5BJdt5keUFZm4jN6XPI981p4eNdgtFPRdFddqVFBd/OotfDhoW9S9WtEKPKvKAbJZBmYhVXLvxKS95LkYhWeklIQQ4WxU4mEgVmgKQUUy115Wxy3N4jwFErJocrCnHPmfv4d18a0ch848ie1LXp3HnfRKaIWwNkZSxeDrofjHJxsGa3ETZcTPMHfYCdvkfC62mZKXBbVgZz2CPoD0D1iCN6wUeVHA3FpKg87mD30J5jVCoa93oq+LEBgMqgsJqrpxCdSO8wdT7Aaw== In\u00a0[0]: Copied! <pre>from spindle_token import transcode_in\nfrom spindle_token.opprl import OpprlV1\n\ntokens2 = transcode_in(\n    ephemeral_tokens, \n    tokens=(OpprlV1.token1, OpprlV1.token2, OpprlV1.token3), \n    private_key=dbutils.secrets.getBytes(\"spindle-token\", \"PrivateKey\"),\n)\ndisplay(tokens2)\n</pre> from spindle_token import transcode_in from spindle_token.opprl import OpprlV1  tokens2 = transcode_in(     ephemeral_tokens,      tokens=(OpprlV1.token1, OpprlV1.token2, OpprlV1.token3),      private_key=dbutils.secrets.getBytes(\"spindle-token\", \"PrivateKey\"), ) display(tokens2) labelgenderopprl_token_1opprl_token_2opprl_token_31Mn6YNZapNFyiP3rPO6JmlNZnxfz4PL9lfn3IOoERk9cSLe2w91EDdKHL+nNgZ60SzeRQzcM3nHDRQevzOfkFLf4dJTJg+8uLtcqHa3sHn0D4=gcbO8zyzdiMsWiWrChcskBcPHJ4z04CmZ4DrY+V20n6XaSpq8mJQ41um85gwSEcoOdIYE2ucFPn9mf3QKmNnBrRTYoA1q9KkSWK/Z6etbMg=AaGOVbbebDiOSq/lMVORAyeEHf8RcfrvPF7SCNei1wcSK7IJph22uxOPnfmmNNpAwzo9Q6Tr8XYOXwxgOYY7JG6NcX9O/1qVSGvEq3wb17E=1Mn6YNZapNFyiP3rPO6JmlNZnxfz4PL9lfn3IOoERk9cSLe2w91EDdKHL+nNgZ60SzeRQzcM3nHDRQevzOfkFLf4dJTJg+8uLtcqHa3sHn0D4=gcbO8zyzdiMsWiWrChcskBcPHJ4z04CmZ4DrY+V20n6XaSpq8mJQ41um85gwSEcoOdIYE2ucFPn9mf3QKmNnBrRTYoA1q9KkSWK/Z6etbMg=AaGOVbbebDiOSq/lMVORAyeEHf8RcfrvPF7SCNei1wcSK7IJph22uxOPnfmmNNpAwzo9Q6Tr8XYOXwxgOYY7JG6NcX9O/1qVSGvEq3wb17E=2FjSXy3EODCctLC6bYmp4PLwTyvIlruiSHVJG+PzkrffKsypBII409DuMDq+20mCoQIOnqZXqChkUNhrqiKPQX+E3wfFQWiJSEPk/HlHYKH7U=w1XiSj7vz4R68DpkQxMqSfFikQvylx3ZPrOoBQWBH3CypDgShSmw+6tC5Ows+hKVZkan+13letcC4u8ifO612dlg63hYZ/ONfTSGevD14ao=5Fk8udPfGm44jIDkFbhRJYIQnG4jjgpMhygEX7eHacpy1fCnuOMJB/5IP87570eISwYWqsRSuDna8iRqywrj8KTiVbMoY7p8Mg7Je3uFGKY=3M5etiSY9RzaVmpefeRGR0ajR08O6aez6ZrJUoAKrfIrNzOPHLAQqi4tXIp+4kvIhp4YygsXjleWirtfs8hbl0fwFWeGQ4vUBBsXUY0jG8MlU=WJ340Z+/qZy/5yTvABqJMYgrkTzZtXdHmKW/OduRiIpidBgVF8iijUfi/+M8nOKnUkgFbsCrqFtI57XP5GNuAvFmptyRS6EqPbOupF8PcNE=xnR2qS0zGdfBZrmBomMJezAPv/UYCQdpW0nFV7KiZotpYG5Q/d9EK5Ew27SaFP2dJNNVCwCEXCV+jM+RBXIuMnxbVWvy8FjPODx3UR5Qrrg=4MtfhWByB2vs7mTwVf1xQZ7n4KVf9U9JqdeINK6DJtMj/7msCQVAljCH+X58pbdjLfMDZlrMHZtiVjpl5i9nfrcj/iou5IClUnycIhupaT8JM=wBgFLcQj/QwyWe7YXrnHvNXoMQvNOmaBFi6q0gMbVSuO1f8xQk8fXoBN18/SH50amlmLs+4GV1eYatY+rvVEFVb4RJMSds91ZH/yjyLIkTw=/OfhKKqUTgAS+vABrSVG9sW0CrpiJmHDkWQ9fTOBAKASwsY9IDnBufIoKSs3SXjr24rYBvdFWMLZWAGfVukz91/yy39MsgeX7MJERxHigAQ= In\u00a0[0]: Copied! <pre>\n</pre>"},{"location":"guides/databricks/#spindle-token-databricks-getting-started-guide","title":"Spindle-token Databricks Getting Started Guide\u00b6","text":"<p>Spindle-token is an implementation of the Open Privacy Preserving Record Linkage (OPPRL) specification. It allows organizations to replace personally identifiable information (PII) with encrypted tokens that can be used to correlate data records pertaining to the same subject while keeping the subject anonymous.</p> <p>This notebook demonstrates the use of the <code>spindle-token</code> python package in databricks to parallelize the tokenization processes using Spark clusters.</p> <p>The following is a 5 row sample dataset of PII that will be tokenized. Notice that despite cosmetic differences the first 2 rows (label = 1) are describing the same subject and will receive the same sets of token values.</p>"},{"location":"guides/databricks/#one-time-workspace-setup","title":"One-time Workspace Setup\u00b6","text":"<p>See the Databricks documenation for managing secrets to get started. You will need an to install the Databricks CLI and authenticate as a user that can manage Databricks secrets.</p> <p>Create a secret scope for <code>spindle-token</code> encryption keys by running the following command with the Databricks CLI.</p> <pre><code>databricks secrets create-scope spindle-token\n</code></pre> <p>Add your private key as a secret in the <code>spindle-token</code> scope.</p> <pre><code>(cat &lt;&lt; EOF\n-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----\nEOF\n) | databricks secrets put-secret spindle-token PrivateKey\n</code></pre> <p>Add the secrets for the public keys of the third party organizations you will be sending and receiving tokenized data to/from.</p> <pre><code>(cat &lt;&lt; EOF\n-----BEGIN PUBLIC KEY-----\n...\n-----END PUBLIC KEY-----\nEOF\n) | databricks secrets put-secret spindle-token MyPartnerPublicKey\n</code></pre>"},{"location":"guides/databricks/#optional-databricks-compute-setup","title":"Optional Databricks Compute Setup\u00b6","text":"<p>Users can provide encryption keys to spindle-token via programmatic access to Databricks secrets (via <code>dbutils</code>) or by setting specific environment variables to take the value from Databricks secrets. See the Databricks secrets documentation for more information.</p> <p>Users who wish to use programmatic access via <code>dbutils</code> don't need any cluster configuration and can skip the remainder of this section.</p> <p>To configure your Databricks compute such that your encryption keys will be available as environment variables, use the following syntax in the Databricks Compute configuration UI. See the Databricks documentation on Environment Variables for more information.</p> <pre><code>SPINDLE_TOKEN_PRIVATE_KEY={{secrets/spindle-token/PrivateKey}}\nSPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY={{secrets/spindle-token/MyPartnerPublicKey}}\n</code></pre>"},{"location":"guides/databricks/#install-spindle-token","title":"Install spindle-token\u00b6","text":"<p>You can install <code>spindle-token</code> on your cluster, or within the notebook. See the Databricks docs for more information.</p>"},{"location":"guides/databricks/#tokenization-of-pii","title":"Tokenization of PII\u00b6","text":""},{"location":"guides/databricks/#transcode-tokens-as-sender","title":"Transcode Tokens As Sender\u00b6","text":"<p>The OPPRL specification also provides procedures for facilitating the safe transfer of data between trusted parties without a secure connection.</p> <p>The process of transcoding will re-encrypt of data from one scheme to another without reversing to the original PII or emitting any intermediate values. When the sender transcodes their tokens, the token values of the records no longer match to the sender's data and records within the data being shared no longer match with eachother regardless of the underlying PII. The tokens produced by the sender with <code>transcode_out</code> are called ephemeral tokens.</p> <p>Notice that the first 2 records pertaining to the same subject (label = 1) no longer have identical tokens.</p>"},{"location":"guides/databricks/#transcode-tokens-as-recipient","title":"Transcode Tokens As Recipient\u00b6","text":"<p>When receiving the data with ephemeral tokens, the recipient must call a function to transcode the tokens again so that they match with the rest of the tokens across their data assets.</p> <p>For this demonstration, we will load the encryption key our hypothetical recipient and process the ephemeral tokens we created above.</p> <p>Notice that the first 2 records pertaining to the same subject (label = 1) have identical tokens again, but do these tokens are not the same as the original tokens because they are encrypted with the scheme for the recipient.</p>"},{"location":"guides/getting-started/","title":"Getting Started","text":"<p>This guide provides installation instructions and a quick tour of the main features of spindle-token. </p>"},{"location":"guides/getting-started/#installation","title":"Installation","text":"<p>Spindle-token is a cross-platform python library built on top of PySpark. It supports the following range of versions:</p> <ul> <li>Python: 3.10+</li> <li>PySpark: 3.5+</li> </ul> <p>The latest stable release of spindle-token can be installed from PyPI into your active Python environment using <code>pip</code>.</p> <pre><code>pip install spindle-token\n</code></pre> <p>You can also build spindle-token from source using Poetry. The source code is hosted on Github. Checkout the commit you wish to build and run  <code>poetry build</code> in the project's root.</p>"},{"location":"guides/getting-started/#encryption-keys","title":"Encryption Keys","text":"<p>Tokenization is a specific use case of cryptography and relies on encryption keys. Spindle-token users are expected to provision and manage their own encryption keys.</p> <p>There are 3 kinds of encryption keys that play different roles:</p> <ol> <li>Your private RSA key - Used to transcode incoming data and derive a symmetric encryption key used to tokenize PII. This key must never be shared or accessed by untrusted parties.</li> <li>Your public RSA key - The public key counterpart to your private key. This key will be shared with trusted parties that will be sending you tokenized data.</li> <li>Trusted partner public keys - A collection of public keys from the various trusted parties that you will be sending tokenized to.</li> </ol>"},{"location":"guides/getting-started/#configuring-encryption-keys","title":"Configuring Encryption Keys","text":"<p>Organization's often manage encryption keys using a secrets manager, such as AWS Secrets Manger, HashiCorp Vault, Databricks Secrets Manger, and EnvKey. It is recommended that spindle-token users adopt a secrets manager in order to control which principals have access to encryption keys.</p> <p>To help encourage users to not hard-code encryption keys in their source code, spindle-token will default to reading encryption keys from environment variables. Most secrets managers support the pattern of injecting secrets as ephemeral environment variables and the use of an <code>.env</code> file on developer workstations allow for easy local development. You can set the <code>SPINDLE_TOKEN_PRIVATE_KEY</code> environment variable with a private RSA keys in the PEM format.</p> <p>In some cases, it may be appropriate to explicitly pass the private keys as arguments to the relevant spindle-token functions. For example, if your organization's secret manager encourages programmatic access at runtime or if you are overriding the encryption key during testing. Spindle-token functions support the explicit passing of encryption keys as <code>bytes</code> but it is highly recommended that users do not hardcode encryption keys into source code or put PEM files with production encryption keys into version control.</p> <p>This guide assumes that your private key is set via the <code>SPINDLE_TOKEN_PRIVATE_KEY</code> environment variable. For information about passing the private key as an explicit argument, see the spindle-token API documentation. </p> <p>Public keys don't need to be managed as secrets. It is possible to specify the public key (in PEM format) for your intended data recipient with the <code>SPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY</code> environment variable or pass the public key to spindle-token functions as an explicit argument. Users can pick whichever method is more convenient. </p>"},{"location":"guides/getting-started/#generating-new-keys","title":"Generating New Keys","text":"<p>Spindle-token expects encryption keys to be represented with the PEM encoding. Private keys should use the PKCS #8 format and public keys should be formatted as SubjectPublicKeyInfo. It is recommended to use a key size of 2048 bits.</p> <p>You can generate these keys using tools like <code>openssl</code> or by calling the <code>generate_pem_keys</code> function provided by Carduus. This function will return a <code>tuple</code> containing 2 instances of <code>bytes</code>. The first is the PEM data for your private key that you must keep secret. The second is the PEM data for your public key that can may share with the parties you intend to receive data from.</p> <p>You can decode these keys into strings of text (using UTF-8) or write them into a <code>.pem</code> file for later use. </p> <pre><code>from spindle_token import generate_pem_keys\n\nprivate, public = generate_pem_keys()  # Or provide key_size= \n\nprint(private.decode())\n# -----BEGIN PRIVATE KEY-----\n# ... [ Base64-encoded private key ] ...\n# -----END PRIVATE KEY-----\n\nprint(public.decode())\n# -----BEGIN PUBLIC KEY-----\n# ... [ Base64-encoded public key ] ...\n# -----END PUBLIC KEY-----\n</code></pre>"},{"location":"guides/getting-started/#tokenization","title":"Tokenization","text":"<p>Tokenization refers to the process of replacing PII with encrypted tokens using a one-way cryptographic function. Spindle-token implements the OPPRL tokenization protocol, which performs a standard set of PII normalizations and transformations such that records pertaining to the same subject are more likely to receive the same token despite minor differences in PII representation across records. According to the OPPRL protocol, the normalized PII is then hashed using SHA512 and encrypted with a symmetric encryption based on a secret key derived from your private RSA key. In the event that the private encryption key of one party is compromised, there risk to all other parties is mitigated by the fact that everyone's tokens are encrypted with a different key.</p> <p>To demonstrate the tokenization process, we will use a dataset of 5 records shown below. Each record is assigned a <code>label</code> that corresponds to the true identity of the subject.</p> <pre><code>pii = spark.createDataFrame(\n    [\n        (1, \"Jonas\", \"Salk\", \"male\", \"1914-10-28\"),\n        (1, \"jonas\", \"salk\", \"M\", \"1914-10-28\"),\n        (2, \"Elizabeth\", \"Blackwell\", \"F\", \"1821-02-03\"),\n        (3, \"Edward\", \"Jenner\", \"m\", \"1749-05-17\"),\n        (4, \"Alexander\", \"Fleming\", \"M\", \"1881-08-06\"),\n    ],\n    (\"label\", \"first_name\", \"last_name\", \"gender\", \"birth_date\")\n)\npii.show()\n# +-----+----------+---------+------+----------+\n# |label|first_name|last_name|gender|birth_date|\n# +-----+----------+---------+------+----------+\n# |    1|     Jonas|     Salk|  male|1914-10-28|\n# |    1|     jonas|     salk|     M|1914-10-28|\n# |    2| Elizabeth|Blackwell|     F|1821-02-03|\n# |    3|    Edward|   Jenner|     m|1749-05-17|\n# |    4| Alexander|  Fleming|     M|1881-08-06|\n# +-----+----------+---------+------+----------+\n</code></pre> <p>To perform tokenization, call the <code>tokenize</code> function and pass it the PII <code>DataFrame</code>, a column mapping, a collection of specifications for token you want to generate, and the private key.</p> <pre><code>from spindle_token import tokenize\nfrom spindle_token.opprl import OpprlV1\n\ntokens = tokenize(\n    pii,\n    col_mapping={\n        OpprlV1.first_name: \"first_name\",\n        OpprlV1.last_name: \"last_name\",\n        OpprlV1.gender: \"gender\",\n        OpprlV1.birth_date: \"birth_date\",\n    },\n    tokens=[OpprlV1.token1, OpprlV1.token2, OpprlV1.token3],\n).drop(\"first_name\", \"last_name\", \"gender\", \"birth_date\")\n# +-----+--------------------+--------------------+--------------------+\n# |label|     opprl_token_1v1|     opprl_token_2v1|     opprl_token_3v1|\n# +-----+--------------------+--------------------+--------------------+\n# |    1|4YO6eFn0u75yrF+Td...|V6uRRgDgXylFsNM2c...|6N+/voOASNM0ivgA7...|\n# |    1|4YO6eFn0u75yrF+Td...|V6uRRgDgXylFsNM2c...|6N+/voOASNM0ivgA7...|\n# |    2|OLU6TLB4XdWmfhvIS...|3QtCKgeIqO20Jgp9E...|JrrjTxjy97Xe93afx...|\n# |    3|mks197t0d8Uhc3l3s...|YccsE4wMErZvz9oBd...|7wMcWiDjImAhuP307...|\n# |    4|kRZfUY8KScpiHKmxC...|1gKSJDcq6YtwdUc7C...|3h8jUydHTIcOU8jhW...|\n# +-----+--------------------+--------------------+--------------------+\n</code></pre> <p>For OPPRL tokens, the column names used for token columns are the OPPRL token names. See the OPPRL protocol for more information on understanding the token name format.</p> <p>Notice that both records with <code>label = 1</code> received the same pair of tokens despite slight representation differences in the original PII.</p> <p>The <code>col_mapping</code> argument is a dictionary that maps standard OPPRL attributes -- logical types of PII fields -- to the corresponding column names from the <code>pii</code> DataFrame. This tells spindle-health how to normalize and enhance the values found in each column. For example, the <code>Opprlv1.first_name</code> object will apply name cleaning rules of version 1 of the OPPRL protocol to the values found in the <code>first_name</code> column. Spindle-token will also use this mapping to determine how to derive other PII attributes (such as first initial, soundex, and metaphone encodings) used to construct some tokens.</p> <p>The <code>tokens</code> argument is collection of OPPRL token specifications. Each token specification denotes a PII fields to jointly hash and encrypt to create a token. The current latest version of the OPPRL protocol supports three token specifications, described below:</p> Token Fields to jointly encrypt <code>token1</code> <code>first_initial</code>, <code>last_name</code>, <code>gender</code>, <code>birth_date</code> <code>token2</code> <code>first_soundex</code>, <code>last_soundex</code>, <code>gender</code>, <code>birth_date</code> <code>token3</code> <code>first_metaphone</code>, <code>last_metaphone</code>, <code>gender</code>, <code>birth_date</code> <p> Why multiple tokens? </p> <p>Each use case has a different tolerance for false positive and false negative matches. By producing multiple tokens for each record using PII attributes, each user can customize their match logic to trade-off between different kinds of match errors. Linking records that match on any token will result in fewer false negatives, and linking records that match all tokens will result in fewer false positives. User can design their own match strategies by using subsets of tokens.</p>"},{"location":"guides/getting-started/#transcoding-and-ephemeral-tokens","title":"Transcoding and Ephemeral Tokens","text":"<p>Tokens generated by a given private key can be transcoded into tokens that match those of another private key through a \"transcoding\" protocol. This process is most commonly done in the context of data sharing.</p> <p>Transcoding is performed when a user wants to share tokenized data with another party. The sender and recipient each have a corresponding transcode function that must be invoked to ensure safe transfer of data between trusted parties. The sender transcodes their tokens into \"ephemeral tokens\" that are specific to the transaction. In other words, the ephemeral tokens do not match the sender's data, the recipients data, or any prior ephemeral token from any any transactions between any sender and recipient. </p> <p>Furthermore, records from the same dataset that have identical PII will be assigned unique ephemeral tokens. This destroys the utility of the tokens until the recipient performs the reverse transcoding process using their private key. This is beneficial in the event that a third party gains access to the dataset during transfer (eg. if transcoded datasets are delivered over an insecure connection) because records pertaining to the same subject cannot be associated with each other.</p>"},{"location":"guides/getting-started/#sender","title":"Sender","text":"<p>Spindle-token provides the <code>transcode_out</code> function in for the sender to call on their tokenized datasets. In the following code snippet, notice the 2 records pertaining to the same subject (<code>label = 1</code>) no longer have identical tokens. The <code>tokens_to_send</code> DataFrame can safely be written files or a database and delivered to the recipient.</p> <pre><code>from spindle_token import transcode_out\nfrom spindle_token.opprl import OpprlV1\n\ntokens_to_send = transcode_out(\n    tokens, \n    tokens=(OpprlV1.token1, OpprlV1.token2, OpprlV1.token3), \n    recipient_public_key=b\"\"\"-----BEGIN PUBLIC KEY----- ...\"\"\",\n)\ntokens.to_send.show()\n# +-----+--------------------+--------------------+--------------------+\n# |label|     opprl_token_1v1|     opprl_token_2v1|     opprl_token_3v1|\n# +-----+--------------------+--------------------+--------------------+\n# |    1|EUqTdA0Yjb5F82oqj...|iiI/sd+sn+t5qhPDg...|JwpZVzJe+3CMocLGK...|\n# |    1|AXVDGgQ0KMa1ek1/v...|YJeYE9pz507CUzlks...|EKY2JdLhduCq+zj+p...|\n# |    2|XmYqnnMDqD9ZUR6yS...|A4e3L03NWeKbbL8vR...|CAZtbKDYjbbHsqVdc...|\n# |    3|mVhnJ1kz+ZRZvlfKo...|uO13H3LHjVnZ1flUp...|C16RKoV4SvWD5wuVp...|\n# |    4|oI/KS52K4H3M+WBdn...|vczsK9qlbU6TN1vBi...|fpTSrlGgtz3vASFXc...|\n# +-----+--------------------+--------------------+--------------------+\n</code></pre> <p>The <code>tokens</code> argument is a iterable collection containing the token specifications of the tokens to transcode. It is expected that the input DataFrame has columns with the same name as the <code>name</code> attribute of the token specification. For example, <code>OpprlV1.token3.name</code> is <code>opprl_token_3v1</code> and therefore the DataFrame must contain a column with that name. See the OPPRL protocol for more information on understanding the token name format.</p> <p>The <code>recipient_public_key</code> argument is the public key provided by the intended recipient. </p>"},{"location":"guides/getting-started/#recipient","title":"Recipient","text":"<p>The <code>transcode_in</code> function provides the transcoding process for the recipient. It is called on a dataset produced by the sender using <code>transcode_out</code> to convert ephemeral tokens into normal tokens that will match other tokenized datasets maintained by the recipient, including prior datasets delivered from the same sender.</p> <p>Notice that the first 2 records pertaining to the same subject (label = 1) have identical tokens again, but do these tokens are not the same as the original tokens because they are encrypted with the scheme for the recipient.</p> <pre><code>from spindle_token import transcode_in\nfrom spindle_token.opprl import OpprlV1\n\ntokens_received = transcrypt_in(\n    tokens_to_send, \n    tokens=(OpprlV1.token1, OpprlV1.token2, OpprlV1.token3), \n)\ntokens_received.show()\n# +-----+--------------------+--------------------+--------------------+\n# |label|       opprl_token_1|       opprl_token_2|       opprl_token_3|\n# +-----+--------------------+--------------------+--------------------+\n# |    1|q1a5o2gg1hX+CFjdP...|gS2WMqSs0SReCN7Xp...|2XAgeBwomiKq2s4xO...|\n# |    1|q1a5o2gg1hX+CFjdP...|gS2WMqSs0SReCN7Xp...|2XAgeBwomiKq2s4xO...|\n# |    2|FEyzz52oI5hym0G/+...|UQDqblSWpqoH5znps...|ud2cG5YD/2yBhjZel...|\n# |    3|aEgXJZGUpkZxt7Z7T...|QZ5LaSiE8pMDmMKEu...|0yQxfv4EFAUHatIva...|\n# |    4|CgmXSDhY8mxZCRV+O...|wu+eNOqsOvh14Bjei...|7aGCrwy8tTDL3RkO6...|\n# +-----+--------------------+--------------------+--------------------+\n</code></pre> <p>As with <code>transcode_out</code>, the <code>tokens</code> argument is a iterable collection containing the token specifications of the tokens to transcode. The input DataFrame is expected to include columns with names matching the  <code>.name</code> of each token specification.</p>"},{"location":"guides/getting-started/#deployment","title":"Deployment","text":"<p>Spindle-token is a Python library that uses PySpark to parallelize and distribute the tokenization and transcode workloads. Your application that uses spindle-token can be submitted to any compatible Spark cluster, or use a connection to a remote spark cluster. Otherwise, spindle-health will start a local Spark process that uses the resources of the host machine.</p> <p>For more information about different modes of deployment, see the official Spark documentation.</p> <ul> <li>Submitting Applications to a Spark cluster</li> <li>Spark Connect</li> <li>Spark Standalone Mode</li> </ul>"},{"location":"guides/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Reference the full API</li> <li>Learn more about using Carduus and extending it's functionality from the advanced user guides:<ul> <li>Using spindle-token on Databricks</li> <li>Defining custom token specifications</li> </ul> </li> </ul>"},{"location":"guides/migrating-from-carduus/","title":"Migrating from carduus to spindle-token","text":"<p>Prior to the v1.0 release of spindle-token, the python library carduus implemented the early version of the Open Privacy Preserving Record Linkage (OPPRL) protocol. Since the release of carduus, new versions of the OPPRL protocol have been published and the spindle-token python package was introduced.  See the pull request for the full story.</p> <p>In summary, carduus only supported version 0 of the OPPRL while spindle-token supports all versions (including version 0). The spindle-token library can be parameterized to produce tokens that match tokens produced by carduus.</p> <p>This guide is intended to help carduus users migrate their code to spindle-token without breaking their tokenized data assets. At a high level, the abstraction of both libraries is the same. The main differences are in parameterization and naming. </p>"},{"location":"guides/migrating-from-carduus/#tokenization","title":"Tokenization","text":"<p>The API for tokenizing PII has undergone the largest change in spindle-token, however the parameters are very similar to carduus.</p> <p>The following code snippets show the function calls to tokenize PII attributes in both carduus and spindle-token. Both code examples assume the private key is passed via environment variables, although it can be passed explicitly in both libraries.</p> <pre><code># carduus\nfrom carduus.token import tokenize, OpprlPii, OpprlToken\n\ntokens = tokenize(\n    pii,\n    pii_transforms=dict(\n        first_name=OpprlPii.first_name,\n        last_name=OpprlPii.last_name,\n        gender=OpprlPii.gender,\n        birth_date=OpprlPii.birth_date,\n    ),\n    tokens=[OpprlToken.token1, OpprlToken.token2, OpprlToken.token3],\n)\n# +-----+--------------------+--------------------+--------------------+\n# |...  |       opprl_token_1|       opprl_token_2|       opprl_token_3|\n# +-----+--------------------+--------------------+--------------------+\n# |     |4YO6eFn0u75yrF+Td...|V6uRRgDgXylFsNM2c...|6N+/voOASNM0ivgA7...|\n# +-----+--------------------+--------------------+--------------------+\n</code></pre> <pre><code># spindle-health\nfrom spindle_token import tokenize\nfrom spindle_token.opprl import OpprlV0 as v0  # OPPRL v0 tokens match carduus\n\ntokens = tokenize(\n    pii,\n    col_mapping={\n        v0.first_name: \"first_name\",\n        v0.last_name: \"last_name\",\n        v0.gender: \"gender\",\n        v0.birth_date: \"birth_date\",\n    },\n    tokens=[v0.token1, v0.token2, v0.token3],\n)\n# +-----+--------------------+--------------------+--------------------+\n# |...  |     opprl_token_1v1|     opprl_token_2v1|     opprl_token_3v1|\n# +-----+--------------------+--------------------+--------------------+\n# |     |4YO6eFn0u75yrF+Td...|V6uRRgDgXylFsNM2c...|6N+/voOASNM0ivgA7...|\n# +-----+--------------------+--------------------+--------------------+\n</code></pre>"},{"location":"guides/migrating-from-carduus/#key-difference","title":"Key Difference","text":"<p>Protocol Versions - Spindle-token requires importing the <code>PiiAttribute</code> and <code>Token</code> objects for specific versions of OPPRL. Carduus only supports version 0 of the OPPRL specification.</p> <p>Column mapping - Carduus <code>pii_transforms</code> use column names as keys and values are PII attribute annotations. This limits each column being used once which prevents different tokens from processing the columns in different ways (ie. using multiple versions of OPPRL at the same time). Spindle-token solves remove this limitation in <code>col_mapping</code> by using PII attributes as the keys and column names as the values.</p> <p>Token Column Names - The spindle-token library produced token columns that have versioned names to distinguish which version of the OPPRL specification was used.</p>"},{"location":"guides/migrating-from-carduus/#transcoding","title":"Transcoding","text":"<p>The API for transcoding between tokens and ephemeral tokens is nearly identical in spindle-token and carduus.</p> <p>Carduus used the made-up verb \"transcypt\" instead of the more straightforward \"transcode\". The semantics are identical with respect to both projects and this guide will use \"transcode\".</p> <pre><code># carduus\nfrom carduus.token import transcrypt_out, transcrypt_in, OpprlPii, OpprlToken\n\nephemeral_tokens = transcrypt_out(\n    tokens, \n    token_columns=(\"opprl_token_1\", \"opprl_token_2\", \"opprl_token_3\"), \n    recipient_public_key=b\"\"\"-----BEGIN PUBLIC KEY----- ...\"\"\",\n)\n\ntokens = transcrypt_in(\n    ephemeral_tokens, \n    token_columns=(\"opprl_token_1\", \"opprl_token_2\", \"opprl_token_3\"), \n)\n</code></pre> <pre><code># spindle-health\nfrom spindle_token import transcode_out, transcode_in\nfrom spindle_token.opprl import OpprlV0 as v0\n\nephemeral_tokens = transcode_out(\n    tokens, \n    tokens=(v0.token1, v0.token2, v0.token3), \n    recipient_public_key=b\"\"\"-----BEGIN PUBLIC KEY----- ...\"\"\",\n)\n\ntokens = transcode_in(\n    ephemeral_tokens, \n    tokens=(v0.token1, v0.token2, v0.token3), \n)\n</code></pre>"},{"location":"guides/migrating-from-carduus/#key-difference_1","title":"Key Difference","text":"<p>Specifying Tokens - Carduus only supported one method of transcoding tokens and therefore the API only required the names of columns containing tokens to transcode. Spindle-token allows each token to use its own protocol (ie. different versions of OPPRL) and thus the transcode functions need instances of <code>Token</code>. The column names are expected to match the <code>name</code> attribute of the <code>Token</code> instance.</p>"},{"location":"opprl/PROTOCOL/","title":"Open Privacy Preserving Record Linkage Protocol","text":"<p>Version 1.0</p>"},{"location":"opprl/PROTOCOL/#1-overview","title":"1. Overview","text":"<p>This document is a specification for the Open Privacy Preserving Record Linkage (OPPRL) protocol, which brings the capability to link records across datasets without sharing raw Personally Identifiable Information (PII). The protocol is designed with the following goals:</p> <ul> <li> <p>Interoperability - Implementations can be created in many data   systems while ensuring that tokens generated by different   implementations remain compatible.</p> </li> <li> <p>Security - Carefully selected encryption algorithms are used at   each point in the process to mitigate any chance of catastrophic   failure or information leakage.</p> </li> <li> <p>Decentralization - No trusted third parties are needed to act as   centralized authorities. No single point of failure.</p> </li> <li> <p>Scalability - Tokenization is an embarrassingly parallel task that   should scale efficiently to billions of records.</p> </li> </ul> <p>Privacy Preserving Record Linkage (PPRL) is a crucial component to data de-identification systems. PPRL obfuscates identifying attributes or other sensitive information about the subjects described in the records of a dataset while still preserving the ability to link records pertaining to the same subject using an encrypted token. This practice is sometimes referred to as \u201ctokenization\u201d and is one of the components of data de-identification.</p> <p>The task of PPRL is to replace the attributes of every record denoting Personally Identifiable Information (PII) with a token produced by a one-way cryptographic function. This prevents observers of the tokenized data from obtaining the PII. The tokens are produced deterministically such that input records with the same, or similar, PII attributes will produce an identical token. This allows practitioners to associate records across datasets that are highly likely to belong to the same data subject without having access to PII.</p> <p>Tokenization is also used when data is shared between organizations to limit, or in some cases fully mitigate, the risk of subject re-identification in the event an untrusted third party gains access to a dataset containing sensitive data. Each party produces encrypted tokens using a different secret key so that any compromised data asset is, at worst, only matchable to other datasets maintained by the same party. During data sharing transactions, a specific \u201ctranscode\u201d data flow is used to first re-encrypt the sender\u2019s tokens into ephemeral tokens that do not match tokens in any other dataset and can only be ingested using the recipient\u2019s secret key. At no point in the \u201ctranscode\u201d data flow is the original PII used.</p>"},{"location":"opprl/PROTOCOL/#2-glossary","title":"2. Glossary","text":"<p>Asymmetric Encryption: Encryption using a pair of keys: a public key for encryption and a private key for decryption. Allows secure communication without pre-sharing a secret key.</p> <p>Attribute: A single field of a record denoting one piece of information about the subject. </p> <p>Custodian: A user in possession of a data asset.</p> <p>Data Asset: A collection of records with attributes. Can be a single dataset, or a collection of related datasets.</p> <p>Derived Attribute: An attribute created by transforming another attribute. For example, a first initial attribute can be derived from a first name attribute. </p> <p>Ephemeral Token: A token produced with non-deterministic encryption (ie. RSA with OAEP and MGF1) that is used when data is in transit from a sender to recipient. Ephemeral tokens cannot be used for record linkage until they are transcoded into normal tokens by the recipient.</p> <p>Implementer: An individual or organization that creates a software tool that implements this specification.</p> <p>Normalization: A transformation applied to an attribute to standardize the representation and remove aesthetic differences. Normalized attributes are more likely to equate across records pertaining to the same subject without increasing the likelihood of equality across records pertaining to different subjects. Example: removing leading and trailing whitespace characters.</p> <p>Personally Identifying Information (PII): Attributes of a data asset that can be used to determine the identity of a subject. Examples include name, residential address, gender, age, phone number, email, as well as other demographic or socio-economic attributes.</p> <p>Recipient: A user that receives a data asset from a custodian.</p> <p>Subject: A person who is being described by one or more records in a data asset.</p> <p>Symmetric Encryption: Encryption using a single secret key for both encryption and decryption. Faster than asymmetric encryption but requires secure key exchange. </p> <p>Token: A string of text produced by encrypting the hash of concatenated, normalized PII fields. Used deterministically for record linkage.</p> <p>Transcode: The process of securely transforming tokens encrypted with a sender\u2019s key into tokens encrypted with a recipient\u2019s key, enabling linkage without sharing private keys or PII. Data being transcoded has ephemeral tokens while in transit.</p> <p>User: The end user of an OPPRL implementation. May refer to an individual or organization.</p>"},{"location":"opprl/PROTOCOL/#3-encryption-keys","title":"3. Encryption Keys","text":"<p>The OPPRL protocol relies heavily on cryptographic functions. Some of these functions encrypt and decrypt values using an encryption key.</p>"},{"location":"opprl/PROTOCOL/#31-user-specific-rsa-key-pair","title":"3.1 User Specific RSA Key Pair","text":"<p>A user/organization specific RSA key pair consisting of a private key and corresponding public key. Keys should be 2048 bits or larger [RFC 8017].</p> <p>Under no circumstances should a user\u2019s private key be shared with other users or any other third party. Doing so would compromise security and privacy of the user\u2019s OPPRL tokens.</p> <p>Implementers may add additional functionality to manage this key pair on behalf of users, or rely on the user to supply their own keys. Secure method for this are out of scope for this protocol.</p>"},{"location":"opprl/PROTOCOL/#32-derived-aes-key","title":"3.2 Derived AES Key","text":"<p>A key used when performing AES-GCM-SIV symmetric encryption and decryption. To reduce the number of encryption keys that users must coordinate, this key is derived from the private key of the user specific RSA key pair.</p> <p>During normal operation, users should not be accessing this AES key and it should never be persisted or output in any way by OPPRL implementations.</p>"},{"location":"opprl/PROTOCOL/#321-version-10","title":"3.2.1 Version &gt;=1.0","text":"<p>A 32 byte key is derived from the user\u2019s private RSA key using the HKDF algorithm [RFC 5869].</p> <p>When deriving the AES key using HKDF, no salt is used and the \u201cinfo\u201d argument is set as the UTF-8 encoding of the text \u201copprl.v1.aes\u201d in order to ensure the derived key is specific to the OPPRL key derivation.</p>"},{"location":"opprl/PROTOCOL/#322-version-10","title":"3.2.2 Version &lt;1.0","text":"<p>A 32 byte key is derived from the user\u2019s private RSA key using the SHAKE256 algorithm [FIPS 202].</p>"},{"location":"opprl/PROTOCOL/#4-functions","title":"4. Functions","text":"<p>A variety of transformations are used when normalizing PII and encoding encrypted data as text. In order to preserve token equality across implementations, this section describes the suite of relevant transformations as functions. These functions are referenced throughout the remainder of this document to describe the behavior of data flows.</p>"},{"location":"opprl/PROTOCOL/#alpha_ws_only","title":"<code>ALPHA_WS_ONLY</code>","text":"<p>Remove all characters of a string aside from alpha characters  (IEEE POSIX Standard regular expression <code>[a-zA-Z]</code>) and whitespace.</p> <p>Example: <code>ALPHA_WS_ONLY(\"Hello... world!\") =&gt; \"Hello world\"</code></p>"},{"location":"opprl/PROTOCOL/#base64_no_newline","title":"<code>BASE64_NO_NEWLINE</code>","text":"<p>Convert binary data into a string of text using the base64 encoding without any newline characters in the output [RFC 4648].</p> <p>Many implementations of base64 conform to the Multipurpose Internet Mail Extensions (MIME) standard which adds newline characters every 76 characters [RFC 2045]. These characters have no impact on decoding, and we remove them because writing multi-line strings to text files can cause issues if not handled carefully.</p> <p>Example: <code>BASE64_NO_NEWLINE(\"abc\") =&gt; \"YWJj\"</code></p>"},{"location":"opprl/PROTOCOL/#collapse_ws","title":"<code>COLLAPSE_WS</code>","text":"<p>Replace all sub-strings of one or more whitespace characters to a single space character.</p> <p>Example: <code>COLLAPSE_WS(\"Hello   world   \") =&gt; \"Hello world \"</code></p>"},{"location":"opprl/PROTOCOL/#decrypt_aes_gcm_siv","title":"<code>DECRYPT_AES_GCM_SIV</code>","text":"<p>Decrypts input binary data using the AES-GCM-SIV algorithm and user\u2019s secret key from Section 3.2. This specific variant of AES is resistant to nonce reuse, which is critical for tokenization applications [RFC 8452].</p>"},{"location":"opprl/PROTOCOL/#decrypt_rsa","title":"<code>DECRYPT_RSA</code>","text":"<p>Decrypts input binary data using the RSA algorithm and the private key corresponding to the public key used to encrypt the data. This function should use OAEP padding with SHA256 for both the mask generation function (MGF1) and the hashing algorithm [RFC 8017].</p> <p>See Section 3.1 for more details about the RSA key pair.</p>"},{"location":"opprl/PROTOCOL/#digits_only","title":"<code>DIGITS_ONLY</code>","text":"<p>Removes all non-digit characters from a string of text.</p> <p>Example: <code>DIGITS_ONLY(\"abc-123 456\") =&gt; \"123456\"</code></p>"},{"location":"opprl/PROTOCOL/#empty_to_null","title":"<code>EMPTY_TO_NULL</code>","text":"<p>Null if the given string is empty, otherwise return the string unchanged.</p> <p>Example:</p> <p><code>EMPTY_TO_NULL(\"\") =&gt; null</code></p> <p><code>EMPTY_TO_NULL(\"abc\") =&gt; \"abc\"</code></p>"},{"location":"opprl/PROTOCOL/#encrypt_aes_gcm_siv","title":"<code>ENCRYPT_AES_GCM_SIV</code>","text":"<p>Encrypts input binary data using the AES-GCM-SIV algorithm and user\u2019s secret key from Section 3.2 [RFC 8452].</p>"},{"location":"opprl/PROTOCOL/#encrypt_rsa","title":"<code>ENCRYPT_RSA</code>","text":"<p>Encrypts input binary data using the RSA algorithm and a recipient\u2019s public key. See Section 3.1 [RFC 8017].</p>"},{"location":"opprl/PROTOCOL/#first_char","title":"<code>FIRST_CHAR</code>","text":"<p>Returns the first character of the given string.</p> <p>Example: <code>FIRST_CHAR(\"abc\") =&gt; \"a\"</code></p>"},{"location":"opprl/PROTOCOL/#format_date","title":"<code>FORMAT_DATE</code>","text":"<p>Returns the given date as a formatted string in the <code>yyyy-MM-dd</code> format.</p> <p>Example: <code>FORMAT_DATE(date) =&gt; \"1997-01-01\"</code></p>"},{"location":"opprl/PROTOCOL/#is_valid_ssn","title":"<code>IS_VALID_SSN</code>","text":"<p>Returns <code>TRUE</code> if the input string is a valid US Social Security Number and <code>FALSE</code> otherwise [SSA 1982, SSA 2011].</p> <ul> <li> <p>The string must be exactly 9 numeric characters.</p> </li> <li> <p>The first character must not be a \u201c9\u201d</p> </li> <li> <p>The area number must not be \u201c000\u201d or \u201c666\u201d</p> </li> <li> <p>The group number must not be \u201c00\u201d</p> </li> <li> <p>The serial number must not be \u201c0000\u201d</p> </li> </ul>"},{"location":"opprl/PROTOCOL/#lowercase","title":"<code>LOWERCASE</code>","text":"<p>Convert all alpha characters of a string to lowercase. Leave digits and whitespace unchanged.</p> <p>Example: <code>LOWERCASE(\"ABC 123\") =&gt; \"abc 123\"</code></p>"},{"location":"opprl/PROTOCOL/#metaphone","title":"<code>METAPHONE</code>","text":"<p>Returns the Metaphone phonetic encoding of a string [Philips 1990].</p> <p>Example: <code>METAPHONE(\"Healthcare\") =&gt; \"HL0KR\"</code></p>"},{"location":"opprl/PROTOCOL/#null_if_not","title":"<code>NULL_IF_NOT</code>","text":"<p>Takes a boolean condition and a value. If the condition is true, returns the value unchanged. Otherwise return <code>NULL</code>.</p> <p>Example:</p> <p><code>NULL_IF_NOT(TRUE, \"abc\") =&gt; \"abc\"</code></p> <p><code>NULL_IF_NOT(FALSE, \"abc\") =&gt; NULL</code></p>"},{"location":"opprl/PROTOCOL/#parse_date","title":"<code>PARSE_DATE</code>","text":"<p>Parses a date (or timestamp) from a string of text. The exact logic will vary by use case depending on the format of the input string.</p>"},{"location":"opprl/PROTOCOL/#remap","title":"<code>REMAP</code>","text":"<p>Remaps the input value according to a lookup table. Non-null input values that are not found in the lookup table are given a default output value.</p> <p><code>NULL</code> values in the input are propagated as <code>NULL</code> output values.</p> <p>Example:</p> <pre><code>lookup = {\"A\" -&gt; \"X\",\n          \"B\" -&gt; \"Y\",\n          \"C\" -&gt; \"Z\"}\nREMAP(\"B\", lookup, default=\"unknown\") =&gt; \"Y\"\nREMAP(\"D\", lookup, default=\"unknown\") =&gt; \"unknown\"\nREMAP(NULL, lookup, default=\"unknown\") =&gt; NULL\n</code></pre>"},{"location":"opprl/PROTOCOL/#remove_ws","title":"<code>REMOVE_WS</code>","text":"<p>Remove all whitespace characters from the given string.</p> <p>Example: <code>REMOVE_WS(\" a b c  d\") =&gt; \"abc\"</code></p>"},{"location":"opprl/PROTOCOL/#sha256","title":"<code>SHA256</code>","text":"<p>Hash the input using the SHA256 function [FIPS 180-4].</p>"},{"location":"opprl/PROTOCOL/#soundex","title":"<code>SOUNDEX</code>","text":"<p>Returns the Soundex phonetic encoding of the input text [Russell 1922].</p> <p>Example: <code>SOUNDEX(\"Healthcare\") =&gt; \"H432\"</code></p>"},{"location":"opprl/PROTOCOL/#to_e164","title":"<code>TO_E164</code>","text":"<p>Takes a string containing a phone number and returns a string of text representing the same phone number in the E.164 international standard [ITU E.164].</p> <p>Example: <code>TO_E164(\"(234) 555-6789\") =&gt; \"+12345556789\"</code></p>"},{"location":"opprl/PROTOCOL/#to_string","title":"<code>TO_STRING</code>","text":"<p>Returns the input as a string of text. If given a string, returns the input unchanged.</p> <p>Example: <code>TO_STRING(123) =&gt; \"123\"</code></p>"},{"location":"opprl/PROTOCOL/#trim","title":"<code>TRIM</code>","text":"<p>Remove all leading and trailing whitespace from a string.</p> <p>Example: <code>TRIM(\"  abc  \") =&gt; \"abc\"</code></p>"},{"location":"opprl/PROTOCOL/#truncate_to_date","title":"<code>TRUNCATE_TO_DATE</code>","text":"<p>Truncates a value representing a point in time to the start of the day. In other words, this function removes all components of the point in time smaller than a day such as hour, minute, and second. The only components present on the outputs should be year, month, and day.</p> <p>If the input value is simply a date with no time components the date should be returned unchanged.</p> <p>Depending on the host platform used to implement the protocol, the input values may be expressed as an instance of a data type called datetime or  timestamp. </p> <p>The inputs should not be a UNIX epoch unless a corresponding timezone is known. Otherwise an accurate date  cannot be derived.</p> <p>The output should be a local date without timezone information.</p> <p>Using SQL semantics, this function should be equivalent to  <code>CAST(DATE_TRUNC(\"day\", input_timestamp) AS DATE)</code>. For most variants of SQL, the invocation of <code>DATE_TRUNC</code> is redundant, but we include it here for illustrative purposes.</p> <p>Example: <code>TRUNCATE_TO_DATE(2025-09-26 14:19:38.975197) =&gt; 2025-09-26</code></p>"},{"location":"opprl/PROTOCOL/#unbase64","title":"<code>UNBASE64</code>","text":"<p>Decode a base64 encoded string into binary.</p>"},{"location":"opprl/PROTOCOL/#uppercase","title":"<code>UPPERCASE</code>","text":"<p>Convert all alpha characters of a string to uppercase. Leave digits and whitespace unchanged.</p> <p>Example: <code>UPERCASE(\"abc 123\") =&gt; \"ABC 123\"</code></p>"},{"location":"opprl/PROTOCOL/#5-canonical-pii-attributes","title":"5. Canonical PII Attributes","text":"<p>This section contains descriptions of the canonical PII attributes used to create the standard OPPRL tokens detailed in Section 6.3. Each attribute has standard normalization that should be shared by all OPPRL implementations to improve link quality.</p>"},{"location":"opprl/PROTOCOL/#51-first-name","title":"5.1 First Name","text":"<p>The subject\u2019s first name.</p> <p>Normalization:</p> <pre><code>x = ALPHA_WS_ONLY(input)\nx = UPPERCASE(x)\nx = COLLAPSE_WS(x)\nx = TRIM(x)\nx = EMPTY_TO_NULL(x)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#52-first-initial","title":"5.2 First Initial","text":"<p>The initial of the subject\u2019s first name.</p> <p>If deriving from the subject\u2019s full first name, perform all normalizations from Section 5.1 prior to computing this attribute.</p> <p>Normalization:</p> <pre><code>x = FIRST_CHAR(input)\nx = UPPERCASE(x)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#53-first-soundex","title":"5.3 First Soundex","text":"<p>The Soundex phonetic encoding of the subject\u2019s first name.</p> <p>If deriving from the subject\u2019s full first name, perform all normalizations from Section 5.1 prior to computing this attribute.</p> <p>Normalization:</p> <pre><code>x = SOUNDEX(input)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#54-first-metaphone","title":"5.4 First Metaphone","text":"<p>The Metaphone phonetic encoding of the subject\u2019s first name.</p> <p>If deriving from the subject\u2019s full first name, perform all normalizations from Section 5.1 prior to computing this attribute.</p> <p>Normalization:</p> <pre><code>x = METAPHONE(input)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#55-last-name","title":"5.5 Last Name","text":"<p>The subject\u2019s last (aka family) name.</p> <p>Normalization:</p> <pre><code>x = ALPHA_WS_ONLY(input)\nx = UPPERCASE(x)\nx = COLLAPSE_WS(x)\nx = TRIM(x)\nx = EMPTY_TO_NULL(x)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#56-last-initial","title":"5.6 Last Initial","text":"<p>The initial of the subject\u2019s last name.</p> <p>If deriving from the subject\u2019s full last name, perform all normalizations from Section 5.5 prior to computing this attribute.</p> <p>Normalization:</p> <pre><code>x = FIRST_CHAR(input)\nx = UPPERCASE(x)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#57-last-soundex","title":"5.7 Last Soundex","text":"<p>The Soundex phonetic encoding of the subject\u2019s last name.</p> <p>If deriving from the subject\u2019s full last name, perform all normalizations from Section 5.5 prior to computing this attribute.</p> <p>Normalization:</p> <pre><code>x = SOUNDEX(input)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#58-last-metaphone","title":"5.8 Last Metaphone","text":"<p>The Metaphone phonetic encoding of the subject\u2019s last name.</p> <p>If deriving from the subject\u2019s full last name, perform all normalizations from Section 5.5 prior to computing this attribute.</p> <p>Normalization:</p> <pre><code>x = METAPHONE(input)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#59-gender","title":"5.9 Gender","text":"<p>The subject\u2019s gender. Normalized non-null values are either \u201cF\u201d for female, \u201cM\u201d for male, or \u201cO\u201d for other.</p> <p>Normalization:</p> <pre><code>x = UPPERCASE(input)\nx = COLLAPSE_WS(x)\nx = TRIM(x)\nx = EMPTY_TO_NULL(x)\nx = FIRST_CHAR(x)\nmapping = {\n  \"F\" -&gt; \"F\"\n  \"W\" -&gt; \"F\", \n  \"G\" -&gt; \"F\",\n  \"M\" -&gt; \"M\",\n  \"B\" -&gt; \"M\",\n}\nx = REMAP(x, mapping, \"O\")\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#510-birth-date","title":"5.10 Birth Date","text":"<p>The subject\u2019s date of birth.</p> <p>Normalization:</p> <pre><code>x = input\nif x is a string:\n  x = PARSE_DATE(x)\nx = TRUNCATE_TO_DATE(x)\nx = FORMAT_DATE(x)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#511-email-address","title":"5.11 Email Address","text":"<p>An email address associated with the subject.</p> <p>Normalization:</p> <pre><code>x = LOWERCASE(input)\nx = REMOVE_WS(x)\nx = EMPTY_TO_NULL(x)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#512-hashed-email-address","title":"5.12 Hashed Email Address","text":"<p>A SHA256 hash of an email address associated with the subject. This attribute is often called a Hashed Email Address (HEM) and is a widely used identifier in industry practice, although it alone has some privacy concerns addressed by tokenization [Liveramp 2022].</p> <p>If deriving from the subject\u2019s email address, perform all normalizations from Section 5.11 prior to computing this attribute.</p> <p>Normalization:</p> <pre><code>x = LOWERCASE(x)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#513-phone-number","title":"5.13 Phone Number","text":"<p>A phone number associated with the subject represented as a E.164 formatted string of text [ITU E.164].</p> <p>Normalization:</p> <pre><code>x = TO_STRING(input)\nx = TO_E164(x)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#514-social-security-number","title":"5.14 Social Security Number","text":"<p>The subject\u2019s US social security number [SSA 1982, SSA 2011].</p> <p>Normalization:</p> <pre><code>x = TO_STRING(input)\nx = DIGITS_ONLY(x)\nx = NULL_IF_NOT(IS_VALID_SSN(x))\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#515-health-plan-group-number","title":"5.15 Health Plan Group Number","text":"<p>The group number of the subject\u2019s health plan.</p> <p>Normalization:</p> <pre><code>x = UPPERCASE(input)\nx = REMOVE_WS(x)\nx = EMPTY_TO_NULL(x)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#516-health-plan-member-id","title":"5.16 Health Plan Member ID","text":"<p>The subject\u2019s health plan member ID (aka subscriber ID).</p> <p>Normalization:</p> <pre><code>x = UPPERCASE(input)\nx = REMOVE_WS(x)\nx = EMPTY_TO_NULL(x)\nreturn x\n</code></pre>"},{"location":"opprl/PROTOCOL/#6-tokenizing-pii","title":"6. Tokenizing PII","text":"<p>To create tokens from a record of PII attributes, a multi-stage workflow is performed that combines multiple PII values together, hashes them, and encrypts the resulting hash with the user\u2019s secret key. The specific subsets of PII attributes and cryptographic functions have been carefully selected to protect the security and privacy of subjects while minimizing the false negative and false positive match rates when linking.</p> <p>For the remainder of this section, we assume that all PII values have been properly normalized. See Section 6 for the canonical normalization rules of every PII attribute.</p>"},{"location":"opprl/PROTOCOL/#61-data-flow-pii-to-tokens","title":"6.1 Data Flow: PII to Tokens","text":"<p>The following subsections describe the process of generating a token from a set of PII attributes, A, from a record, R.</p> <p>We denote the PII value of R corresponding to attribute a\u2004\u2208\u2004A as R<sub>a</sub>. For example, if A\u2004=\u2004{first,\u2006 last,\u2006 birth_date} then it may be that R<sub>first </sub>\u2004\u21a6\u2004 John and R<sub>last </sub>\u2004\u21a6\u2004 Doe.</p> <p>If R<sub>a</sub>\u2004=\u2004NULL for any a\u2004\u2208\u2004A, the output token is NULL and all steps of this process should be skipped.</p>"},{"location":"opprl/PROTOCOL/#611-joining-pii","title":"6.1.1 Joining PII","text":"<p>The PII values for the token\u2019s attributes are joined into one plaintext string of text. Assuming the token\u2019s attributes are independent, the cardinality of this string is the product of attribute cardinalities. Large cardinalities protect against frequency attacks.</p> <p>Input: A record R, containing normalized PII values for each token attribute a\u2004\u2208\u2004A.</p> <p>Process:</p> <p>Let A<sup>\u2032</sup> be a sorted version of A such that attribute names are arranged in lexicographic order. This ensures the joined PII plaintext is consistent irrespective of the order of at PII attributes and record fields.</p> <p>Join all PII values R<sub>a</sub> for all a\u2004\u2208\u2004A<sup>\u2032</sup> in order. Each values is separated by a colon (<code>:</code>) character.</p> <p>Result: A string of text containing all PII values delimited by a colon.</p> <p>Example: <code>\"1970-01-01:John:Doe\"</code></p>"},{"location":"opprl/PROTOCOL/#612-hashing","title":"6.1.2 Hashing","text":"<p>The joined PII plaintext is obfuscated by a cryptographic hash function (<code>SHA512</code>) to prevent any reversal of tokens back to PII. After the PII has been hashed, the PII plaintext is never used at any point during tokenization or linking [FIPS 180-4].</p> <p>Input: A joined plaintext PII string from Section 6.1.1.</p> <p>Process: Hash the input with the <code>SHA512</code> hash function.</p> <p>Result: A 64 byte hash value.</p>"},{"location":"opprl/PROTOCOL/#613-encryption","title":"6.1.3 Encryption","text":"<p>The hash values from Section 6.2.2 are the same for all users when generated from the same PII records. It is crucial for each user to generate tokens that are unique for that user. If one user has a  re-identification incident \u2013 such as a custodial leak of tokens and their plaintext PII \u2013 the subject identities for the tokens of all other users remain secure through use of a different secret key. For this reason, the hash values are encrypted with each user\u2019s respective secret key.</p> <p>Input:</p> <ul> <li> <p><code>hash</code>: A 64 byte hash value produced by   Section 6.1.2.</p> </li> <li> <p><code>key</code>: The 32 byte AES key described in   Section 3.2.</p> </li> </ul> <p>Process:</p> <p>Encrypt the hash value with the user\u2019s AES key using the AES-GCM-SIV encryption method. To ensure encryption is deterministic, a 12 byte \u201cnonce\u201d of all zeros is used. It is critical to use the Synthetic Initialization Vector (SIV) variation of AES-GCM because it protects against catastrophic data loss when reusing a nonce [RFC 8452].</p> <p>In the context of tokenizing PII for PPRL, encryption must be  deterministic to ensure that the same PII values result in the same token and therefore can be linked with an equality comparison.</p> <p>Result: The binary data of the encrypted token.</p>"},{"location":"opprl/PROTOCOL/#614-representation","title":"6.1.4 Representation","text":"<p>The final token must be represented as text such that it can be easily written to a large variety of data files and databases.</p> <p>Input: Binary data of the encrypted token produced in Section 6.1.3.</p> <p>Process:</p> <p>The token binary is converted into a text representation using the base64 encoding using the logic in the <code>BASE64_NO_NEWLINE</code> function (See Section 4).</p> <p>Result: The final token.</p>"},{"location":"opprl/PROTOCOL/#62-token-specifications","title":"6.2 Token Specifications","text":"<p>The following lists include the subsets of PII attributes from Section 5 that are used to create each canonical token in the OPPRL specification.</p> <p>All attributes are listed in the exact lexicographical order (by name) that they must appear in the joined PII string in Section 6.1.1.</p>"},{"location":"opprl/PROTOCOL/#versions-00","title":"Versions &gt;=0.0","text":"<ul> <li> <p>Token 1: Birth Date, First Initial, Gender, Last Name</p> </li> <li> <p>Token 2: Birth Date, First Soundex, Gender, Last Soundex</p> </li> <li> <p>Token 3: Birth Date, First Metaphone, Gender, Last Metaphone</p> </li> </ul>"},{"location":"opprl/PROTOCOL/#versions-10","title":"Versions &gt;=1.0","text":"<ul> <li> <p>Token 4: Birth Date, First Initial, Last Name</p> </li> <li> <p>Token 5: Birth Date, First Soundex, Last Soundex</p> </li> <li> <p>Token 6: Birth Date, First Metaphone, Last Metaphone</p> </li> <li> <p>Token 7: First Name, Phone Number</p> </li> <li> <p>Token 8: Birth Date, Phone Number</p> </li> <li> <p>Token 9: First Name, Social Security Number</p> </li> <li> <p>Token 10: Birth Date, Social Security Number</p> </li> <li> <p>Token 11: Email Address</p> </li> <li> <p>Token 12: Hashed Email Address</p> </li> <li> <p>Token 13: Health Plan Group Number, Health Plan Member ID</p> </li> </ul>"},{"location":"opprl/PROTOCOL/#7-transcoding-tokens","title":"7. Transcoding Tokens","text":"<p>A key function of Privacy Preserving Record Linkage (PPRL) is the ability to associate records describing the same Subject without requiring knowledge of the Subject\u2019s identity. This task is commonly called \u201clinking\u201d or \u201cmatching\u201d.</p> <p>A common use case is to link datasets originating from different Custodians. As described above, every Custodian\u2019s data assets have tokens created from their unique private key(s) and therefore tokenized data cannot be linked directly. This is important for preserving security and privacy in the event of a Custodian data breach. In the scenario where an untrusted third party obtains (potentially re-identified) OPPRL tokens of one Custodian, all other Custodians with OPPRL tokens are protected by the fact that their data assets contain different token values for the same subjects.</p> <p>To enable cross-custodian data linking without introducing a universal token representation that could be used by untrusted third parties, a \u201ctranscode\u201d cryptographic workflow is performed in coordination between Custodian and Recipient. The workflow is broken up into 2 data flows: one performed by the Custodian to prepare data to be shared, and one performed by the recipient to create linkable tokenized records.</p>"},{"location":"opprl/PROTOCOL/#71-data-flow-custodian-tokens-to-ephemeral-tokens","title":"7.1 Data Flow: Custodian Tokens to Ephemeral Tokens","text":"<p>Prior to this data flow, the Recipient must have shared their public RSA key with the Custodian that will be sending data. This key exchange can occur over unsecured communication channels as long as the Recipient\u2019s private key has not been compromised and the Custodian has authenticated the source of public key as the intended recipient. These topics are out of scope for this protocol.</p> <p>Inputs:</p> <ul> <li> <p><code>token</code>: A base64 encoded token   (Section 6.1)</p> </li> <li> <p><code>recipient_public_key</code>: The recipient\u2019s public RSA key   (Section 3.1)</p> </li> <li> <p><code>private_key</code>: The custodian\u2019s private RSA key   (Section 3.1)</p> </li> </ul> <p>Process:</p> <pre><code>b = UNBASE64(token)\nh = DECRYPT_AES_GCM_SIV(b, private_key)\ne = ENCRYPT_RSA(h, recipient_public_key)\nreturn BASE64_NO_NEWLINE(e)\n</code></pre> <p>See function definitions in .</p> <p>Result: A base64 encoded ephemeral token.</p>"},{"location":"opprl/PROTOCOL/#72-data-flow-ephemeral-tokens-to-recipient-tokens","title":"7.2 Data Flow: Ephemeral Tokens to Recipient Tokens","text":"<p>This data flow is performed by a Recipient of ephemeral tokens created by a Custodian with the Recipient\u2019s public key. Ephemeral tokens are base6</p> <p>Inputs:</p> <ul> <li> <p><code>ephemeral_token</code>: A base64 encoded ephemeral token   (Section 7.1)</p> </li> <li> <p><code>private_key</code>: The recipient\u2019s private RSA key   (Section 3.1)</p> </li> </ul> <p>Process:</p> <pre><code>b = UNBASE64(ephemeral_token)\nh = DECRYPT_RSA(b, private_key)\nt = ENCRYPT_AES_GCM_SIV(h, private_key)\nreturn BASE64_NO_NEWLINE(t)\n</code></pre> <p>See function definitions in .</p> <p>Result: A base64 encoded token that can be linked (via equality) to other data assets tokenized with the same Recipient\u2019s private key.</p>"},{"location":"opprl/PROTOCOL/#8-security-engineering-considerations","title":"8. Security Engineering Considerations","text":"<p>This section contains information for implementers that falls outside the scope of standardized tokenization and linking, but are important considerations with respect to security and privacy. Ultimately, implementers are responsible for making informed architectural decisions based on the specific scenarios they are targeting.</p>"},{"location":"opprl/PROTOCOL/#81-secrets-management","title":"8.1 Secrets Management","text":"<p>The security of OPPRL replies upon the secrecy of user\u2019s private RSA key, and the derived AES key. It is common for multiple individual users (both personnel and systems) within the same user organization to collaboratively maintain tokenized data assets, and therefore shared access to the organization\u2019s secret key is required.</p> <p>It is highly recommended that users adopt a key management system with role-based access control (RBAC) such that organization administrators can enforce the principle of least privilege. In addition, using an audit logging solution to record and monitor administrative and  access activity related to private encryption keys is strongly  recommended. </p>"},{"location":"opprl/PROTOCOL/#82-key-rotation","title":"8.2 Key Rotation","text":"<p>Users should consider periodically rotating encryption keys to reduce the volume of data encrypted with a single key and shorten the window of vulnerability for a compromised key.</p> <p>User\u2019s can perform a one-time Transcoding workflow as described in Section 7 where the user acts as both the sending Custodian and the Recipient.</p> <p>After rotating encryption keys, users must share their new public RSA key with the custodians that send them data. </p> <p>When data recipients use a key rotation policy, it is recommended that  data custodian's sending data should communicate which public key they are using. This communication can happen out-of-band or the sender  can include some form of checksum  generated from the public key used to transcode the tokens into ephemeral tokens. This is currently out of scope for the OPPRL protocol.</p>"},{"location":"opprl/PROTOCOL/#references","title":"References","text":"<p>[FIPS 202] National Institute of Standards and Technology. 2015. \u201cSHA-3 Standard: Permutation-Based Hash and Extendable-Output Functions.\u201d Federal Information Processing Standards Publications (FIPS) 202. Washington, D.C.: U.S. Department of Commerce. https://doi.org/10.6028/nist.fips.202.</p> <p>[FIPS 180-4] National Institute of Standards and Technology. 2023. \u201cSecure Hash Standard  (SHS).\u201d Federal Information Processing Standards Publications (FIPS) 180-4,  March 07, 2023 Revision. Washington, D.C.: U.S. Department of Commerce. https://doi.org/10.6028/NIST.FIPS.180-4.</p> <p>[RFC 2045] Freed, Ned, and Dr. Nathaniel S. Borenstein. 1996. \u201cMultipurpose Internet Mail Extensions (MIME) Part One: Format of Internet Message Bodies.\u201d Request for Comments. RFC 2045; RFC Editor. https://doi.org/10.17487/RFC2045.</p> <p>[RFC 8452] Gueron, Shay, Adam Langley, and Yehuda Lindell. 2019. \u201cAES-GCM-SIV: Nonce Misuse-Resistant Authenticated Encryption.\u201d Request for Comments. RFC 8452; RFC Editor. https://doi.org/10.17487/RFC8452.</p> <p>[RFC 4648] Josefsson, Simon. 2006. \u201cThe Base16, Base32, and Base64 Data Encodings.\u201d Request for Comments. RFC 4648; RFC Editor. https://doi.org/10.17487/RFC4648.</p> <p>[RFC 5869] Krawczyk, Hugo, and Pasi Eronen. 2010. \u201cHMAC-based Extract-and-Expand Key Derivation Function (HKDF).\u201d Request for Comments. RFC 5869; RFC Editor. https://doi.org/10.17487/RFC5869.</p> <p>[RFC 8017] Moriarty, Kathleen, Burt Kaliski, Jakob Jonsson, and Andreas Rusch. 2016. \u201cPKCS #1: RSA Cryptography Specifications Version 2.2.\u201d Request for Comments. RFC 8017; RFC Editor. https://doi.org/10.17487/RFC8017.</p> <p>[Liveramp 2022] LiveRamp. 2022. \u201cEmail Hashing: The Trouble with Hashed Emails (HEMs).\u201d https://liveramp.com/blog/the-trouble-with-hashed-emails-hems/.</p> <p>[Philips 1990] Philips, Lawrence. 1990. \u201cHanging on the Metaphone.\u201d In. https://api.semanticscholar.org/CorpusID:59912108.</p> <p>[Russel 1922] Russell, Robert C. 1922. \u201cIndex.\u201d U.S. Patent Office; U.S. Patent.</p> <p>[ITU E.164] International Telecommunication Union. 2010. \u201cThe International Public Telecommunication Numbering Plan.\u201d Recommendation ITU-T E.164.</p> <p>[SSA 1982] Social Security Administration. 1982. \u201cMeaning of the Social Security Number.\u201d 11. Vol. 45. Social Security Administration. https://www.ssa.gov/policy/docs/ssb/v45n11/v45n11p29.pdf.</p> <p>[SSA 2011] Social Security Administration. 2011. \u201cSocial Security Is Changing the Way SSNs Are Issued.\u201d Social Security Administration. https://www.ssa.gov/kc/SSAFactSheet--IssuingSSNs.pdf.</p>"},{"location":"opprl/contrib/","title":"Contributing to the OPPRL Protocol","text":"<p> This specification is incomplete.</p>"},{"location":"opprl/contrib/#contributing-opprl-evaluation-data","title":"Contributing OPPRL Evaluation Data","text":"<p>Post on the discussion board in the OPPRL section. </p>"},{"location":"opprl/contrib/#contributing-research","title":"Contributing Research","text":"<p>Post on the discussion board in the OPPRL section. </p>"},{"location":"opprl/contrib/#proposing-changes","title":"Proposing Changes","text":"<p>Open a PR to spec document in the Carduus repository.</p>"}]}