<!-- 
THIS MARKDOWN FILE CONTAINS STATICALLY GENERATED HTML GENERATED USING MKDOCSTRING FROM THE carduus SOURCE CODE.
MANUAL EDITS SHOULD NOT BE NECESSARY, HOWEVER THIS FILE CAN BE REGENERATED BY CHECKING OUT THE carduus BRANCH
ON GITHUB AND RUN `poetry run mkdocs build`
-->

<h1 id="command-line-interface">Command Line Interface</h1>
<p>The spindle-token command line interface (CLI) offers tokenization and transcription capabilities of data files from the local file system.</p>
<h1 id="usage-guide">Usage Guide</h1>
<p>The spindle-token CLI is included with every installation of the spindle-token library. Install spindle-token to your python (virtual) environment using <code>pip</code>. See our <a href="http://token.spindlehealth.com/guides/getting-started/">getting started guide</a> for more information. Make sure the python interpreter directory is on your PATH.</p>
<p>You can test your installation and environment setup by running the <code>--help</code> command. You should see documentation about the spindle-token CLI.</p>
<div class="highlight"><pre><span></span><code>spindle-token --help
</code></pre></div>
<p>Once installed, you can run the commands for <code>tokenize</code> and <code>transcrypt</code> with the relevant options and arguments. All commands and sub-commands follow the same general design. Positional arguments and paths the the input data and desired location to write output data. Options configure how the input data is transformed. For example, options dictate which tokens should be generated, which file format to use, and the encryption key.</p>
<p>This example invocation of the <code>tokenize</code> command illustrates the general pattern.</p>
<div class="highlight"><pre><span></span><code>spindle-token tokenize \
    --token opprl_token_1 --token opprl_token_2 --token opprl_token_3 \
    --key private_key.pem \
    --format csv \
    --parallelism 1 \
    pii.csv tokens.csv
</code></pre></div>
<h3 id="encryption-keys">Encryption Keys</h3>
<p>The OPPRL protocol leaves the responsibility of encryption key management to the user. The spindle-token CLI assumes the public and private keys are stored in files on the local filesystem. The location of the PEM file can be passed using the corresponding option or an environment variable. This table describes the option names and environment variables that can be use to supply private and public keys respectively.</p>
<p><strong>Private Key</strong></p>
<p>The private RSA key can be set using one of the following methods:</p>
<ul>
<li>Use the <code>--key</code> option (or <code>-k</code> alias) to specify a path to a PEM file.</li>
<li>Set the <code>SPINDLE_TOKEN_PRIVATE_KEY_FILE</code> environment variable to specify a path the PEM file.</li>
<li>Set the <code>SPINDLE_TOKEN_PRIVATE_KEY</code> environment variable to specify the key as a UTF-8 string. If both environment variables are set, the <code>_FILE</code> variant takes precedence.</li>
</ul>
<p><strong>Public Keys</strong></p>
<p>The public keys of data recipients (used in transcryption) can be set using one of the following methods:</p>
<ul>
<li>Use the <code>--recipient</code> option (or <code>-r</code> alias) to specify a path to a PEM file.</li>
<li>Set the <code>SPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY_FILE</code> environment variable to specify a path the PEM file.</li>
<li>Set the <code>SPINDLE_TOKEN_RECIPIENT_PUBLIC_KEY</code> environment variable to specify the key as a UTF-8 string. If both environment variables are set, the <code>_FILE</code> variant takes precedence.</li>
</ul>
<h3 id="file-formats">File Formats</h3>
<p>The spindle-token CLI supports multiple data file formats. The recommended file format is <a href="https://parquet.apache.org/">parquet</a> because parquet files are efficient, compressed, and have unambiguous schemas. The spindle-token CLI also supports CSV files. The file format must be specified using the <code>--format</code> option or <code>SPINDLE_TOKEN_FORMAT</code> environment variable.</p>
<p>When using CSV files there are a few assumptions that the data file(s) must meet. </p>
<ol>
<li>
<p>The first row of each CSV file must be column headers. See the <a href="#column-names">next section</a> for expectation on the column names.</p>
</li>
<li>
<p>The field separate (aka delimiter) should be a pipe <code>|</code> character.</p>
</li>
</ol>
<p>Input datasets can either be a single data file or partitioned into a directory of multiple data files. Splitting larger datasets into multiple data files can help the CLI <a href="#parallelism">parallelize</a> over larger datasets. If the input dataset is a single data file the output dataset will be single data file. Similarly, if the input dataset is a directory the argument for the output location will be a directory that will include a partitioned output dataset.</p>
<h3 id="column-names">Column Names</h3>
<p><a name="column-names"></a></p>
<p>THe OPPRL tokenization protocol requires specific PII attributes to be normalized, transformed, concatenated, hashed, and then encrypted together. Thus, the spindle-token CLI must know which columns of the input dataset correspond to each logical PII attribute (first name, last name, birth date, etc).</p>
<p>The spindle-token CLI requires specific column naming for PII columns so that the proper normalization rules are applied to each attribute and the final tokens are created from the correct subset of inputs. The following list contains the exact column names the CLI will look for.</p>
<ul>
<li><code>first_name</code></li>
<li><code>last_name</code></li>
<li><code>gender</code></li>
<li><code>birth_date</code></li>
</ul>
<p>If you are only adding tokens that require a subset of these PII fields, the input dataset may omit columns for the other PII attributes that are not required. For information on which PII attributes are required for each token in the OPPRL protocol, see the official <a href="http://token.spindlehealth.com/opprl/PROTOCOL/">specification</a>.</p>
<h3 id="parallelism">Parallelism</h3>
<p><a name="parallelism"></a></p>
<p>The spindle-token CLI supports multi-threaded parallelism. This helps work with larger datasets that are partitioned into multiple part files within a directory. If <code>--parallelism</code> is set to a number &gt;1 then that number of partition files will be processed at once. This option can also be set with the <code>SPINDLE_TOKEN_PARALLELISM</code> environment variable. If parallelism is not provided, the spindle-token will default to using the same number of threads as the host machine has logical cores.</p>
<h1 id="commands">Commands</h1>
<p>The help text, options, and arguments of every command and sub-command of the spindle-token CLI. You can get this documentation for the specific version of the CLI installed in your python environment using the <code>--help</code> option on any command or sub-command.</p>
<h2 id="spindle-token-tokenize">spindle-token tokenize</h2>
<p>Add tokens to a dataset of PII.</p>
<p>Creates a dataset at the OUTPUT location that adds encrypted OPPRL tokens to the INPUT dataset.
Does not modify the INPUT dataset.</p>
<p>INPUT is the path to the dataset to tokenize. If INPUT is a file, it must be of the format provided to
the <code>--format</code> option. If INPUT is a directory, all files within the directory that match the given
format will be considered a partition of the dataset.</p>
<p>OUTPUT is the file or directory in which the tokenized dataset will be written. If INPUT is a
file, the OUTPUT will be written to as a file. If INPUT is a directory, the OUTPUT will be a directory
containing a dataset partitioned into files.</p>
<p><strong>Usage:</strong></p>
<div class="highlight"><pre><span></span><code>spindle-token tokenize [OPTIONS] INPUT OUTPUT
</code></pre></div>
<p><strong>Options:</strong></p>
<div class="highlight"><pre><span></span><code>  -t, --token [opprl_token_1|opprl_token_2|opprl_token_3]
                                  An OPPRL token to add to the dataset. Can be
                                  passed multiple times.  [required]
  -k, --key FILENAME              The PEM file containing your private key.
  -f, --format [parquet|csv]      The file format of input and output data
                                  files.  [required]
  -p, --parallelism INTEGER       The number of worker threads to parallelize
                                  over. Useful when the input dataset is
                                  partitioned into multiple part files. If not
                                  supplied, defaults to the number of logical
                                  cores.
  --help                          Show this message and exit.
</code></pre></div>
<h2 id="spindle-token-transcrypt">spindle-token transcrypt</h2>
<p>Prepare tokenized datasets to be sent or received.</p>
<p><strong>Usage:</strong></p>
<div class="highlight"><pre><span></span><code>spindle-token transcrypt [OPTIONS] COMMAND [ARGS]...
</code></pre></div>
<p><strong>Options:</strong></p>
<div class="highlight"><pre><span></span><code>  --help  Show this message and exit.
</code></pre></div>
<h3 id="in">in</h3>
<p>Convert a dataset of ephemeral tokens into tokens.</p>
<p>INPUT is the path to the dataset of ephemeral tokens to create tokens from. If INPUT is a file, it must
be of the format provided to the <code>--format</code> option. If INPUT is a directory, all files within the
directory that match the given format will be run.</p>
<p>OUTPUT is the file or directory in which the tokenized dataset will be written. If INPUT is a
file, the OUTPUT will be written to as a file. If INPUT is a directory, the OUTPUT will be a directory
containing a dataset partitioned into files.</p>
<p><strong>Usage:</strong></p>
<div class="highlight"><pre><span></span><code>spindle-token transcrypt in [OPTIONS] INPUT OUTPUT
</code></pre></div>
<p><strong>Options:</strong></p>
<div class="highlight"><pre><span></span><code>  -t, --token [opprl_token_1|opprl_token_2|opprl_token_3]
                                  The column name of an OPPRL token on the
                                  input data to transcrypt.  [required]
  -k, --key FILENAME              The PEM file containing your private key.
  -f, --format [parquet|csv]      The file format of input and output data
                                  files.  [required]
  -p, --parallelism INTEGER       The number of worker threads to parallelize
                                  over. Useful when the input dataset is
                                  partitioned into multiple part files. If not
                                  supplied, defaults to the number of logical
                                  cores.
  --help                          Show this message and exit.
</code></pre></div>
<h3 id="out">out</h3>
<p>Prepare ephemeral tokens for a specific recipient.</p>
<p>INPUT is the path to the dataset of tokens to create ephemeral tokens from. If INPUT is a file, it must
be of the format provided to the <code>--format</code> option. If INPUT is a directory, all files within the
directory that match the given format will be considered a partition of the dataset.</p>
<p>OUTPUT is the file or directory in which the tokenized dataset will be written. If INPUT is a
file, the OUTPUT will be written to as a file. If INPUT is a directory, the OUTPUT will be a directory
containing a dataset partitioned into files.</p>
<p><strong>Usage:</strong></p>
<div class="highlight"><pre><span></span><code>spindle-token transcrypt out [OPTIONS] INPUT OUTPUT
</code></pre></div>
<p><strong>Options:</strong></p>
<div class="highlight"><pre><span></span><code>  -t, --token [opprl_token_1|opprl_token_2|opprl_token_3]
                                  The column name of an OPPRL token on the
                                  input data to transcrypt.  [required]
  -r, --recipient FILENAME        The PEM file containing the recipients
                                  public key.
  -k, --key FILENAME              The PEM file containing your private key.
  -f, --format [parquet|csv]      The file format of input and output data
                                  files.  [required]
  -p, --parallelism INTEGER       The number of worker threads to parallelize
                                  over. Useful when the input dataset is
                                  partitioned into multiple part files. If not
                                  supplied, defaults to the number of logical
                                  cores.
  --help                          Show this message and exit.
</code></pre></div>
<h1 id="limitations">Limitations</h1>
<p>The following limitations of the spindle-token CLI are. For a superior experience, consider using spindle-token as a Python library. If your use case requires addressing some of these limitations, please open an <a href="https://github.com/spindle-health/spindle-token/issues">issue</a> with additional details. </p>
<h3 id="no-horizontal-scaling">No Horizontal Scaling</h3>
<p>The spindle-token CLI is built with <a href="https://spark.apache.org/">Apache Spark</a> to allow for data parallelism. Spark is designed to distribute workloads horizontally across a cluster of multiple machines connected to the same network. The spindle-token CLI runs spark in "local" mode that switches execution to a multi-threaded design on a single host machine.</p>
<p>The spindle-token CLI cannot be passed to <code>spark-submit</code>, nor is there currently a way to pass <a href="https://spark.apache.org/docs/latest/spark-connect-overview.html">spark connect</a> information to a remote spark cluster. If you would like to use spindle-token on a Spark cluster, it is recommended that you use the spindle-token Python library.</p>
<h3 id="no-remote-file-systems">No Remote File Systems</h3>
<p>The spindle-token CLI reads and writes files using the local file system. This means there is no native support for files stored on remote filesystems, like S3. </p>
<p>You may be able to work with remote file systems if you have a method of mounting the remote file system to the local file system.</p>
<p>If you require working with datasets on remote filesystems like S3, it is recommended that you use the spindle-token Python library and configure pyspark to read from S3.</p>
